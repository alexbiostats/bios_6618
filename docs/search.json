[
  {
    "objectID": "recitation/rTemplate/index.html",
    "href": "recitation/rTemplate/index.html",
    "title": "TITLE HERE",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nFirst Header"
  },
  {
    "objectID": "recitation/r8/index.html",
    "href": "recitation/r8/index.html",
    "title": "McNemar’s Paired Test and Discordant Pairs",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r8/index.html#no-discordant-pairs",
    "href": "recitation/r8/index.html#no-discordant-pairs",
    "title": "McNemar’s Paired Test and Discordant Pairs",
    "section": "No Discordant Pairs",
    "text": "No Discordant Pairs\nFirst let’s consider a situation where there is absolute no effect observed for our factor between a pre- and post-test:\n\n\nCode\nmytab2 &lt;- data.frame( 'Yes Post' = c(70,0), 'No Post' = c(0,30) , row.names = c('Yes Pre','No Pre'))\n\nkbl(mytab2, col.names = c('Yes Post','No Post'), align = c('c','c')) %&gt;% \n  kable_styling(bootstrap_options = \"bordered\", full_width = F) %&gt;%\n  column_spec(1, bold = TRUE)\n\n\n\n\n\n\nYes Post\nNo Post\n\n\n\n\nYes Pre\n70\n0\n\n\nNo Pre\n0\n30\n\n\n\n\n\n\n\nThis corresponds to a McNemar’s test:\n\n\nCode\nmat2 &lt;- matrix(c(70,0, 0,30), nrow=2, byrow=T) # create matrix to give McNemar's\nmcnemar.test(mat2)\n\n\n\n    McNemar's Chi-squared test\n\ndata:  mat2\nMcNemar's chi-squared = NaN, df = 1, p-value = NA\n\n\nHere we see we broke McNemar’s! In this extreme, the test statistic (\\(\\chi^2\\)) and p-value are not defined (i.e., \\(n_{D} = 0\\) from our lecture notes). However, we can see the table and determine there is no difference between pre/post periods."
  },
  {
    "objectID": "recitation/r8/index.html#post-test-better",
    "href": "recitation/r8/index.html#post-test-better",
    "title": "McNemar’s Paired Test and Discordant Pairs",
    "section": "Post-Test Better",
    "text": "Post-Test Better\nLet’s assume now we have an improvement in our post-test, but a few people may randomly do worse:\n\n\nCode\nmytab3 &lt;- data.frame( 'Yes Post' = c(55,15), 'No Post' = c(3,27) , row.names = c('Yes Pre','No Pre'))\n\nkbl(mytab3, col.names = c('Yes Post','No Post'), align = c('c','c')) %&gt;% \n  kable_styling(bootstrap_options = \"bordered\", full_width = F) %&gt;%\n  column_spec(1, bold = TRUE)\n\n\n\n\n\n\nYes Post\nNo Post\n\n\n\n\nYes Pre\n55\n3\n\n\nNo Pre\n15\n27\n\n\n\n\n\n\n\nThis corresponds to a McNemar’s test:\n\n\nCode\nmat3 &lt;- matrix(c(55,15, 3,27), nrow=2, byrow=T) # create matrix to give McNemar's\nmcnemar.test(mat3)\n\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  mat3\nMcNemar's chi-squared = 6.7222, df = 1, p-value = 0.009522\n\n\nIn this case we have a clear difference between our discordant pairs (\\(p=\\) 0.0095), and our qualitative review of the table suggests it is because 15 cases that were “No” on the pre-test ultimately became a “Yes” on the post-test, but only 3 moved from “Yes” to “No” pre- to post-test."
  },
  {
    "objectID": "recitation/r8/index.html#pre-test-better",
    "href": "recitation/r8/index.html#pre-test-better",
    "title": "McNemar’s Paired Test and Discordant Pairs",
    "section": "Pre-Test Better",
    "text": "Pre-Test Better\nLet’s assume now we have an intervention that makes individuals do worse in the exact opposite way from before:\n\n\nCode\nmytab4 &lt;- data.frame( 'Yes Post' = c(67,3), 'No Post' = c(15,15) , row.names = c('Yes Pre','No Pre'))\n\nkbl(mytab4, col.names = c('Yes Post','No Post'), align = c('c','c')) %&gt;% \n  kable_styling(bootstrap_options = \"bordered\", full_width = F) %&gt;%\n  column_spec(1, bold = TRUE)\n\n\n\n\n\n\nYes Post\nNo Post\n\n\n\n\nYes Pre\n67\n15\n\n\nNo Pre\n3\n15\n\n\n\n\n\n\n\nThis corresponds to a McNemar’s test:\n\n\nCode\nmat4 &lt;- matrix(c(67,3, 15,15), nrow=2, byrow=T) # create matrix to give McNemar's\nmcnemar.test(mat4)\n\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  mat4\nMcNemar's chi-squared = 6.7222, df = 1, p-value = 0.009522\n\n\nWe see that we have the same p-value (\\(p=\\) 0.0095), this is because the discordant pairs are what we focus on for the hypothesis test, even though the marginal (i.e., overall) rates of yes/no for the pre-test are different in each case."
  },
  {
    "objectID": "recitation/r8/index.html#similar-discordant-rates",
    "href": "recitation/r8/index.html#similar-discordant-rates",
    "title": "McNemar’s Paired Test and Discordant Pairs",
    "section": "Similar Discordant Rates",
    "text": "Similar Discordant Rates\nFor our final case, let’s assume there is similar rates of change suggesting our intervention does not have an affect and performance may be somewhat random:\n\n\nCode\nmytab5 &lt;- data.frame( 'Yes Post' = c(45,25), 'No Post' = c(22,3) , row.names = c('Yes Pre','No Pre'))\n\nkbl(mytab5, col.names = c('Yes Post','No Post'), align = c('c','c')) %&gt;% \n  kable_styling(bootstrap_options = \"bordered\", full_width = F) %&gt;%\n  column_spec(1, bold = TRUE)\n\n\n\n\n\n\nYes Post\nNo Post\n\n\n\n\nYes Pre\n45\n22\n\n\nNo Pre\n25\n3\n\n\n\n\n\n\n\nThis corresponds to a McNemar’s test:\n\n\nCode\nmat5 &lt;- matrix(c(42,25, 22,3), nrow=2, byrow=T) # create matrix to give McNemar's\nmcnemar.test(mat5)\n\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  mat5\nMcNemar's chi-squared = 0.085106, df = 1, p-value = 0.7705\n\n\nIn this case we do not have a significant difference (\\(p=\\) 0.7705), and our qualitative review of the table suggests it is because ultimately similar numbers of individuals switched between yes/no in both directions (i.e., yes to no, no to yes)."
  },
  {
    "objectID": "recitation/r6/index.html",
    "href": "recitation/r6/index.html",
    "title": "Is the CLT Only for the Mean?",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nIs the CLT Only for the Mean?\nThe central limit theorem (CLT) only applies for the sample mean. However, this is an extremely strong result since this holds as long as (1) the mean and variance exist (i.e., no Cauchy distributions here!), (2) the observations are independent and identically distributed (i.e., come from the same distribution), and (3) you have sufficient \\(n\\) (sample size). For normally distributed data, the CLT holds exactly. For all other data, the approximation becomes more accurate as \\(n\\) increases.\nHowever, once we know the sample mean is normally distributed we are able to do lots of statistical inference that may be more challenging if each distribution’s sample mean had its own properties.\nFurther, there are other flavors of CLT’s that relax our independence (CLT under weak dependence) or identically distributed (Lyapunov CLT) assumptions with some additional conditions that must be met.\nThere has also been related theoretical work that connects to the sample mean, but in very different contexts. For example, see the “Beyond the classical framework” subsection of the CLT’s Wiki page."
  },
  {
    "objectID": "recitation/r57/index.html",
    "href": "recitation/r57/index.html",
    "title": "Post-Hoc Testing Justification for One-Way ANOVA",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nPost Hoc Testing Justification\nFor our ANOVA examples, we discussed various post hoc (i.e., after the fact) testing strategies (e.g., least significant difference, Bonferroni adjustment, Tukey’s honestly significant difference, Dunnett’s test). The question may arise, when should we actually conduct post hoc testing? There are two general camps:\n\nPost hoc testing is only justified if the global hypothesis test (i.e., our overall \\(F\\)-test from the ANOVA) is statistically significant (e.g., \\(p &lt; 0.05\\) if our study has set \\(\\alpha=0.05\\)).\nIf you were actually interested in the pairwise comparisons from the get-go, don’t both with fitting the ANOVA model. Instead, design your analytic strategy to directly address the question(s) of interest! In other words, we have no post hoc testing since we planned and intended to evaluate the pairwise comparisons of interest all along!\n\nFor our homework, since we don’t have background information on planning for pairwise comparisons, we should treat the approach as post hoc and only conduct the follow-up tests if the the overall ANOVA suggests it is warranted."
  },
  {
    "objectID": "recitation/r55/index.html",
    "href": "recitation/r55/index.html",
    "title": "Z-Scores",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r55/index.html#definition",
    "href": "recitation/r55/index.html#definition",
    "title": "Z-Scores",
    "section": "Definition",
    "text": "Definition\nIf \\(X \\sim N(\\mu,\\sigma^2)\\), we can leverage many of the features of the normal distribution (e.g., symmetric, approximately 95% of the distribution falling within \\(\\mu \\pm 2 \\times \\sigma\\)). However, \\(X\\) is contingent upon the context of a given problem. \\(P(X \\geq 72)\\) may make sense if estimating the probability that height is greater than 72 inches for a human, but is less informative if we are examining the heights of ants.\nInstead, we can standardize any normally distributed variable with some mean, \\(\\mu\\), and a variance, \\(\\sigma^2\\), so that it has a unitless mean of 0 and variance of 1:\n\\[ Z = \\frac{X-\\mu}{\\sigma} \\]\nNow, a calculation of \\(P(Z&gt;1)\\) has a consistent meaning across any standard normal random variable (i.e., the probability that something is 1 standard deviation above the mean or greater).\nThe trade-off is that the measure is essentially contextless, we would need to transform back to our original scale if we wanted to evaluate with respect to a given \\(\\mu\\) and \\(\\sigma^2\\). However, if someone presented a range of comparisons using Z-scores, we could quickly identify which ones were more significant for hypothesis testing (i.e., had a large positive or negative value) or had more extreme values (e.g., an observation with a value of 8 indicates it is 8 standard deviations above the mean, which is highly unlikely if the data is truly normally distributed)."
  },
  {
    "objectID": "recitation/r55/index.html#where-do-we-find-z-scores",
    "href": "recitation/r55/index.html#where-do-we-find-z-scores",
    "title": "Z-Scores",
    "section": "Where Do We Find \\(Z\\) Scores?",
    "text": "Where Do We Find \\(Z\\) Scores?\nThroughout the semester, we will see various statistical tests utilizing the \\(Z\\)-statistic to evaluate statistical significance. We will also see it used when calculating a confidence interval when we are willing or able to assume normality."
  },
  {
    "objectID": "recitation/r55/index.html#standardizing-variables",
    "href": "recitation/r55/index.html#standardizing-variables",
    "title": "Z-Scores",
    "section": "Standardizing Variables",
    "text": "Standardizing Variables\nWhile only normal random variables, \\(X\\), can be made into a standard normal random variable, \\(Z\\). We can still standardized our data more generally to have a mean of 0 and standard deviation of 1 (note that the variance is also 1 since \\(1^{2}=1\\)). For situations where we are calculating the sample mean, we can rely on the central limit theorem to know that \\(\\bar{X}\\) is normally distributed, which can be transformed to a standard normal distribution itself.\nFor raw data, the scale() function can standardize any variable, whether or not it is normally distributed. Let’s start by examining a normally distributed random variable:\n\n\nCode\n# standardize a normally distributed variable\nset.seed(888)\n\n# simulate raw data\nnorm_dat &lt;- rnorm(n=500, mean=50, sd=15)\nmean(norm_dat)\n\n\n[1] 50.3258\n\n\nCode\nsd(norm_dat)\n\n\n[1] 14.88208\n\n\nCode\n# standardize to mean=0, sd=1\nstandardized_norm_dat &lt;- scale(norm_dat)\nmean(standardized_norm_dat)\n\n\n[1] 1.185051e-16\n\n\nCode\nsd(standardized_norm_dat)\n\n\n[1] 1\n\n\nCode\n# create histograms of raw data vs. standardized data\npar(mfrow=c(1,2))\nhist(norm_dat, main='Raw Data')\nhist(standardized_norm_dat, main='Standardized Data')\n\n\n\n\n\nWe see that the histograms are fairly normal looking (since we did simulate normally distributed data), but that the standardized version of our data has a mean near 0 with a standard deviation of 1.\nFor non-normally distributed data, we see we still achieve the same mean of 0 and standard deviation of 1, but we do not force normality upon the data. For example, consider exponentially simulated data:\n\n\nCode\n# standardize a normally distributed variable\nset.seed(888)\n\n# simulate raw data\nexp_dat &lt;- rexp(n=500, rate=0.5)\nmean(exp_dat)\n\n\n[1] 2.082536\n\n\nCode\nsd(exp_dat)\n\n\n[1] 2.031941\n\n\nCode\n# standardize to mean=0, sd=1\nstandardized_exp_dat &lt;- scale(exp_dat)\nmean(standardized_exp_dat)\n\n\n[1] 9.307084e-17\n\n\nCode\nsd(standardized_exp_dat)\n\n\n[1] 1\n\n\nCode\n# create histograms of raw data vs. standardized data\npar(mfrow=c(1,2))\nhist(exp_dat, main='Raw Data')\nhist(standardized_exp_dat, main='Standardized Data')\n\n\n\n\n\nWe see in this case that the standardized variable is no longer forced to have \\(x \\geq 0\\) since its mean is 0 and the standard deviation is 1. The strong right skew to our data points is still present after transformation."
  },
  {
    "objectID": "recitation/r53/index.html",
    "href": "recitation/r53/index.html",
    "title": "Is It Possible to Tell The Role of a Variable (e.g., PEV, Confounder, etc.) From the Regression Model?",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nThe Role of a Variable\nIs it possible to determine the role a variable might be used for by simply looking at a single regression model (either the model written out, its line of code, the corresponding output, etc.)?\nShort answer: no. Unfortunately, if we are only given a single model with no context, we have no way of knowing what role each variable plays. Is it the primary explanatory variable, a confounder, a mediator, a precision variable, or some other special type of role we haven’t even touched on yet?\nPart of this challenge is because the mathematics/code behind the scene for any given model do not take into account if something is serving a specific role. Rather, the formulas we’ve derived simply take the data and work to provide our estimates for the beta coefficients, their standard errors, etc.\nIf we are given a little more context or multiple models to work with, we may be able to infer certain questions that are being asked or that a certain variable appears in every single one, but even then we know the specific question(s) being asked may be ambiguous (e.g., the models used for confounding and mediation are the same).\nTruly, this touches more on statistics as an art than a strict scientific or mathematical application."
  },
  {
    "objectID": "recitation/r51/index.html",
    "href": "recitation/r51/index.html",
    "title": "Simulation vs. Bootstrap vs. Permutation",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nSimulation vs. Bootstrap vs. Permutation\nWhile each of these three concepts involve some element of randomness, they have some major differences:\n\nA simulation uses randomness to generate a set of data where you are able to set the known truth. Since the data generated is random, any single simulation could be extremely different from the set truth by chance. Therefore, simulation studies rely on 1000s of simulated data sets to draw conclusions and summarize performance.\nA bootstrap uses randomness to resample observations with replacement from a single data set. For a simulation study, you would apply a bootstrap to each simulated data set. In practice, we just apply the bootstrap to our collected sample of real-world data (i.e., where we don’t know the “truth”). Bootstraps summarize the variability of our estimator (usually in the form of a confidence interval, but we could also use its standard error).\nA permutation uses randomness to resample the group label without replacement while keeping other information constant (i.e., it shuffles the group labels). For a simulation study, you’d apply your permutation test to each simulated data set to summarize. In practice, we’d apply it once to our real world data. Permutation tests are used to generate a null distribution, even if we don’t know the theoretical properties of our estimator, and to calculate a p-value."
  },
  {
    "objectID": "recitation/r5/index.html",
    "href": "recitation/r5/index.html",
    "title": "Long-Run Properties of \\(\\alpha\\) and \\(\\beta\\) (Power)",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nLong-Run Properties of \\(\\alpha\\) and \\(\\beta\\) (Power)\nFrom our lecture we noted that “Neyman and Pearson assumed that \\(\\alpha\\) and \\(\\beta\\) were in terms of the long run (i.e., over infinite repeated samples).” This gets at the idea that frequentist power and type I error rates are estimates of what will occur over many, many repeated experiments (i.e., the long run), instead of what will happen in any one trial.\nWe do interpret our NHST (null hypothesis significance testing) framework (an amalgamation of Fisher’s p-value with Neyman and Pearson’s rejection regions) as properties that could apply to any individual study. For example, power (\\(1-\\beta\\)) can be thought of as the probability that you reject the null hypothesis assuming a difference truly exists.\nIn practice, any one study results in a yes/no decision. Either we found our effect (if it exists) or we didn’t, so it doesn’t actually tell us much about the overall probability of finding the effect across repeated studies.\nTo evaluate the long run properties, we can explore some simulation studies! Assume we wish to have 90% power to detect a difference of 0.5 units with \\(\\sigma=1\\) and \\(\\alpha=0.05\\). This would require 44 participants (remember to always round your sample size up):\n\n\nCode\npower.t.test(delta=0.5, sd=1, sig.level=0.05, power=0.9, n=NULL, type='one.sample')\n\n\n\n     One-sample t test power calculation \n\n              n = 43.99552\n          delta = 0.5\n             sd = 1\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\n\nFor our simulation we’ll examine how the estimated power changes from conducting 1 to 10000 experiments for both the alternative and null hypothesis while simulating data from a normal distribution:\n\n\nCode\nset.seed(6618) # set seed for reproducibility\n\nntest &lt;- 10000 # number of tests\nn &lt;- 44 # from power calculation\nalpha &lt;- 0.05\n\n# simulate data and save p-values from t-test\nalt_dat &lt;- sapply(1:ntest, function(x) t.test(rnorm(n=n, mean=0.5, sd=1))$p.value )\nnull_dat &lt;- sapply(1:ntest, function(x) t.test(rnorm(n=n, mean=0, sd=1))$p.value )\n\n# let's explore the first set of p-values for each simulation:\nround(alt_dat[1:10],4)\n\n\n [1] 0.0553 0.0081 0.0030 0.0001 0.0484 0.0121 0.0006 0.0001 0.0000 0.0022\n\n\nCode\nround(null_dat[1:10],4)\n\n\n [1] 0.5685 0.6993 0.3586 0.2545 0.6701 0.6403 0.8579 0.2685 0.4645 0.3779\n\n\nWe can see from the first 10 p-values, there are 1/10 alternative hypotheses that have false negatives (i.e., where p&gt;0.05) and 0/10 null hypothesis that had a false positive (i.e., where p&lt;0.05). Although, this isn’t a huge number of tests, so we can also visualize the trends. Let’s start with the first 100 tests to see how power and type I error rates change as we repeat the experiment more often:\n\n\nCode\n# calculate rejection rate (i.e., power and type I error rate) for 1st through ntest-th p-value\npower &lt;- sapply(1:ntest, function(x) mean(alt_dat[1:x] &lt;= alpha))\nt1e &lt;- sapply(1:ntest, function(x) mean(null_dat[1:x] &lt;= alpha))\n\n# zoom into first 100 tests\nplot(x=1:100, y=power[1:100], xlab='Number of Experiments', ylab='Rejection Rate', type='l', ylim=c(0,1))\nlegend('top', xpd=T, bty='n', horiz=T, col=c('black','blue'), lty=c(1,1), legend=c('Power','Type I Error Rate'), inset=-0.125)\nlines(x=1:100, y=t1e[1:100], col='blue')\nabline(h=c(0.05,0.90), lty=2, col='gray85')\n\n\n\n\n\nWe can see in the first 100 experiments our type I error rate is a little conservative, although it converges to 0.05 around 90 experiments. For power, it starts very low (since the first experiment resulted in a false negative) and then quickly starts to approximate our target of 90% power before being overly optimistic.\nWith 100 experiments, we may be satisfied that things are “close enough”, but we can explore the long(er) run probability:\n\n\nCode\n# plot power and t1e over all ntest experiments\nplot(x=1:ntest, y=power, xlab='Number of Experiments', ylab='Rejection Rate', type='l', ylim=c(0,1))\nlegend('top', xpd=T, bty='n', horiz=T, col=c('black','blue'), lty=c(1,1), legend=c('Power','Type I Error Rate'), inset=-0.125)\nlines(x=1:ntest, y=t1e, col='blue')\nabline(h=c(0.05,0.90), lty=2, col='gray85')\n\n\n\n\n\nEven though there is still some noise, we see both our type I error rate and power converge to \\(\\alpha\\) and \\(1-\\beta\\) by 10000 experiments. And if we extended this further to \\(\\infty\\) experiments, we’d converge to \\(\\alpha\\) and \\(1-\\beta\\)."
  },
  {
    "objectID": "recitation/r48/index.html",
    "href": "recitation/r48/index.html",
    "title": "Hypothesis Testing for F-tests",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r48/index.html#the-overall-model-fit",
    "href": "recitation/r48/index.html#the-overall-model-fit",
    "title": "Hypothesis Testing for F-tests",
    "section": "The Overall Model Fit",
    "text": "The Overall Model Fit\nOne question we might be interested in is, do calories and smoking status contribute significantly to the prediction of beta-carotene levels? Specifically, is this model better than using just the sample mean on its own?\nMore generally, we might ask this question as do any of the predictors in my model contribute significantly to the prediction of my outcome?\nThe hypothesis we are testing is evaluated with the overall \\(F\\)-test:\n\n\\(H_0\\): \\(\\beta_1 = \\beta_2 = \\beta_3 = 0\\)\n\\(H_1\\): at least one \\(\\beta_{i}\\) (\\(i \\in (1,2,3)\\)) is not equal to 0\n\nThere are two direct approaches we’ve covered in R that use functions without any additional packages:\n\nIf you fit your model with lm, the summary() of your model includes the overall \\(F\\)-test in the output.\n\nlm_full &lt;- lm(y ~ x1 + x2 + x3, data=dat)\nsummary(lm_full)\n\nIf you fit your model with glm (or also lm), you can use the anova function to compare 2 models. In this case, we’d still need to fit a null model with only the intercept as our comparator.\n\nglm_full &lt;- glm(y ~ x1 + x2 + x3, data=dat)\nglm_null &lt;- glm(y ~ 1, data=dat)\nanova(glm_full, glm_null, test='F')\n\n\nOne challenge is that your full model may contain variables that obfuscate the true effect and mislead the potential significance of variables. For example, in this problem we will see that 3c suggests no significant predictors in the model, but the model without calories in 3e does have a significant overall \\(F\\)-test! We will discuss other strategies to evaluating model fit in the near future (e.g., model selection criterion)."
  },
  {
    "objectID": "recitation/r48/index.html#reduced-models",
    "href": "recitation/r48/index.html#reduced-models",
    "title": "Hypothesis Testing for F-tests",
    "section": "Reduced Models",
    "text": "Reduced Models\nAnother question that might be of interest is if a specific variable contributes meaningfully above and beyond that of other variables in the model.\nMore generally, we might ask this question as does this subset of variables (which could be just a single variable) contribute significantly to the prediction of our outcome above and beyond the other variables remaining in the model?\nIn the case of categorical variables with multiple categories, this is similar to asking is my multi-categorical variable significantly associated with my outcome?\nIn other words, we may see the terms associated, contribute, or predict use fairly interchangeably in our class. In some contexts there are special considerations (e.g., prediction modeling is not something we cover in BIOS 6611 beyond diagnostic testing principles like sensitivity and specificity).\nThese types of hypotheses are evaluated with the partial \\(F\\)-test.\nRecall, for our carotenoids data our full regression model is \\(Y = \\beta_0 + \\beta_1 X_{\\text{former}} + \\beta_2 X_{\\text{current}} + \\beta_3 X_{\\text{calories}} + \\epsilon\\). We may ask questions like:\n\nIs smoking status significantly associated with plasma beta-carotene levels?\n\n\\(H_0\\): \\(\\beta_1 = \\beta_2 = 0\\)\n\\(H_1\\): either \\(\\beta_1\\) and/or \\(\\beta_2\\) are not 0\nThe most direct way to implement the partial \\(F\\)-test is using the anova function for both lm and glm models:\n\nglm_full &lt;- glm(y ~ x1 + x2 + x3, data=dat)\nglm_red &lt;- glm(y ~ x3, data=dat)\nanova(glm_full, glm_red, test='F')\n\n\nIs calories significantly associated with plasma beta-carotene levels?\n\n\\(H_0\\): \\(\\beta_3 = 0\\)\n\\(H_1\\): \\(\\beta_3 \\neq 0\\)\nThis question could also be answered directly from the regression output’s coefficient table with the \\(t\\)-statistic and p-value."
  },
  {
    "objectID": "recitation/r46/index.html",
    "href": "recitation/r46/index.html",
    "title": "Log Transformations and Interpretations in Linear Regression",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nLog Transformations and Interpretations\nFirst, remember that in statistics and mathematics we almost always mean the natural log (\\(\\ln\\), \\(\\log_{e}\\)) and not \\(\\log_2\\) which may be more commonly assumed in computer science/informatics settings or \\(\\log_{10}\\) that might be more commonly assumed in engineering contexts.\nWhen we take a log-transformation of our outcome, \\(Y\\), it obviously changes the model we are fitting. For example, consider the blood storage dataset from our course website:\n\n\nCode\ndatb &lt;- read.csv('../../.data/Blood_Storage.csv')\n\n\nThe dataset includes 316 men who had undergone radical prostatectomy and received transfusion during or within 30 days of the surgical procedure and had available PSA follow-up data. Of the 316 men, 307 have data for prostate volume (in grams) and age (in years).\nLet’s examine the outcome of prostate volume and a single predictor of age. The true regression equation, for our observed prostate volume is\n\\[ Y = \\beta_{0,Y} + \\beta_{1,Y} X_{age}+ \\epsilon; \\; \\epsilon \\sim N(0,\\sigma^{2}_{e})  \\]\nThe true regression equation for our log-transformed prostate volume is very similar:\n\\[ \\log(Y) = \\beta_{0,\\log(Y)} + \\beta_{1,\\log(Y)} X_{age}+ \\epsilon; \\; \\epsilon \\sim N(0,\\sigma^{2}_{e})  \\]\nIf we fit both models we also have similar interpretations for our beta coefficients, but they change with respect to what the outcome is:\n\n\\(\\hat{\\beta}_{1,Y}\\) for \\(Y\\): For a one-year increase in age, prostate volume increases by an average of \\(\\hat{\\beta}_{1,Y}\\).\n\\(\\hat{\\beta}_{1,\\log(Y)}\\) for \\(\\log(Y)\\): For a one-year increase in age, log(prostate volume) increases by an average of \\(\\hat{\\beta}_{1,\\log(Y)}\\).\n\nHowever, we often want to interpret our outcome for \\(\\log(Y)\\) back on its original scale. Here, we can do a back-transformation from \\(\\log(Y)\\) to \\(Y\\). For our regression model, this changes our interpretation of \\(\\beta_{1,\\log(Y)}\\) on an arithmetic scale to \\(\\exp(\\beta_{1,\\log(Y)})\\) on a multiplicative scale. This follows for our confidence intervals as well.\nLet’s see our example and interpret:\n\n\nCode\nmod2 &lt;- lm( log(PVol) ~ Age, data=datb )\nsummary(mod2)\n\n\n\nCall:\nlm(formula = log(PVol) ~ Age, data = datb)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.85316 -0.25833 -0.02639  0.18992  1.56272 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.83345    0.18124   15.63  &lt; 2e-16 ***\nAge          0.01820    0.00295    6.17 2.17e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3719 on 305 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.111, Adjusted R-squared:  0.108 \nF-statistic: 38.07 on 1 and 305 DF,  p-value: 2.169e-09\n\n\nCode\nconfint(mod2)\n\n\n                 2.5 %     97.5 %\n(Intercept) 2.47681662 3.19008051\nAge         0.01239449 0.02400252\n\n\nThe slope is the average increase in log(prostate volume) for a one year increase in age. In our problem, it represents that log(prostate volume) increases on average by 0.0182 log(grams) for a one year increase in age (95% CI: 0.0124, 0.0240 log(grams)).\nIf we exponentiate the slope, our interpretation changes from additive change to multiplicative change: \\(e^{0.0182}=1.018367\\). On average, a one year increase in age results in a prostate volume that is 1.84% higher (i.e., 1.0184 times higher). This also applies to our 95% confidence interval: \\(e^{(0.0124, 0.0240)} = (1.0125,1.0243)\\). We are 95% confident that a one year increase in age results in a prostate volume (in grams) that is 1.25% to 2.43% higher (i.e., 1.0125 to 1.0243 higher).\nIf we conduct any diagnostics (e.g., plots, calculating residuals), we will do this for the model we fit with log(PVol). Otherwise, we are really examining the relationship of the original, non-transformed PVol that may lead to different conclusions on model assumptions.\nThe intercept when back-transformed now represents the geometric mean and not the arithmetic mean."
  },
  {
    "objectID": "recitation/r44/index.html",
    "href": "recitation/r44/index.html",
    "title": "Simple Linear Regression Assumptions",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nSimple Linear Regression Assumptions\nFor simple linear regression, our true regression equation is\n\\[ Y = \\beta_0 + \\beta_{1} X_{1} + \\epsilon, \\; \\epsilon \\sim N(0,\\sigma_{e}^2) \\]\nThere are five assumptions we make when using simple linear regression:\nExistence: For any fixed value of the variable \\(X\\), \\(Y\\) is a random variable with a certain probability distribution having finite mean and variance.\nIndependence: The errors, \\(\\epsilon_i\\), are independent (i.e., \\(Y\\)-values are statistically independent of one another).\nLinearity: The mean value of \\(Y\\) (or a transformation of \\(Y\\)), \\(\\mu_{Y|X}=E(Y)\\), is a straight-line function of \\(X\\) (or a transformation of \\(X\\)).\nHomoscedasticity: The errors, \\(\\epsilon_i\\), at each value of the predictor, \\(x_i\\), have equal variance (i.e., the variance of \\(Y\\) is the same for any \\(X\\)). That is, \\[ \\sigma_{Y|X}^2 = \\sigma_{Y|X=1}^2 = \\sigma_{Y|X=2}^2 =... = \\sigma_{Y|X=x}^2 \\]\nNormality: The errors, \\(\\epsilon_i\\), at each value of the predictor, \\(x_i\\), are normally distributed (i.e., for any fixed value of \\(X\\), \\(Y\\) has a normal distribution). (Note this assumption does not state that Y is normally distributed.)\nIf our data do not meet these assumptions, we have various alternatives to consider (e.g., data transformations, different types of models, etc.). It can be helpful to visualize what the linearity, homoscedasticity, and normality assumptions look like in practice for simple linear regression when we only have one predictor variable. Below are two examples we will walk through:\n\n\n\nVisualization of linearity, homoscedasticity, and normality assumptions for simple linear regression.\n\n\n\n\n\nAnother visualization example, with data observations included."
  },
  {
    "objectID": "recitation/r42/index.html",
    "href": "recitation/r42/index.html",
    "title": "Permutation Test p-values",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r42/index.html#one-sided-p-value-calculation",
    "href": "recitation/r42/index.html#one-sided-p-value-calculation",
    "title": "Permutation Test p-values",
    "section": "One-Sided p-value Calculation",
    "text": "One-Sided p-value Calculation\nIn the case of a one-sided p-value the approach is a little more straight forward, but we still need to consider the context of our problem!!\n\n\nCode\n# Calculate histogram, but do not draw it\nmy_hist &lt;- hist(result , plot=F)\n \n# Color vector\nmy_color= ifelse(my_hist$breaks &gt; obs_ratio, 'lightblue' , 'orange' )\n \n# Final plot\nplot(my_hist, col=my_color, border=T, xlab='Var(Placebo) / Var(Vaccine)', main='Permutation Distribution for Ratio of Variances')\nrect(xleft=obs_ratio, ybottom=0, xright=1.2, ytop=2042, col='lightblue')\nabline( v=obs_ratio, lty=2, col='blue', lwd=2)\ntext(x=2, y=1000, 'result &gt;= obs_ratio', col='blue')\ntext(x=0.3, y=1000, 'result &lt;= obs_ratio', col='orangered2', srt=60)\n\n\n\n\n\nHere our interpretation then depends on our a priori specified null hypothesis (i.e., that we expect the ratio to be larger than 1 or small than 1 based on our context).\nNumerically, these values are:\n\n\nCode\n# The correct calculations\n((sum(result &gt;= obs_ratio) + 1)/(B+1)) # H0 that the expected ratio &gt; 1\n\n\n[1] 0.3992\n\n\nCode\n((sum(result &lt;= obs_ratio) + 1)/(B+1)) # H0 that the expected ratio &lt; 1\n\n\n[1] 0.6009"
  },
  {
    "objectID": "recitation/r40/index.html",
    "href": "recitation/r40/index.html",
    "title": "Confidence Interval Interpretations and Calculations",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r40/index.html#interpretation",
    "href": "recitation/r40/index.html#interpretation",
    "title": "Confidence Interval Interpretations and Calculations",
    "section": "Interpretation",
    "text": "Interpretation\nAs a reminder of the general interpretation of any confidence interval: The 95% confidence interval is (a, b). We are 95% confident that the true mean/difference/OR/RR/etc. is in this interval.\nStrictly speaking, in the null hypothesis significance testing (NHST) framework that underlies much of frequentist statistics, we cannot say that there is a 95% probability that the true value is in the interval. Instead, we must refer to confidence."
  },
  {
    "objectID": "recitation/r40/index.html#can-we-evaluate-significance",
    "href": "recitation/r40/index.html#can-we-evaluate-significance",
    "title": "Confidence Interval Interpretations and Calculations",
    "section": "Can We Evaluate Significance?",
    "text": "Can We Evaluate Significance?\nIn general, we can evaluate the 95% confidence interval to see if our \\(H_0\\) value is present in the interval. We generally have two conclusions:\n\nIf it is in the interval, we would fail to reject \\(H_0\\). We cannot conclude the null hypothesis is different.\nIf it isn’t in the interval, we would reject \\(H_0\\). We would conclude there is a significant difference from the null hypothesis.\n\nWhenever conducting an experiment we can set the \\(H_0\\) value to be anything, although it should be motivated by the clinical/scientific context. In practice, we usually have two “default” null values we frequently use:\n\nFor continuous outcomes, \\(H_0\\) is there is no difference (i.e., the difference is 0 or the mean value is 0, etc.)\nFor ratio outcomes (e.g., odds ratios, risk ratios), \\(H_0\\) is there is no difference (i.e., the OR or RR is 1)"
  },
  {
    "objectID": "recitation/r40/index.html#will-i-ever-see-a-p-value-not-match-the-confidence-interval",
    "href": "recitation/r40/index.html#will-i-ever-see-a-p-value-not-match-the-confidence-interval",
    "title": "Confidence Interval Interpretations and Calculations",
    "section": "Will I Ever See a p-value NOT Match the Confidence Interval?",
    "text": "Will I Ever See a p-value NOT Match the Confidence Interval?\nYes, it is very possible to see a mismatch between a p-value and the confidence interval.\nThis occurs because there are multiple ways we can derive p-values and corresponding intervals, and packages or our calculations will not always match. Some of the common derivation methods include:\n\nWald test and CI\nScore test and CI\nLikelihood Ratio and CI\n\nFor 6618 we won’t delve too deeply into these differences, but we should be aware they exist. For some additional details (and theory) check out\n\nFAQ: How are the likelihood ratio, Wald, and Lagrange multiplier (score) tests different and/or similar?\nPenn State Hypothesis Tests and Related Intervals\n\nIn practice, the derivations have some trade-offs. Ones that might seem overly simple may have lower coverage (i.e., the proportion of confidence intervals the include the true value for a simulation study) at the expense of its simplicity. Others that are more complex may involve calculations or derivations that, without being coded into an existing function/package, would be a little too tedious to implement. In practice we can use any variation of the calculations, and if we know it has a certain assumption for its derivation we can include that note in our analysis plan or write-up to be precise."
  },
  {
    "objectID": "recitation/r39/index.html",
    "href": "recitation/r39/index.html",
    "title": "What Are Some R Packages for Diagnostic Testing Calculations?",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\nNote, this recitation was written in 2022 and packages may have changed."
  },
  {
    "objectID": "recitation/r39/index.html#epir-package",
    "href": "recitation/r39/index.html#epir-package",
    "title": "What Are Some R Packages for Diagnostic Testing Calculations?",
    "section": "epiR Package",
    "text": "epiR Package\nI like the epiR package since it can quickly return summaries based on the provided 2x2 table with confidence intervals (epi.tests gives the diagnostic testing summaries, epi.2x2 provides the OR, RR, etc.):\n\n\nCode\nepiR::epi.tests(tab4)\n\n\n          Outcome +    Outcome -      Total\nTest +          152          241        393\nTest -           48          559        607\nTotal           200          800       1000\n\nPoint estimates and 95% CIs:\n--------------------------------------------------------------\nApparent prevalence *                  0.39 (0.36, 0.42)\nTrue prevalence *                      0.20 (0.18, 0.23)\nSensitivity *                          0.76 (0.69, 0.82)\nSpecificity *                          0.70 (0.67, 0.73)\nPositive predictive value *            0.39 (0.34, 0.44)\nNegative predictive value *            0.92 (0.90, 0.94)\nPositive likelihood ratio              2.52 (2.21, 2.88)\nNegative likelihood ratio              0.34 (0.27, 0.44)\nFalse T+ proportion for true D- *      0.30 (0.27, 0.33)\nFalse T- proportion for true D+ *      0.24 (0.18, 0.31)\nFalse T+ proportion for T+ *           0.61 (0.56, 0.66)\nFalse T- proportion for T- *           0.08 (0.06, 0.10)\nCorrectly classified proportion *      0.71 (0.68, 0.74)\n--------------------------------------------------------------\n* Exact CIs\n\n\nIf we investigate the names, we can also extract the specific estimates and confidence intervals of interest:\n\n\nCode\n# save epiR::epi.tests(tab4) as object\nresR &lt;- epiR::epi.tests(tab4)\n\n# check names for returned output\nnames(resR)\n\n\n[1] \"detail\"     \"tab\"        \"method\"     \"digits\"     \"conf.level\"\n\n\nCode\n# see output for detail\nresR$detail\n\n\n   statistic        est      lower      upper\n1         ap 0.39300000 0.36258124  0.4240507\n2         tp 0.20000000 0.17562057  0.2261594\n3         se 0.76000000 0.69469371  0.8174281\n4         sp 0.69875000 0.66564151  0.7303860\n5    diag.ac 0.71100000 0.68181223  0.7389396\n6    diag.or 7.34508990 5.13510725 10.5061770\n7       nndx 2.17983651 1.82543694  2.7751936\n8     youden 0.45875000 0.36033522  0.5478140\n9     pv.pos 0.38676845 0.33835951  0.4368964\n10    pv.neg 0.92092257 0.89652027  0.9411214\n11    lr.pos 2.52282158 2.21270687  2.8763994\n12    lr.neg 0.34347048 0.26728509  0.4413713\n13    p.rout 0.60700000 0.57594932  0.6374188\n14     p.rin 0.39300000 0.36258124  0.4240507\n15    p.tpdn 0.30125000 0.26961405  0.3343585\n16    p.tndp 0.24000000 0.18257190  0.3053063\n17    p.dntp 0.61323155 0.56310356  0.6616405\n18    p.dptn 0.07907743 0.05887859  0.1034797\n\n\nCode\n# check object type for detail\nclass( resR$detail )\n\n\n[1] \"data.frame\"\n\n\nCode\n# extract sensitivity\nresR$detail[ which(resR$detail$statistic == 'se'), ]\n\n\n  statistic  est     lower     upper\n3        se 0.76 0.6946937 0.8174281\n\n\nCode\n# Alternative ways we could extract sensitivity\n## Save resR$detail as its own object to avoid lots of $'s\nresR_detail &lt;- resR$detail\nresR_detail[ which(resR_detail$statistic == 'se'), ]\n\n\n  statistic  est     lower     upper\n3        se 0.76 0.6946937 0.8174281\n\n\nCode\n## Use square brackets to pull statistic from resR$detail\nresR$detail[ which(resR$detail[,'statistic'] == 'se'), ]\n\n\n  statistic  est     lower     upper\n3        se 0.76 0.6946937 0.8174281\n\n\nThe limitation of epi.tests is that the negative and positive predictive value summaries are based on the sample prevalence (the function output calls it the “true” prevalence: tprev). If we have a different population prevalence to use, we either have to manually extract the estimates for sensitivity and specificity to use in our formulas, or explore other functions.\nNOTE: epiR updated their package so that epi.tests() uses the above approach to extract our statistic(s) of interest. In older versions, you could use something along the lines of resR$rval to produce something like resR$detail. To update to the latest version of the package, you can use install.packages('epiR') or click through options in RStudio."
  },
  {
    "objectID": "recitation/r39/index.html#caret-package",
    "href": "recitation/r39/index.html#caret-package",
    "title": "What Are Some R Packages for Diagnostic Testing Calculations?",
    "section": "caret Package",
    "text": "caret Package\nThe caret package includes functions that can calculate our diagnostic summaries, while also including the population prevalence. There are also multiple ways to provide the data for the functions.\nFor example, to calculate our sensitivity we can either provide our 2x2 table or the vectors of our results. For the 2x2 table the rows and columns must have the same names, and for the vectors we still need to provide factors:\n\n\nCode\n# make table with same names for row and column\ntab4c &lt;- table(biom_gt4 = factor(diag_dat$biom&gt;4, levels=c(T,F), labels=c(1,0)), truth = factor(diag_dat$case, levels=c(1,0), labels=c(1,0)) )\n\ncaret::sensitivity(tab4c)\n\n\n[1] 0.76\n\n\nCode\n# feed in vectors of factor variables instead\ncaret::sensitivity(factor(diag_dat$biom&gt;4, levels=c(T,F), labels=c(1,0)), factor(diag_dat$case, levels=c(1,0), labels=c(1,0)))\n\n\n[1] 0.76\n\n\nFor NPV and PPV, we can specify the prevalence (or if we leave it blank it uses the sample estimate):\n\n\nCode\n# Use sample prevalence to compare to epiR output above\ncaret::posPredValue(tab4c, prevalence=NULL)\n\n\n[1] 0.3867684\n\n\nCode\n# Specify population prevalence of 10%\ncaret::posPredValue(tab4c, prevalence=0.1)\n\n\n[1] 0.2189413\n\n\nA downside to this approach, however, is there is no automatically provided confidence intervals. We either have to find a formula to use or implement a bootstrap resampling strategy to estimate a confidence interval."
  },
  {
    "objectID": "recitation/r37/index.html",
    "href": "recitation/r37/index.html",
    "title": "Diagnostic Testing: Likelihood Ratios, Odds, and Bayes’ Theorem",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r37/index.html#positive-and-negative-likelihood-ratios",
    "href": "recitation/r37/index.html#positive-and-negative-likelihood-ratios",
    "title": "Diagnostic Testing: Likelihood Ratios, Odds, and Bayes’ Theorem",
    "section": "Positive and Negative Likelihood Ratios",
    "text": "Positive and Negative Likelihood Ratios\nThe positive and negative likelihood ratios (LR+ and LR-) reflect if our test is a “good” test to use in practice. Additionally, these summaries do not depend on the prevalence. The measures summarize ratios of true and false positive or negative results:\n\nLR+: the number of true positive results per false positive result (large ratios are better), reflects the “strength” of a positive test\nLR-: the number of false negative results per true negative result (small ratios are better), reflects the “strength” of a negative test\n\nIn our lecture slides, we have the following calculation for LR+:\n\\[ \\text{LR+} = \\; \\frac{P(T|D)}{P(T|\\bar{D})} = \\frac{\\text{sensitivity}}{1-\\text{specificity}} = \\frac{\\text{TPR}}{\\text{FPR}}  \\]\nWe see from this, that we indeed would prefer a large ratio since it indicates a larger rate of true positive predictions as compared to false positive predictions. Similarly, for LR- we have:\n\\[ \\text{LR-} = \\; \\frac{P(\\bar{T}|D)}{P(\\bar{T}|\\bar{D})} = \\frac{1-\\text{sensitivity}}{\\text{specificity}} = \\frac{\\text{FNR}}{\\text{TNR}} \\]\nAgain, we can see that a smaller ratio is desired given the true negative rate is in the denominator, and a low false negative rate is preferred."
  },
  {
    "objectID": "recitation/r37/index.html#odds-versus-probabilities",
    "href": "recitation/r37/index.html#odds-versus-probabilities",
    "title": "Diagnostic Testing: Likelihood Ratios, Odds, and Bayes’ Theorem",
    "section": "Odds versus Probabilities",
    "text": "Odds versus Probabilities\nIn many cases we may still want to leverage the prevalence to calculate results reflective of a given population, especially if the prior test results had a special study design or were tested in a different population. We can leverage the likelihood ratios from above to incorporate our prior belief of the probability of having the outcome (i.e., the prevalence) through Bayes’ theorem.\n\nBayes’ Theorem and Our Connection to Odds\nIn general, Bayes’ theorem states:\n\\[ P(\\theta|X) = \\frac{P(X|\\theta)P(\\theta)}{P(X)} \\to \\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Data}} \\]\nIn the context of testing a hypothesis (\\(H\\)) for some data (\\(X\\)) this reflects:\n\\[ P(H|X) = \\frac{P(X|H) P(H)}{P(X)} \\]\nwhere we define each term as\n\n\\(P(H|X)\\) is the posterior (probability) that \\(H\\) is true after the data has been considered\n\\(P(X|H)\\) is the likelihood and represents the evidence for \\(H\\) provided by the observed data \\(X\\) (i.e., the probability of observing our data under the given hypothesis)\n\\(P(H)\\) is the prior (probability) that \\(H\\) is true before the data is considered\n\\(P(X)\\) is the total probability of the data which takes into account all possible hypotheses\n\nBayes’ theorem can also be rewritten in terms of odds if we have two specific outcomes that are mutually exclusive, such as \\(D\\) and \\(\\bar{D}\\) (i.e., either someone has or does not have the disease). This can be noted by writing out the ratio of the posteriors for having and not having the disease given our data:\n\\[ \\frac{P(D|X)}{P(\\bar{D}|X)} = \\frac{\\frac{P(X|D) P(D)}{P(X)}}{\\frac{P(X|\\bar{D}) P(\\bar{D})}{P(X)}} = \\frac{P(X|D) P(D)}{P(X|\\bar{D}) P(\\bar{D})} \\]\nSince each person either has or does not have the disease, we know that \\(P(\\bar{D}) = 1-P(D)\\), further rewriting our equation to\n\\[ \\frac{P(D|X)}{P(\\bar{D}|X)}  = \\frac{P(X|D) P(D)}{P(X|\\bar{D}) (1 - P(D))} = \\frac{P(D)}{1-P(D)} \\times \\frac{P(X|D)}{P(X|\\bar{D})} \\]\nOur missing piece for our context of diagnostic testing is to identify what our data (\\(X\\)) is. In our current context, it reflects the outcome of the diagnostic test (\\(T\\) or \\(\\bar{T}\\) depending on the question we are trying to answer):\n\\[ \\frac{P(D|T)}{P(\\bar{D}|T)} =  \\frac{P(D)}{1-P(D)} \\times \\frac{P(T|D)}{P(T|\\bar{D})} = \\frac{P(D)}{1-P(D)} \\times LR+  \\]\nWe now see that we have the positive likelihood ratio! This is where we can draw a direct connection to the odds of an event, which are defined as\n\\[ O(D) = \\frac{P(D)}{1-P(D)} \\]\nSince \\(P(D)\\) is our prior belief that someone has the outcome, \\(O(D)\\) represents the prior odds that someone has the outcome.\nIf we were interested in the perspective of not having the outcome for those with a negative test, we similarly have a ratio of probabilities that lead to our calculation of the posterior odds of \\(\\bar{D}\\):\n\\[ \\frac{P(\\bar{D}|\\bar{T})}{P(D|\\bar{T})} = \\frac{P(\\bar{T}|\\bar{D}) P(\\bar{D})}{P(\\bar{T}|D) P(D)} = \\frac{P(\\bar{D})}{P(D)} \\times \\frac{P(\\bar{T}|\\bar{D})}{P(\\bar{T}|D)} = \\frac{1-P(D)}{P(D)} \\times \\frac{1}{LR-}   \\]\nHere we see the negative likelihood ratio, but we have to take its inverse based on the rewritten equation. Our relationship to the odds for not having the outcome of interest is similar to that above:\n\\[ O(\\bar{D}) = \\frac{P(\\bar{D})}{1 - P(\\bar{D})} = \\frac{1-P(D)}{P(D)} \\]\n\n\nOdds, What Are You?\nAs we just saw, odds and probabilities have a direct connection. I personally am not a huge fan of odds because I just do not naturally think in terms of odds. For example,\n\nOdds range from 0 to \\(\\infty\\) and represent a ratio of outcomes\nProbabilities range from 0 to 1 and tend to be more intuitively understood\n\nFortunately, we can rework our odds and probability equations above to solve for the probabilities:\n\\[ O(D) = \\frac{P(D)}{1-P(D)} \\to P(D) = \\frac{O(D)}{1+O(D)} \\]\nFor example, if we are given the the posterior odds of having the outcome is 2 and the corresponding posterior probability is 67% with a 20% population prevalence, we have the following interpretations:\n\nAfter observing a positive test result, our odds in favor of having the outcome are 2:1.\nThe posterior probability of having the outcome is 67% (a change from our prior probability (i.e., prevalence) of 20%).\n\nThe odds of 2:1 indicate that out of three positive test outcomes, two of the three participants will actually have the outcome and one person will have been misclassified as having the outcome when they truly do not. In other words, the false positive will occur once for every two times we have a true positive. I personally think the posterior probability is easier to imagine, that if your test is positive you have a 67% probability of having the outcome of interest."
  },
  {
    "objectID": "recitation/r35/index.html",
    "href": "recitation/r35/index.html",
    "title": "How Are t.test and power.t.test Different?",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nComparison of t.test and power.t.test\nIn Power and Type I Error Rate Calculations via Simulation Studies we conducted a power calculation for a given combination of assumptions. In the case of the unknown SD we used power.t.test to enter arguments related to the sample size (n), detectable difference (delta), standard deviation (sd), type I error rate (sig.level), and power (power), with whatever argument was NULL being solved for.\nOn the other hand, once we have collected (or simulated) data to analyze, then we can implement the actual t-test (e.g., using t.test). Since we only have a single sample we can only estimate the mean and SD from our sample of \\(n\\) and evaluate the one-sample t-test results like a p-value for statistical significance.\nHowever, in a single experiment or single simulated data set, we don’t know if our statistical result is a true positive, false positive, true negative, or false negative. In other words, we can’t estimate \\(\\alpha\\) or \\(\\beta\\) which were assumed for our initial power calculation. This is because in the NHST (null hypothesis significance testing) approach, \\(\\alpha\\) and \\(\\beta\\) can only be estimated from repeated experiments, which we know in practice hardly happens.\nTo summarize, we only use power.t.test if we trying to design a future experiment. In practice, once we have data we will use t.test to actually compare the one-sample mean to some defined null value (or compare two samples to each other, paired samples, etc.). If we wanted to use simulation to conduct our power calculations, then we will use t.test applied to a “large” number of simulated data sets to see the performance based on the proportion of times we reject or fail to reject our null hypothesis."
  },
  {
    "objectID": "recitation/r33/index.html",
    "href": "recitation/r33/index.html",
    "title": "Power and Type I Error with Comparison of Known and Unknown One-Sample Mean Calculations",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r33/index.html#relationship-of-type-i-error-type-ii-error-alpha-beta",
    "href": "recitation/r33/index.html#relationship-of-type-i-error-type-ii-error-alpha-beta",
    "title": "Power and Type I Error with Comparison of Known and Unknown One-Sample Mean Calculations",
    "section": "Relationship of Type I Error, Type II Error, \\(\\alpha\\), \\(\\beta\\)",
    "text": "Relationship of Type I Error, Type II Error, \\(\\alpha\\), \\(\\beta\\)\n\n\n\n\n\n\n\n\nReality → What we decide ↓\n\\(H_0\\) True\n\\(H_0\\) False/\\(H_1\\) True\n\n\n\n\nFail to reject \\(H_0\\)\nCorrectProbability of correct decision =\\(1-\\alpha\\) = level of confidence\nType II ErrorP(Type II Error)=\\(\\beta\\)\n\n\nReject \\(H_0\\)\nType I ErrorP(Type I Error) = \\(\\alpha\\)(level of significance)\nCorrectProbability of correct decision =\\(1-\\beta\\) = Power\n\n\n\nOur 2x2 table summarizes the unknown reality of our hypothesis test and the conclusions we make based on a sample we collect to run the hypothesis test on. We see that there are four possible outcomes, two of which are correct and two of which represent erroneous conclusions. It is these erroneous conclusions that lead to Type I and Type II Errors, which are the probability of a false positive or false negative, respectively."
  },
  {
    "objectID": "recitation/r33/index.html#known-vs.-unknown-sd-for-power-calculations",
    "href": "recitation/r33/index.html#known-vs.-unknown-sd-for-power-calculations",
    "title": "Power and Type I Error with Comparison of Known and Unknown One-Sample Mean Calculations",
    "section": "Known vs. Unknown SD for Power Calculations",
    "text": "Known vs. Unknown SD for Power Calculations\nWhen we make the assumption that the standard deviation is known or unknown for a power calculation, we end up two different conclusions as to the possible power, needed sample size, the difference we can detect, etc. This is because the assumption of known standard deviation allows us to use the normal distribution for our power calculation, whereas the unknown standard deviation implies we have to estimate the SD from our sample and we end up using the t-distribution.\nThe reason for the distributional change is that we need to account for the additional uncertainty introduced by estimating the SD. If we treat the SD as known then it is a constant value (i.e., whatever we “know” it to be). If the SD is unknown, we know the sampling distribution is \\(\\chi^2_{n-1}\\) (where \\(n-1\\) is our degrees of freedom).\n\nSample Size Example - Known SD\nA new treatment for celiac disease, an autoimmune condition where people cannot eat wheat, barley, or rye due to the body perceiving it as an immune threat, has been developed. Previously the only “treatment” for celiac disease was a strict adherence to a gluten-free diet, but a researcher believes the new treatment will allow individuals to eat foods that may have been cross-contaminated with gluten without activating their immune system and destroying their small intestine.\nOne way to measure the effectiveness of the treatment is examine the change in the level of antibodies in a participant’s blood after exposure to cross-contaminated meals. One such antibody test is the Tissue Transglutaminase IgA antibody (tTG-IgA) test. The researcher desires 83% power to detect a change of 5 U/mL (units per milliliter) with \\(\\alpha=0.05\\) and \\(\\sigma=5\\).\nLet’s first calculate the known SD case: \\[ n = \\frac{\\sigma^2 \\left( Z_{1-\\beta} + Z_{1-\\frac{\\alpha}{2}} \\right)^2}{(\\mu_0 - \\mu_1)^2} \\]\nWe know from our problem that we need to calculate \\(Z_{1-\\beta} = Z_{0.83}\\) and \\(Z_{1-\\frac{\\alpha}{2}} = Z_{0.975}\\). We can do this in R with the qnorm function:\n\n\nCode\nqnorm(0.83) # the value of \"Z\" where its cumulative probability under a standard normal distribution is 0.83\n\n\n[1] 0.9541653\n\n\nCode\nqnorm(0.975) # similarly, the value of \"Z\" where the CDF indicates 0.975 area under the curve\n\n\n[1] 1.959964\n\n\n\\(\\begin{aligned} n =& \\; \\frac{\\sigma^2 \\left( Z_{1-\\beta} + Z_{1-\\frac{\\alpha}{2}} \\right)^2}{(\\mu_0 - \\mu_1)^2} \\\\ =& \\; \\frac{5^2(Z_{0.83} + Z_{0.975})^2}{5^2} \\\\ =& \\; \\frac{5^2 (0.95 + 1.96)^2}{25} \\\\ =& \\; 8.4681 \\end{aligned}\\)\nWhen the SD is known we would need to enroll 9 participants (remember we need to round up, especially if our investigator wants at least 83% power).\n\nVerifying the Known SD Power\nLet’s graphically compare our distribution of the null and alternative distribution for \\(\\bar{X}\\) under the known SD assumption first. Recall, \\(\\bar{X} \\sim N(\\mu,\\frac{\\sigma^2}{n})\\), so for our sample size calculation we have \\(\\bar{X} \\sim N(\\mu,\\frac{25}{9})\\).\n\n\n\n\n\nUnder \\(H_0\\) we know that we would need to enroll 9 participants to achieve at least 83% power. We can calculate what this corresponds to by doing some algebra with our standard normal \\(Z\\): \\[ Z_{0.975} = 1.96 = \\frac{\\bar{x}-\\mu}{\\sigma / \\sqrt{n}} = \\frac{\\bar{x}-0}{5 /\\sqrt{9}} \\implies \\bar{x} = 1.96 \\frac{5}{\\sqrt{9}} \\approx 3.27  \\]\nIf we didn’t want to have the time of our lives doing algebra, we could also leverage the qnorm() function by specifying our given mean and SD:\n\n\nCode\nqnorm(0.975, mean=0, sd=sqrt(25/9))\n\n\n[1] 3.266607\n\n\nIf we add that information to our figure, we can see that the area under our curve for \\(H_1\\) is at least 83%:\n\n\n\n\n\nAs we noted on the plot, we achieve 85% power with \\(n=9\\), \\(\\sigma=5\\), \\(\\alpha=0.05\\), and a detectable difference of 5:\n\n\nCode\n1 - pnorm(3.27, mean=5, sd=5/3) # upper AUC for H1\n\n\n[1] 0.850365\n\n\nCode\npnorm(3.27, mean=5, sd=5/3, lower.tail=F) # upper AUC for H1 specified with lower.tail argument\n\n\n[1] 0.850365\n\n\nIf we wanted to be more precise with our impractical \\(n=8.4681\\), we can see we do achieve exactly 83% power:\n\n\nCode\nspecific_n &lt;- (qnorm(0.83)+qnorm(0.975))^2 # from our \"by hand\" formula\n1 - pnorm(qnorm(0.975, mean=0, sd=sqrt(25/specific_n)), mean=5, sd=5/sqrt(specific_n) )\n\n\n[1] 0.83\n\n\n\n\nCompare Results to NHST Visualization\nWe can also use the nifty NHST visualization by Kristoffer Magnusson: https://rpsychologist.com/d3/nhst/. The big difference with the visualization tool is that Cohen’s \\(d\\) is used instead of specifying the mean and standard deviation separately:\n\\[ d = \\frac{\\mu - \\mu_0}{\\sigma} = \\frac{5 - 0}{5} = 1  \\]\nWe can confirm our power is 85% in the visualization by setting \\(d=1\\) and our other parameters.\n\n\n\nSample Size Example - Unknown SD\nNext, let’s see what happens for the unknown SD case:\n\n\nCode\npower.t.test(n=NULL, delta=5, sd=5, sig.level=0.05, power=0.83, type='one.sample')\n\n\n\n     One-sample t test power calculation \n\n              n = 10.57855\n          delta = 5\n             sd = 5\n      sig.level = 0.05\n          power = 0.83\n    alternative = two.sided\n\n\nWhen we treat the SD as unknown we now need to enroll 11 participants.\n\nBut Why Does This Happen?\nLet’s break down a bit more about why the normal distribution and t-distribution end up with different estimates. When we assume the SD is unknown we now have to estimate it from the data we collect. In the context of our power calculations, it reflects a greater level of uncertainty as to what we are planning for. We can visually see this uncertainty by adding the t-distribution to our \\(H_0\\) figure:\n\n\n\n\n\nWe can see that as the degrees of freedom increase, the t-distribution gets closer in shape to our normal distribution. This apparent change is notable especially when comparing \\(df=1\\) to \\(df=10\\), where smaller degrees of freedom lead to the t-distribution decreasing the peak and increasing the fattiness of the tails. This will have a direct effect if we didn’t change any of our calculations relative to the standard normal distribution.\nFor example, if we look at the overlap of our two t-distributions under \\(H_0\\) and \\(H_1\\) while keeping the known SD critical values from above with our \\(n=11\\) from the power calculation (where \\(\\frac{s}{\\sqrt{n}}=\\frac{5}{\\sqrt{11}}=1.51\\)):\n\n\n\n\n\nWe can calculate this via the metRology package’s pt.scaled function:\n\n\nCode\nlibrary(metRology)\n1 - pt.scaled(3.27, df=10, mean=5, sd=5/sqrt(11))\n\n\n[1] 0.8610669\n\n\nIn actuality, since are assuming an unknown SD, we should recalculate the critical value appropriate for this context. Doing so below, with the qt.scaled() function, and applying it to our estimate of power is a little closer to the targeted 83% power:\n\n\nCode\nqt.scaled(0.975, df=11-1, mean=0, sd=5/sqrt(11)) # calculate the critical value based on our unknown SD\n\n\n[1] 3.359046\n\n\nCode\n1 - pt.scaled(3.36, df=11-1, mean=5, sd=5/sqrt(11)) # calculate power under critical value\n\n\n[1] 0.8489082\n\n\nMore precisely, enrolling \\(n=10.57855\\) achieves approximately 83% power:\n\n\nCode\nn &lt;- 10.57855 # use n from power.t.test above\ncrit_tdist_n &lt;- qt.scaled(0.975, df=n-1, mean=0, sd=5/sqrt(n)) # calculate the critical value based on our unknown SD\ncrit_tdist_n # critical value to compare to 3.36 above\n\n\n[1] 3.445844\n\n\nCode\n1 - pt.scaled(crit_tdist_n, df=n-1, mean=5, sd=5/sqrt(n)) # calculate power under critical value\n\n\n[1] 0.8315566\n\n\nAgain, however, in practice we round up to \\(n=11\\) to ensure at least 83% power is achieved (and because we are unable to enroll fractions of an event or participant)."
  },
  {
    "objectID": "recitation/r31/index.html",
    "href": "recitation/r31/index.html",
    "title": "Sampling Distributions of the Mean, Median, and Variance",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r31/index.html#simulated-data-to-plot-histograms-of-sampling-distributions-of-the-mean-median-and-variance",
    "href": "recitation/r31/index.html#simulated-data-to-plot-histograms-of-sampling-distributions-of-the-mean-median-and-variance",
    "title": "Sampling Distributions of the Mean, Median, and Variance",
    "section": "Simulated Data to Plot Histograms of Sampling Distributions of the Mean, Median, and Variance",
    "text": "Simulated Data to Plot Histograms of Sampling Distributions of the Mean, Median, and Variance\nFor a population that is normally distributed with mean 40 and standard deviation 10, generate histograms showing the sampling distribution of the mean, median, and variance. Use 1,000 simulation iterations that each have a sample size of n = 10.\nSolution:\n\n\nCode\nset.seed(515) # set seed for reproducibility\n\nnsim &lt;- 1000 # set number of simulations\nn &lt;- 10 # set sample size\n\n# initialize 3 vectors to store our results for the sample mean, sample median, and sample variance\nmeanV &lt;- rep(NA, 1000)\nmedianV &lt;- rep(NA, 1000)\nvarV &lt;- rep(NA, 1000)\n\n# loop through our nsim simulations to save the sample summary statistics\nfor(i in 1:nsim){\n  random &lt;- rnorm(n, mean = 40, sd = 10)\n  meanV[i] &lt;- mean(random)\n  medianV[i] &lt;- median(random)\n  varV[i] &lt;- var(random)\n}\n\n# create a panel of our 3 histograms\npar( mfrow=c(1,3) )\nhist(meanV)\nhist(medianV)\nhist(varV)"
  },
  {
    "objectID": "recitation/r31/index.html#distributions-of-the-sampling-distribution",
    "href": "recitation/r31/index.html#distributions-of-the-sampling-distribution",
    "title": "Sampling Distributions of the Mean, Median, and Variance",
    "section": "Distributions of the Sampling Distribution",
    "text": "Distributions of the Sampling Distribution\nBased on theory and/or your plots from 3a, what is the distribution of the sample mean and sample median in this case (e.g., uniform, exponential, gamma, normal, etc.)?\nSolution:\nThe sample mean and median are both normally distribution for data simulated from a normal distribution. This can be seen in the plots for the mean and median where the histograms both look approximately normal. Theoretically, the sample mean is normally distributed by the central limit theorem.\nThe sample median is a bit trickier. In a 1955 paper, Dr. Chu\\(^1\\) notes the distribution for a sample median “tends rapidly to normality”. A post on Cross Validated walks through some more detailed answers where the sample median’s distribution is normal given various assumptions that must be met.\n\\(^1\\)Chu, John T. “On the distribution of the sample median.” The Annals of Mathematical Statistics (1955): 112-116."
  },
  {
    "objectID": "recitation/r31/index.html#validating-the-sampling-distribution-of-the-variance",
    "href": "recitation/r31/index.html#validating-the-sampling-distribution-of-the-variance",
    "title": "Sampling Distributions of the Mean, Median, and Variance",
    "section": "Validating the Sampling Distribution of the Variance",
    "text": "Validating the Sampling Distribution of the Variance\nWhen the underlying population is normally distributed, the sampling distribution of \\(\\frac{(n - 1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\) (i.e., it is chi-squared with \\(n-1\\) degrees of freedom).\nValidate this theoretical sampling distribution by plotting the histogram of simulation results from part a for the 1,000 sample variances with the theoretical distribution.\nMore hints: It may be helpful to note that in part a we generated a vector of sampling variances, but the theoretical distribution is also involves \\((n-1)\\) and \\(\\sigma^2\\) (i.e., the sample size minus 1 and the “true” variance). So we may need to multiply the vector from part a by something to match the scales. Additionally, you should use the probability=TRUE option if you are using the hist() function to plot the histogram since the default is to plot the y-axis with respect to the frequency instead of probability.\nSolution:\nThe chi-squared distribution is a continuous distribution with one parameter (\\(k\\), also known as its degrees of freedom). The shape of the distribution is controlled entirely by the value of \\(k\\) we provide, and in R we can plot its probability density function (PDF) using dchisq(x, df), where x is the values we wish to calculate the height (i.e., density) of the PDF and df is our degrees of freedom.\nOne way to overlay the probability density function is to use functions such as curve( dchisq(x, df=9), col='green', lwd=2, add=T). However, this specific density curve is for \\(\\chi^2_{n-1}\\), which is the sampling distribution of \\(\\frac{(n - 1)s^2}{\\sigma^2}\\).\nHowever, in 3a we only calculated the sample variance, \\(s^2\\). So if we try to add the density curve to our plot from 3a we have:\n\n\nCode\nhist(varV, main='Histogram of variances', probability=TRUE) # set probability=T for density instead of frequency\ncurve( dchisq(x, df=9), col='green', lwd=2, add=T)\n\n\n\n\n\nWe can tell that the chi-squared curve we added to the histogram is not a match for just \\(s^2\\). In other words, \\(s^2 \\nsim \\chi^2_{n-1}\\) (i.e., \\(s^2\\) is not distributed as a \\(chi^{2}_{n-1}\\) distribution).\nInstead, we need to multiply our sample variances from 3a by \\(\\frac{n-1}{\\sigma^2}\\) to match the given sampling distribution:\n\n\nCode\nvarVshift &lt;- varV * (9/10^2) # multiply varV, o.w. x-axis range is diff.\nhist(varVshift, main='Histogram of variances', probability=TRUE) \ncurve( dchisq(x, df=9), col='green', lwd=2, add=T)\n\n\n\n\n\nBy plotting a histogram of \\(\\frac{(n-1)s^2}{\\sigma^2}\\) we see that the sampling distribution does match a \\(\\chi^{2}_{n-1}\\).\n\nPlot with Mathematical Labels\nPerhaps we wanted to be very explicit about what our x-axis is, but writing “(n-1)s^2 / sigma^2” doesn’t quite scratch that itch. We can use expression() to change the x-axis to mathematically represent our equation:\n\n\nCode\nhist(varVshift, main='Histogram of sampling distribution for sample variances', probability=TRUE, xlab='')\nmtext(text = expression(frac((n - 1) ~s^2, sigma^2)),\n      side = 1, # place it on bottom of figure\n      line = 3.5) # change position of label to avoid overlapping x-axis\ncurve( dchisq(x, df=9), col='green', lwd=2, add=T)\n\n\n\n\n\n\n\nPlot with ggplot\nWe can also make a version using ggplot by using the stat_function() function:\n\n\nCode\nlibrary(ggplot2)\n\nggplot() + \n  geom_histogram(aes(x = varVshift, y = ..density..), # ..density.. specifies we want the density on the y-axis instead of a count\n                 fill='gray', \n                 colour='black',\n                 breaks = seq(0,28,2)) + # specified to better match the base R version\n  stat_function(fun = dchisq, # function to apply and plot\n                args = list(df=9), # additional argument to pass along (e.g., the df for our chi-squared dist.)\n                xlim = c(0,28), # I added this range so the line would go over the whole histogram, otherwise it stops at the max X value (i.e., I didn't like how the line without this argument stopped mid-histogram bin)\n                color = 'green', \n                size=1) # makes the line a little thicker\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nAs an aesthetic bonus, if you wanted to change the style of your ggplot you can choose different themes. For instance, if you wanted to better replicate base R’s minimalistic aesthetic, we could add theme_classic():\n\n\nCode\nlibrary(ggplot2)\n\nggplot() + \n  geom_histogram(aes(x = varVshift, y = ..density..), fill='gray', colour='black', breaks = seq(0,28,2)) + \n  stat_function(fun = dchisq, args = list(df=9), xlim = c(0,28), color = 'green', size=1) + \n  theme_classic() # creates a white background with no lines"
  },
  {
    "objectID": "recitation/r3/index.html",
    "href": "recitation/r3/index.html",
    "title": "Population Moments",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nPopulation Moments\nIn statistics we are often interested in estimating the average of some random variable and a summary of its variability. Oftentimes, these are represented by the mean and variance (or standard deviation). However, there are other measures that can describe the shape of a distribution, such as skewness and kurtosis.\nFortunately, these all exist under a unifying framework of the moments of a function. However, to be more useful we may modify the raw moments to be either central moments (e.g., variance) or standardized moments (e.g., skewness and kurtosis).\nLet’s see a summary of these measures before diving a little deeper into skewness and kurtosis:\n\n\n\n\n\n\n\n\n\n\nMoment\nSummary\nType of Moment\nFormula\nGeneral Description\n\n\n\n\n1st\nMean\nRaw\n\\(\\mu = E(X)\\)\nLocation of distribution\n\n\n2nd\nVariance\nCentral\n\\(\\sigma^2 = E[(X-\\mu)^2]\\)\nVariability of distribution\n\n\n3rd\nSkewness\nStandardized\n\\(\\gamma_1 = E\\left[\\left( \\frac{X-\\mu}{\\sigma} \\right)^3 \\right]\\)\nSymmetry of distribution\n\n\n4th\nKurtosis\nStandardized\n\\(\\text{Kurt}[X] = E\\left[\\left( \\frac{X-\\mu}{\\sigma} \\right)^4 \\right]\\)\nTailedness of distribution\n\n\n\n\n\nSkewness\nSkewness is a measure of the symmetry of a distribution. Symmetric distributions like the normal, \\(t\\), and uniform, have no skewness. However, distributions may be skewed to the right (i.e., positively skewed) where there is a greater probability of observing larger values of \\(x\\) or to the left (i.e., negatively skewed) where there is a greater probability of observing smaller values of \\(x\\).\nSkewness may be important when chosing the appropriate summary measure. The mean, median, and mode are all equivalent with non-skewed distributions (an exception being the uniform, where any value in the sample space could be the mode). However, the mean and median will likely not match when skewness is present.\n\n\nKurtosis\nKurtosis is a measure of the tailedness of a probability distribution. A higher value of kurtosis leads to more extreme outliers or deviations relative to a distribution with lower kurtosis.\nOftentimes we are interested in describing the excess kurtosis of a distribution. To do this we have to define a gold standard, which in practice is the univariate normal distribution (whose excess kurtosis is equal to 0, also known as mesokurtic).\nA platykurtic distribution has negative excess kurtosis (i.e., there is lower probability of having extreme outliers/deviations in our tails). A leptokurtic distribution has heavier/fatter tails than a normal distribution and a greater chance of observing more extreme values.\nSome examples are presented below for each form of excess kurtosis:\n\n\nCode\nset.seed(914) # set seed for reproducibility\n\nmeso &lt;- rnorm(n=10000)\nlept &lt;- rt(n=10000, df=3)\nplat &lt;- runif(n=10000, min=-3, max=3)\n\n# estimate ranges to set breaks to be consistent for all distributions\nmin_range &lt;- floor( min(c(meso,lept,plat)) ) # round down to closest integer\nmax_range &lt;- ceiling( max(c(meso,lept,plat)) ) # round up to closest integer\n\npar(mfrow=c(3,1)) # panel figure with 3 rows, 1 column\nhist(meso, main='Mesokurtic (Normal)', xlab='x', ylab='Density', xlim=c(-4,4), breaks=seq(min_range,max_range,by=0.2), prob=T)\nhist(lept, main='Leptokurtic (t)', xlab='x', ylab='Density', xlim=c(-4,4), breaks=seq(min_range,max_range,by=0.2), prob=T)\nhist(plat, main='Platykurtic (Uniform)', xlab='x', ylab='Density', xlim=c(-4,4), breaks=seq(min_range,max_range,by=0.2), prob=T)"
  },
  {
    "objectID": "recitation/r28/index.html",
    "href": "recitation/r28/index.html",
    "title": "Brief, but Complete, Interpretations: A t-test Example",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r28/index.html#independent-two-samples-t-test",
    "href": "recitation/r28/index.html#independent-two-samples-t-test",
    "title": "Brief, but Complete, Interpretations: A t-test Example",
    "section": "Independent Two-Samples t-test",
    "text": "Independent Two-Samples t-test\n\n\nCode\ncontrol_group &lt;- c(1.0,0.5,0.3,0.0,-0.2,-1.2)\ntreatment_group &lt;- c(0.2,-0.4,-1.4,-1.6,-2.6,-3.2)\n  \nt.test( x=treatment_group, y=control_group, \n        alternative='two.sided',  #can also choose one-sided\n        paired=FALSE, #if set to TRUE, does a paired t-test\n        var.equal=FALSE ) #if TRUE, assumes equal variances\n\n\n\n    Welch Two Sample t-test\n\ndata:  treatment_group and control_group\nt = -2.5857, df = 8.0464, p-value = 0.03218\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.9624658 -0.1708676\nsample estimates:\n  mean of x   mean of y \n-1.50000000  0.06666667 \n\n\nBased on these results, the four elements of our brief, but complete, interpretation may include:\n\nA point estimate: \\(-1.5 - (0.067) = -1.567 = -1.57\\)\nAn interval estimate: \\((-2.96, -0.17)\\)\nA measure of uncertainty in the decision: \\(p=0.032\\)\nA decision: We reject the null hypothesis since p&lt;0.05 and the 95% confidence interval does not include 0 (i.e., our null)\n\nThe mean weight loss over six weeks in the treatment group was 1.57 kg {point estimate} (95% CI: 0.17 to 2.96 kg) {interval estimate} greater than in the control group, representing a significant difference {decision} (p=0.032) {uncertainty}."
  },
  {
    "objectID": "recitation/r28/index.html#paired-t-test",
    "href": "recitation/r28/index.html#paired-t-test",
    "title": "Brief, but Complete, Interpretations: A t-test Example",
    "section": "Paired t-test",
    "text": "Paired t-test\nLet’s use the same data, but assume that we designed our study to match each control and treatment participant so we could conduct a paired analysis. In this case, we can simply change the paired=FALSE argument in our t.test() function to paired=TRUE:\n\n\nCode\ncontrol_group &lt;- c(1.0,0.5,0.3,0.0,-0.2,-1.2)\ntreatment_group &lt;- c(0.2,-0.4,-1.4,-1.6,-2.6,-3.2)\n  \nt.test( x=treatment_group, y=control_group, \n        alternative='two.sided',  #can also choose one-sided\n        paired=TRUE, #set equal to TRUE, assuming each person's measurement is \n        var.equal=FALSE ) #if TRUE, assumes equal variances\n\n\n\n    Paired t-test\n\ndata:  treatment_group and control_group\nt = -6.1714, df = 5, p-value = 0.001627\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.2192323 -0.9141011\nsample estimates:\nmean difference \n      -1.566667 \n\n\nBased on these results, the four elements of our brief, but complete, interpretation may include:\n\nA point estimate: \\(-1.567 = -1.57\\)\nAn interval estimate: \\((-2.22, -0.91)\\)\nA measure of uncertainty in the decision: \\(p=0.002\\) (rounding to 3 decimal places)\nA decision: We reject the null hypothesis since p&lt;0.05 and the 95% confidence interval does not include 0 (i.e., our null)\n\nThe mean weight loss in our paired sample over six weeks in the treatment group was 1.57 kg {point estimate} (95% CI: 0.91 to 2.22 kg) {interval estimate} greater than in the control group, representing a significant difference {decision} (p=0.002) {uncertainty}.\nNOTE: We are assuming, based on our data entry, that the 1st, 2nd, etc. element is our desired match between the vectors."
  },
  {
    "objectID": "recitation/r26/index.html",
    "href": "recitation/r26/index.html",
    "title": "Approaches to Generating Data for a Simulation: for loops and apply Functions",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r26/index.html#for-loops-with-vectors",
    "href": "recitation/r26/index.html#for-loops-with-vectors",
    "title": "Approaches to Generating Data for a Simulation: for loops and apply Functions",
    "section": "for loops with vectors",
    "text": "for loops with vectors\nLet’s start by walking through an example where we use a for loop to loop through our sample sizes.\n\n\nCode\n# Step 1: Set the seed for reproducibility\nset.seed(515)\n\n# Step 2: Define objects of interest\nsample_size &lt;- seq(from=100, to=50000, by=100) # generate vector from 100 to 100,000 in increments of 100 using the seq function\nlambda &lt;- 3 # set lambda for our exponential distribution to be easily updated if we wanted to look at a different scenario\n\n# Step 3: Initialize (i.e., create) vector or other object to save our results\nbias_median &lt;- rep(NA, times=length(sample_size))\n\n# Step 4: Loop through a chunk of code to calculate our bias\nfor(i in 1:length(sample_size)){\n  \n  sim &lt;- rexp(n=sample_size[i], rate=lambda) # simulate a sample of sample_size[i] with rate=lambda, where [i] is pulling the \"ith\" element from our sample size vector sample_size\n  \n  median_sim &lt;- median(sim) # calculate the median of each sim\n  \n  bias &lt;- median_sim - (1/lambda) # calculate the bias of the median in estimating the mean\n  \n  bias_median[i] &lt;- bias # save our results\n  \n}\n\nbias_median[c(1:3,498:500)] # print the first and last 3 bias estimates\n\n\n[1] -0.1122742 -0.1141976 -0.1129037 -0.1055317 -0.1051049 -0.1012018\n\n\nWe can then create a plot of our resulting bias estimates over the different sample sizes:\n\n\nCode\nplot(x=sample_size, y=bias_median, xlab='Sample Size', ylab='Bias Estimate of the Median', type='l')\n\n\n\n\n\nGiven that we know that the mean is not equal to the median for the exponential distribution, we shouldn’t be too surprised to see that the median would be a biased estimator to use in place of the median! However, even if we did not know the true theoretical mean and median for the distribution, this simulation gives us evidence that they are not the same.\nWhile still biased, we can note that the estimator is still consistent as the sample size increases (i.e., the lines seems to be converging to a consistent estimate, such as \\(\\frac{\\ln(2)}{\\lambda} - \\frac{1}{\\lambda} = \\frac{\\ln(2) - 1}{\\lambda}\\), which in our case is \\(\\frac{\\ln(2) - 1}{3}= -0.102\\)."
  },
  {
    "objectID": "recitation/r26/index.html#for-loops-with-a-matrix",
    "href": "recitation/r26/index.html#for-loops-with-a-matrix",
    "title": "Approaches to Generating Data for a Simulation: for loops and apply Functions",
    "section": "for loops with a matrix",
    "text": "for loops with a matrix\nLet’s check out an example where we also save the sample size and median estimate from each simulated sample size by using a matrix:\n\n\nCode\n# Step 1: Set the seed for reproducibility\nset.seed(515)\n\n# Step 2: Define objects of interest\nsample_size &lt;- seq(from=100, to=50000, by=100) # generate vector from 100 to 100,000 in increments of 100 using the seq function\nlambda &lt;- 3 # set lambda for our exponential distribution to be easily updated if we wanted to look at a different scenario\n\n# Step 3: Instead of saving the bias, let's also save the median for each simulation by using a matrix\nbias_median_mat &lt;- matrix(nrow=length(sample_size), ncol=3) # could also specify names for the rows and/or columns\n\n# Step 4: Loop through a chunk of code to calculate our bias\nfor(i in 1:length(sample_size)){\n  \n  sim &lt;- rexp(n=sample_size[i], rate=lambda) # simulate a sample of sample_size[i] with rate=lambda, where [i] is pulling the \"ith\" element from our sample size vector sample_size\n  \n  median_sim &lt;- median(sim) # calculate the median of each sim\n  \n  bias &lt;- median_sim - (1/lambda) # calculate the bias of the median in estimating the mean\n  \n  bias_median_mat[i,] &lt;- c(sample_size[i], bias, median_sim) # save our results for sample size (1st column of matrix), bias (second column), and median (3rd column)\n  \n}\n\n# Let's view the first 3 and last 3 rows of the matrix and plot the estimates\nbias_median_mat[c(1:3, 498:500),]\n\n\n      [,1]       [,2]      [,3]\n[1,]   100 -0.1122742 0.2210592\n[2,]   200 -0.1141976 0.2191358\n[3,]   300 -0.1129037 0.2204296\n[4,] 49800 -0.1055317 0.2278016\n[5,] 49900 -0.1051049 0.2282284\n[6,] 50000 -0.1012018 0.2321315\n\n\nCode\nplot(x=bias_median_mat[,1], y=bias_median_mat[,2], xlab='Sample Size', ylab='Bias Estimate of the Median', type='l')\n\n\n\n\n\nNotice how the bias estimates and the figure are identical for our approach saving the results in a vector since we set the seed!"
  },
  {
    "objectID": "recitation/r26/index.html#apply-statements",
    "href": "recitation/r26/index.html#apply-statements",
    "title": "Approaches to Generating Data for a Simulation: for loops and apply Functions",
    "section": "apply Statements",
    "text": "apply Statements\nThis is definitely a bit more advanced, and we’ll dig into this in a few more weeks. But to start exposing you to different approaches, we can use apply statements to facilitate the simulation study!\nGenerally we will need to write or use an existing function in R (to be discussed in greater detail in the future). In our case, we can modify the code in our for loop above:\n\n\nCode\nmy_exp_sim &lt;- function(n, lambda=3){\n### Function simulate exponential data\n# n: sample size to use\n# lambda: lambda parameter to use in simulating data\n  \n  sim &lt;- rexp(n=n, rate=lambda) # simulate a sample of n with rate=rate\n  \n  median_sim &lt;- median(sim) # calculate the median of each sim\n  \n  bias &lt;- median_sim - (1/lambda) # calculate the bias of the median in estimating the mean\n  \n  return(c(sample_size=n, bias=bias, median=median_sim)) # estimates to return from the function\n}\n\nmy_exp_sim(n=10) # will use rate=3 by default since that is what I specified in the function above\n\n\nsample_size        bias      median \n 10.0000000  -0.1669902   0.1663431 \n\n\nCode\nmy_exp_sim(n=10, lambda=5) # we can also change the rate to whatever we want\n\n\nsample_size        bias      median \n10.00000000  0.03291594  0.23291594 \n\n\nNow let’s try using sapply which will more efficiently “loop” through our vectors and apply it to the function:\n\n\nCode\nset.seed(515)\n\n# Note: the first part of sapply (X) \"seq(from=100, to=50000, by=100)\" is the vector of values we wish to work through\n# Note: the second part of sapply (FUN) \"function(x) my_exp_sim(n=x, rate=3)\" takes the values from our \"X\" and uses them as the value for \"x\", which in our case represents our sample size\nbias_median_sapply &lt;- sapply(X = seq(from=100, to=50000, by=100), FUN = function(x) my_exp_sim(n=x, lambda=3))\n\nbias_median_sapply &lt;- t(bias_median_sapply) # sapply creates results that are stored in a matrix/array that is 3 rows and 500 columns, I like to transpose the matrix to have 500 rows and 3 columns to both better see the data and because I more intuitively think of each row as a new simulation myself\n\nbias_median_sapply[c(1:3,498:500),]\n\n\n     sample_size       bias    median\n[1,]         100 -0.1122742 0.2210592\n[2,]         200 -0.1141976 0.2191358\n[3,]         300 -0.1129037 0.2204296\n[4,]       49800 -0.1055317 0.2278016\n[5,]       49900 -0.1051049 0.2282284\n[6,]       50000 -0.1012018 0.2321315\n\n\nCode\nplot(x=bias_median_sapply[,1], y=bias_median_sapply[,2], xlab='Sample Size', ylab='Bias Estimate of the Median', type='l')"
  },
  {
    "objectID": "recitation/r24/index.html",
    "href": "recitation/r24/index.html",
    "title": "Generalized Linear Models and Their Connection to Ordinary Least Squares Linear Regression",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r24/index.html#gaussian-identity-link",
    "href": "recitation/r24/index.html#gaussian-identity-link",
    "title": "Generalized Linear Models and Their Connection to Ordinary Least Squares Linear Regression",
    "section": "Gaussian, Identity Link",
    "text": "Gaussian, Identity Link\nLet’s see some examples of how our models change with different approaches. We’ll simulate a simple case of \\(n=50\\) with a few predictors:\n\n\nCode\nset.seed(1207) # set seed for reproducibility\n\nn &lt;- 50\nx1 &lt;- rnorm(n=n, mean=10, sd=2)\nx2 &lt;- rexp(n=n, rate=0.5)\nx3 &lt;- rbinom(n=n, size=1, prob=0.6)\nerror &lt;- rnorm(n=n, mean=0, sd=3)\ny &lt;- 150 + -2*x1 + 4*x2 - 5*x3 + error\n\n\n\n\nCode\n# Gaussian, Identity link\nm_identity &lt;- glm(y ~ x1 + x2 + x3, family=gaussian(link = \"identity\"))\nsummary(m_identity)\n\n\n\nCall:\nglm(formula = y ~ x1 + x2 + x3, family = gaussian(link = \"identity\"))\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 150.4104     2.0191  74.492  &lt; 2e-16 ***\nx1           -2.0104     0.2039  -9.861 6.34e-13 ***\nx2            3.9184     0.2023  19.374  &lt; 2e-16 ***\nx3           -5.4021     0.8358  -6.463 5.84e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 8.66977)\n\n    Null deviance: 4037.74  on 49  degrees of freedom\nResidual deviance:  398.81  on 46  degrees of freedom\nAIC: 255.72\n\nNumber of Fisher Scoring iterations: 2\n\n\nThis is our tried and true model, our linear regression with an additive interpretation. In this case, our simulation values are pretty darn close to what we set!"
  },
  {
    "objectID": "recitation/r24/index.html#gaussian-inverse-link",
    "href": "recitation/r24/index.html#gaussian-inverse-link",
    "title": "Generalized Linear Models and Their Connection to Ordinary Least Squares Linear Regression",
    "section": "Gaussian, Inverse Link",
    "text": "Gaussian, Inverse Link\n\n\nCode\n# Gaussian, Inverse link\nm_inverse &lt;- glm(y ~ x1 + x2 + x3, family=gaussian(link = \"inverse\"))\nsummary(m_inverse)\n\n\n\nCall:\nglm(formula = y ~ x1 + x2 + x3, family = gaussian(link = \"inverse\"))\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.600e-03  1.062e-04  62.120  &lt; 2e-16 ***\nx1           1.064e-04  1.107e-05   9.613 1.41e-12 ***\nx2          -1.986e-04  1.014e-05 -19.592  &lt; 2e-16 ***\nx3           2.731e-04  4.433e-05   6.161 1.66e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 8.993352)\n\n    Null deviance: 4037.74  on 49  degrees of freedom\nResidual deviance:  413.69  on 46  degrees of freedom\nAIC: 257.55\n\nNumber of Fisher Scoring iterations: 4\n\n\nThese estimates don’t have as nice an interpretation and it is hard to tell if they seem “right” given our simulation setting.\nHowever, if we change our values of Y to be estimated as the inverse of our \\(\\mathbf{X}\\) we will see a match of our simulation:\n\n\nCode\ny2 &lt;- (1 / (150 + -2*x1 + 4*x2 - 5*x3 + error)) # inverse of 1/XB\nm_inverse2 &lt;- glm(y2 ~ x1 + x2 + x3, family=gaussian(link = \"inverse\"))\nsummary(m_inverse2)\n\n\n\nCall:\nglm(formula = y2 ~ x1 + x2 + x3, family = gaussian(link = \"inverse\"))\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 150.8010     2.0409   73.89  &lt; 2e-16 ***\nx1           -2.0499     0.2001  -10.25 1.87e-13 ***\nx2            3.9120     0.2201   17.77  &lt; 2e-16 ***\nx3           -5.4886     0.8393   -6.54 4.48e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 2.496697e-08)\n\n    Null deviance: 1.0956e-05  on 49  degrees of freedom\nResidual deviance: 1.1485e-06  on 46  degrees of freedom\nAIC: -727.56\n\nNumber of Fisher Scoring iterations: 3\n\n\nThis suggests that the relationship between Y and \\(\\mathbf{X}\\) is an inverse link (although note it isn’t the exact same \\(\\hat{\\beta}\\)’s from the identity link). However, it still doesn’t have a nice interpretation of the \\(\\hat{\\beta}\\)’s."
  },
  {
    "objectID": "recitation/r24/index.html#gaussian-log-link",
    "href": "recitation/r24/index.html#gaussian-log-link",
    "title": "Generalized Linear Models and Their Connection to Ordinary Least Squares Linear Regression",
    "section": "Gaussian, Log Link",
    "text": "Gaussian, Log Link\nThe idea behind this model is perhaps most similar to what we saw earlier this semester but with some different assumptions:\n\n\nCode\n# Gaussian, Log link\nm_log &lt;- glm(y ~ x1 + x2 + x3, family=gaussian(link = \"log\"))\nsummary(m_log)\n\n\n\nCall:\nglm(formula = y ~ x1 + x2 + x3, family = gaussian(link = \"log\"))\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.017036   0.014648 342.510  &lt; 2e-16 ***\nx1          -0.014655   0.001502  -9.758 8.82e-13 ***\nx2           0.027948   0.001427  19.585  &lt; 2e-16 ***\nx3          -0.038556   0.006085  -6.336 9.05e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 8.767716)\n\n    Null deviance: 4037.74  on 49  degrees of freedom\nResidual deviance:  403.31  on 46  degrees of freedom\nAIC: 256.28\n\nNumber of Fisher Scoring iterations: 3\n\n\nLet’s compare it to a Gaussian family with the identity link but with \\(\\log(Y)\\):\n\n\nCode\n# Gaussian, Identity Link with log(Y)\nm_identity_lnY &lt;- glm(log(y) ~ x1 + x2 + x3, family=gaussian(link = \"identity\"))\nsummary(m_identity_lnY)\n\n\n\nCall:\nglm(formula = log(y) ~ x1 + x2 + x3, family = gaussian(link = \"identity\"))\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.019159   0.014829 338.465  &lt; 2e-16 ***\nx1          -0.014885   0.001497  -9.941 4.91e-13 ***\nx2           0.028017   0.001485  18.862  &lt; 2e-16 ***\nx3          -0.039117   0.006139  -6.372 8.00e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.0004676353)\n\n    Null deviance: 0.209303  on 49  degrees of freedom\nResidual deviance: 0.021511  on 46  degrees of freedom\nAIC: -235.67\n\nNumber of Fisher Scoring iterations: 2\n\n\nIn this case, our predictors are not identical (and, in fact, they happen to be quite similar). BUT, it turns out these two models are making very different assumptions:\n\nThe log link transforms the model itself (i.e., we have a log-likelihood of \\(l(\\mu,\\sigma^2;y) = \\sum_{i=1}^{n} \\left\\{\\frac{y_i \\exp(\\mathbf{X}'\\boldsymbol{\\beta}) - (\\exp(\\mathbf{X}'\\boldsymbol{\\beta}))^2 / 2}{\\sigma^2} - \\frac{y_i^2}{2 \\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2) \\right\\}\\)). The Gaussian errors are still on the natural scale, so the error variance is constant for all mean values of \\(Y\\).\nThe \\(\\log(Y)\\) with identity link transforms the data itself. Here, if we retransform \\(\\log(Y)\\) back to \\(Y\\) the variance will change with the mean.\n\nWhy might we care? It turns out this removes the retransformation problem where \\(E[\\log(Y)] = \\mathbf{X}'\\boldsymbol{\\beta}\\) but \\(E(\\log(Y)|X) \\neq \\log(E(Y|X))\\)"
  },
  {
    "objectID": "recitation/r22/index.html",
    "href": "recitation/r22/index.html",
    "title": "Collinearity in Regression",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r22/index.html#structural-multicollinearity",
    "href": "recitation/r22/index.html#structural-multicollinearity",
    "title": "Collinearity in Regression",
    "section": "Structural Multicollinearity",
    "text": "Structural Multicollinearity\nThis form of multicollinearity is introduced by the inclusion of polynomial or interaction terms in the model. In both cases, we can imagine that there probably should be correlation between these related variables.\n\nPolynomial Regression\nLet’s first see an example of our polynomial regression with raw polynomials:\n\n\nCode\nlibrary(car) # load package for vif function\ndat &lt;- read.csv('../../.data/nhanes1516_sexhormone.csv')\ndat &lt;- dat[which(dat$MALE==T),]\ndat &lt;- dat[!is.na(dat$SHBG),]\n\ndat[,c('age','age2','age3')] &lt;- poly(dat$RIDAGEYR,3, raw=T)\nmod_raw3 &lt;- lm(SHBG ~ age + age2 + age3, data=dat)\nvif(mod_raw3)\n\n\n     age     age2     age3 \n167.0021 852.3380 298.5833 \n\n\nWe see that our three age terms (i.e., \\(X_{age}\\), \\(X_{age}^2\\), and \\(X_{age}^{3}\\)) have VIF values &gt;&gt;10. This is expected since we simply squared and cubed our age term.\nIf we fit orthogonal polynomial terms, we see that the multicollinearity is removed:\n\n\nCode\ndat[,c('orthage','orthage2','orthage3')] &lt;- poly(dat$RIDAGEYR,3, raw=F)\nmod_orth3 &lt;- lm(SHBG ~ orthage + orthage2 + orthage3, data=dat)\nvif(mod_orth3)\n\n\n orthage orthage2 orthage3 \n       1        1        1 \n\n\nNow, all VIF values are less than 10, suggesting no concerns with multicollinearity.\nAs a brief, aside, we can see how poly() created our raw and orthogonal \\(X_{age}\\) terms:\n\n\nCode\nhead(dat)\n\n\n    X  SEQN MALE RIDAGEYR TESTOSTERONE ESTRADIOL  SHBG age age2   age3\n1   1 83732 TRUE       62          367      19.9 42.86  62 3844 238328\n2   2 83733 TRUE       53          505      28.3 29.04  53 2809 148877\n3   3 83734 TRUE       78          104      12.7 27.02  78 6084 474552\n8   8 83741 TRUE       22          543      34.4 21.76  22  484  10648\n10 10 83743 TRUE       18          381      27.1 17.86  18  324   5832\n11 11 83744 TRUE       56          685      31.9 73.08  56 3136 175616\n       orthage      orthage2    orthage3\n1   0.01766653 -0.0036872569 -0.02046891\n2   0.01087120 -0.0151759086 -0.01646149\n3   0.02974713  0.0324476628  0.02458503\n8  -0.01253495 -0.0060449164  0.01960798\n10 -0.01555510  0.0006320036  0.01440223\n11  0.01313631 -0.0120533373 -0.01934986\n\n\n\n\nInteraction Term\nStructural multicollinearity can also occur due to the presence of an interaction:\n\n\nCode\nmod_int_all &lt;- lm(SHBG ~ RIDAGEYR + ESTRADIOL + TESTOSTERONE + RIDAGEYR*ESTRADIOL + RIDAGEYR*TESTOSTERONE + RIDAGEYR*ESTRADIOL*TESTOSTERONE, data=dat)\nmod_int_two &lt;- lm(SHBG ~ RIDAGEYR + ESTRADIOL + TESTOSTERONE + RIDAGEYR*ESTRADIOL + RIDAGEYR*TESTOSTERONE, data=dat)\nmod_int_none &lt;- lm(SHBG ~ RIDAGEYR + ESTRADIOL + TESTOSTERONE, data=dat)\n\n# compare VIFs from models with 3-way and 2-way interaction, only 2-way interaction, only main effects\nvif(mod_int_all)\n\n\n                       RIDAGEYR                       ESTRADIOL \n                       8.518435                       16.211995 \n                   TESTOSTERONE              RIDAGEYR:ESTRADIOL \n                      14.326071                       32.468986 \n          RIDAGEYR:TESTOSTERONE          ESTRADIOL:TESTOSTERONE \n                      25.602547                       27.562449 \nRIDAGEYR:ESTRADIOL:TESTOSTERONE \n                      39.002071 \n\n\nCode\nvif(mod_int_two)\n\n\n             RIDAGEYR             ESTRADIOL          TESTOSTERONE \n             3.989791             11.198447             10.400867 \n   RIDAGEYR:ESTRADIOL RIDAGEYR:TESTOSTERONE \n            18.727071             14.772772 \n\n\nCode\nvif(mod_int_none)\n\n\n    RIDAGEYR    ESTRADIOL TESTOSTERONE \n    1.177162     2.094151     1.908693 \n\n\nWe see the presence of multicollinearity in the models with interactions, but this is to be expected given the interaction term. Given the lack of multicollinearity in the model with only main effects, this is likely caused by the interactions and not underlying relationships."
  },
  {
    "objectID": "recitation/r22/index.html#data-multicollinearity",
    "href": "recitation/r22/index.html#data-multicollinearity",
    "title": "Collinearity in Regression",
    "section": "Data Multicollinearity",
    "text": "Data Multicollinearity\nAnother type of multicollinearity is when two variables are closely related (e.g., they either measure similar constructs/phenomenon or are correlated by chance). This suggests that the variables may be attempting to explain the same or very similar aspects of the variability in our outcome \\(Y\\).\nTo examine this, let’s explore a simulation where we simulate a multiple linear regression and add an extra variable that is highly related to one of our simulated predictors:\n\n\nCode\nset.seed(6618) # set seed for reproducibility\nn &lt;- 100\n\nx1 &lt;- rbinom(n=n, size=1, prob=0.3)\nx2 &lt;- rnorm(n=n, mean=5, sd=2)\nx3 &lt;- rnorm(n=n, mean=0, sd=3)\nx2_corr &lt;- x2 + runif(n=n, -0.25, 0.25) # simulate highly correlated variable\n\ny &lt;- 5 + 0.8*x1 + 2*x2 + -1*x3 + rnorm(n=n, mean=0, sd=2)\n\n# Fit true model, evaluate coefficients and VIF\nlm_true &lt;- lm(y ~ x1 + x2 + x3)\nsummary(lm_true)\n\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9318 -1.2701  0.0105  1.3527  4.2596 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.09362    0.49547  10.280   &lt;2e-16 ***\nx1           0.12039    0.37765   0.319    0.751    \nx2           2.05506    0.08522  24.114   &lt;2e-16 ***\nx3          -1.00879    0.05932 -17.005   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.774 on 96 degrees of freedom\nMultiple R-squared:  0.8905,    Adjusted R-squared:  0.887 \nF-statistic: 260.2 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nvif(lm_true)\n\n\n      x1       x2       x3 \n1.001741 1.015950 1.017582 \n\n\nNow let’s add x2_corr and check its Pearson’s linear correlation with x2:\n\n\nCode\n# Fit model with x2_corr, evaluate coefficients and VIF\nlm_mc &lt;- lm(y ~ x1 + x2 + x3 + x2_corr)\nsummary(lm_mc)\n\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3 + x2_corr)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9656 -1.2914  0.0406  1.3509  4.1876 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.09832    0.49824  10.233   &lt;2e-16 ***\nx1           0.11427    0.38024   0.301    0.764    \nx2           1.73501    1.24708   1.391    0.167    \nx3          -1.01146    0.06051 -16.715   &lt;2e-16 ***\nx2_corr      0.32064    1.24639   0.257    0.798    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.783 on 95 degrees of freedom\nMultiple R-squared:  0.8905,    Adjusted R-squared:  0.8859 \nF-statistic: 193.2 on 4 and 95 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nvif(lm_mc)\n\n\n        x1         x2         x3    x2_corr \n  1.005669 215.433748   1.048501 216.117251 \n\n\nCode\ncor(x2, x2_corr) # check correlation\n\n\n[1] 0.997602\n\n\nWe see that our estimate of the effect of \\(X_2\\) has been decreased and is no longer statistically significant. In evaluating the VIF, we see it is &gt;&gt;10! This is because our two variables have a Pearson’s correlation of 0.998.\nIn practice, we’d need to decide whether to keep \\(X_2\\) or the \\(X_{2,corr}\\) in the model but we wouldn’t have the benefit for simulating data to know the truth. Instead, we’d need to make a decision based on the context of the data and the problem we are investigating.\nIf we did choose to keep \\(X_{2,corr}\\) we would see the following results:\n\n\nCode\n# Fit model with x2_corr, evaluate coefficients and VIF\nlm_corr &lt;- lm(y ~ x1 + x2_corr + x3)\nsummary(lm_corr)\n\n\n\nCall:\nlm(formula = y ~ x1 + x2_corr + x3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2969 -1.2145 -0.0964  1.3580  3.8000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.16641    0.49824  10.369   &lt;2e-16 ***\nx1           0.08148    0.38136   0.214    0.831    \nx2_corr      2.05060    0.08601  23.842   &lt;2e-16 ***\nx3          -1.02518    0.05999 -17.088   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.792 on 96 degrees of freedom\nMultiple R-squared:  0.8883,    Adjusted R-squared:  0.8848 \nF-statistic: 254.5 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nvif(lm_corr)\n\n\n      x1  x2_corr       x3 \n1.001805 1.019173 1.020688 \n\n\nIn this case, given the high correlation, our findings look very similar to lm_true, even though technically x2_corr was not used to simulate the outcome values."
  },
  {
    "objectID": "recitation/r20/index.html",
    "href": "recitation/r20/index.html",
    "title": "Classical Criteria of Confounding",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nClassical Criteria of Confounding\n\n\n\n\n\nFrom our DAG, we can define three models of interest:\n\nCrude Model: \\(\\hat{Y} = \\hat{\\beta}_{01} + \\hat{\\beta}_{crude}X\\)\nAdjusted Model: \\(\\hat{Y} = \\hat{\\beta}_{02} + \\hat{\\beta}_{adj}X + \\hat\\beta_{C}C\\)\nCovariate Model: \\(\\hat{C} = \\hat{\\gamma}_{0} + \\hat{\\gamma}_{X}X\\)\n\nWe will use these three models to evaluate potential confounders.\nThe classical criteria for confounding is comprised of 3 considerations:\n\nA confounding factor must be associated with the exposure (or PEV) under study. From our three models, this is the association of \\(X\\) and \\(C\\) represented by \\(\\hat{\\gamma}_{X}\\).\nA confounding factor must be a risk factor or a surrogate for a risk factor for the disease. From our three models, this is the association of \\(C\\) and \\(Y\\) given \\(X\\) represented by \\(\\hat{\\beta}_{C}\\).\nA confounding factor must not be affected by the exposure or the disease. This one is based on the context/understanding.\n\nThere is also a separate operational criterion of confounding where confounding is present if there is a “meaningful” difference between \\(\\hat{\\beta}_{crude}\\) and \\(\\hat{\\beta}_{adj}\\), which will depend on the context and what would represent a clinically relevant change.\nIf clinically meaningful change is uncertain, we might calculate the percent change in one of two ways:\n\n\\(\\frac{\\hat{\\beta}_{crude} - \\hat{\\beta}_{adj}}{\\hat{\\beta}_{crude}} \\times 100\\) (favored by biostatisticians)\n\\(\\frac{\\hat{\\beta}_{crude} - \\hat{\\beta}_{adj}}{\\hat{\\beta}_{adj}} \\times 100\\) (favored by epidemiologists)\n\nIn both cases we need to rely on knowledge of the subject area to determine if the arrows in our DAG are indicating that the variable under considering is a confounder (and not a mediator, etc.)."
  },
  {
    "objectID": "recitation/r19/index.html",
    "href": "recitation/r19/index.html",
    "title": "Interpreting Polynomial Regression Models, Selecting Your Highest Order Polynomial Term, and Calculus in R",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r19/index.html#interpretation-of-trends",
    "href": "recitation/r19/index.html#interpretation-of-trends",
    "title": "Interpreting Polynomial Regression Models, Selecting Your Highest Order Polynomial Term, and Calculus in R",
    "section": "Interpretation of Trends",
    "text": "Interpretation of Trends\nLet’s examine the model fit for each of the models:\n\n\\(\\hat{Y} = 55.903 + -0.095 X\\)\n\\(\\hat{Y} = 102.362 + -3.351 X + 0.039 X^2\\)\n\\(\\hat{Y} = 152.581 + -9.177 X + 0.203 X^2 + -0.001 X^3\\)\n\\(\\hat{Y} = 221.308 + -20.371 X + 0.730 X^2 + -0.011 X^3 + 0.00005 X^4\\)\n\nOur simple linear regression model (i.e., a polynomial regression with order 1), is easy to interpret and follows our usual pattern: For a 1 year increase in age, SHBG significantly decreases on average by 0.095 nmol/L (95% CI: 0.042 to 0.148 nmol/L decrease; p&lt;0.001).\nHowever, the interpretations of any models with higher order terms is made far more challenging. Now, the expected change in our outcome will also depend on the value of both \\(X\\) and \\(X^2\\) since \\(X=20 \\implies X^2=400\\) which is not an equivalent change to \\(X=60 \\implies X^2=3600\\). In other words, a one-unit increase in age will have different effects depending on the starting point given its non-linear trend over age.\nMaking it a “brief, but complete” interpretation is a little more challenging since we cannot meaningful interpret the beta coefficients and individual p-values are affected by multicollinearity (in this model with raw, not orthogonal, polynomial terms). However, you consider a variety of approaches:\n\nOne of the easiest ways to interpret these models is to plot the trend and describe the trends.\nEvaluate the overall contribution of the polynomials (e.g., with a partial \\(F\\)-test).\nDescribe the predicted outcome at select predictors (e.g., using GLHT; may be challenging if adjusting for other variables since those would also have an influence).\nCalculate the local/global min max, estimate inflection points, or estimate rates of change using calculus.\n\n\nCalculus in R\nConsider our quadratic polynomial. We can estimate the min/max point by taking the first order derivative with respect to \\(X\\) of our model and setting it equal to 0, then solving for \\(X\\):\n\\[\\begin{align*}\n\\hat{Y} = f(X) &= 102.362 + -3.351 X + 0.039 X^2 \\\\\nf'(X) &=  -3.351 + 2 \\times 0.039 X = -3.351 + 0.078 X \\\\\n0 &= -3.351 + 0.078 X \\implies X = \\frac{3.351}{0.078} = 42.96\n\\end{align*}\\]\nIf we plug this back into our regression equation we get the predicted minimum: \\(\\hat{Y} = 102.362 + -3.351(42.96) + 0.039(42.96)^2 = 30.38\\) nmol/L of SHBG.\nR also has the ability to do this calculus for us:\n\n\nCode\n# first write our expression\nf &lt;- expression( 102.362 + -3.351*x + 0.039 * x^2 )\n\n# then use the D() function and specify the variable to derive with respect to\nD(f, 'x')\n\n\n0.039 * (2 * x) - 3.351\n\n\nCode\n# uniroot() function calculates where it equals 0 given a range of X values\nfind_root &lt;- function(x) eval( D(f,'x') )\nuniroot(find_root, c(6,80))\n\n\n$root\n[1] 42.96154\n\n$f.root\n[1] -4.440892e-16\n\n$iter\n[1] 2\n\n$init.it\n[1] NA\n\n$estim.prec\n[1] 6.103516e-05\n\n\nMore generally, we can use mosaicCalc::Zeros() to find all candidate points:\n\n\nCode\nlibrary(mosaicCalc)\n\n# first write our expressions for the first order derivatives, then findZeros\nf2 &lt;- D( 102.362 + -3.351*x + 0.039 * x^2 ~ x )\nfindZeros(f2(x) ~ x, xlim = range(6, 80))\n\n\n   x\n1 43\n\n\nCode\nf3 &lt;- D( 152.581 + -9.177*x + 0.203*x^2 + -0.001*x^3 ~ x )\nfindZeros(f3(x) ~ x, xlim = range(6, 80))\n\n\n        x\n1 28.6824\n\n\nCode\nf4 &lt;- D( 221.308 + -20.371*x + 0.730*x^2 + -0.011*x^3 + 0.00005*x^4 ~ x )\nfindZeros(f4(x) ~ x, xlim = range(6, 80))\n\n\nnumeric(0)\n\n\nCode\n# notice that none were found for f4 due to rounding, we can instead use paste0() and pull the coefficients directly:\nf4_eq &lt;- paste0(coef(mod_poly4)[1], '+', coef(mod_poly4)[2], '*x +', coef(mod_poly4)[3], '*x^2 +', coef(mod_poly4)[4], '*x^3 +', coef(mod_poly4)[5], '*x^4 ~ x')\nf4_eq # print regression equation with more decimals to check\n\n\n[1] \"221.308434170064+-20.3710021214272*x +0.729584043956372*x^2 +-0.0105089215893748*x^3 +5.36746951571261e-05*x^4 ~ x\"\n\n\nCode\nf4_coef &lt;- D( as.formula(f4_eq) )\nfindZeros(f4_coef(x) ~ x, xlim = range(6, 80))\n\n\n        x\n1 25.8878\n\n\nFrom these models, we could interpret that the minimum SHBG is expected, on average, at 43 (quadratic), 28.7 (cubic), or 25.9 (cuartic) years old. We could also estimate the minimum (and 95% CI) by plugging it into our regression equation."
  },
  {
    "objectID": "recitation/r19/index.html#polynomial-regression-and-parsimony",
    "href": "recitation/r19/index.html#polynomial-regression-and-parsimony",
    "title": "Interpreting Polynomial Regression Models, Selecting Your Highest Order Polynomial Term, and Calculus in R",
    "section": "Polynomial Regression and Parsimony",
    "text": "Polynomial Regression and Parsimony\nWhen we fit a regression, we want to find a model that avoids under- and over-fitting the data. Further, we also want to consider the parsimony of our model (i.e., does it have to be as complex or could it be simpler?).\nParsimony in the context of polynomial regression means considering models with fewer higher order terms both to avoid overfitting and having extra, potentially unnecessary, coefficients in our model.\nThe evaluation of parsimony is often more subjective, but we can use quantitative approaches to determine how many polynomial terms are statistically justified as a starting place.\n\nPartial F-test on Raw Polynomials\nIf we fit models on our raw polynomials, we may have concerns about collinearity, which could make it challenging to identify the appropriate number of terms.\nLet’s start by fitting a sequence of models and running partial F-tests:\n\n\nCode\nmod_lm &lt;- lm(SHBG~RIDAGEYR, data=dat)\nmod_poly2 &lt;- lm(SHBG~poly(RIDAGEYR,2, raw=T), data=dat)\nmod_poly3 &lt;- lm(SHBG~poly(RIDAGEYR,3, raw=T), data=dat)\nmod_poly4 &lt;- lm(SHBG~poly(RIDAGEYR,4, raw=T), data=dat)\nmod_poly5 &lt;- lm(SHBG~poly(RIDAGEYR,5, raw=T), data=dat)\nmod_poly6 &lt;- lm(SHBG~poly(RIDAGEYR,6, raw=T), data=dat)\nmod_poly7 &lt;- lm(SHBG~poly(RIDAGEYR,7, raw=T), data=dat)\nmod_poly8 &lt;- lm(SHBG~poly(RIDAGEYR,8, raw=T), data=dat)\nmod_poly9 &lt;- lm(SHBG~poly(RIDAGEYR,9, raw=T), data=dat)\nmod_poly10 &lt;- lm(SHBG~poly(RIDAGEYR,10, raw=T), data=dat)\nmod_poly11 &lt;- lm(SHBG~poly(RIDAGEYR,11, raw=T), data=dat)\n\n# partial F-tests (i.e., equivalent to observing coefficients and analyzing highest order t-statistic and p-value)\nanova(mod_lm,mod_poly2,mod_poly3,mod_poly4,mod_poly5,mod_poly6,mod_poly7,mod_poly8,mod_poly9,mod_poly10,mod_poly11)\n\n\nAnalysis of Variance Table\n\nModel  1: SHBG ~ RIDAGEYR\nModel  2: SHBG ~ poly(RIDAGEYR, 2, raw = T)\nModel  3: SHBG ~ poly(RIDAGEYR, 3, raw = T)\nModel  4: SHBG ~ poly(RIDAGEYR, 4, raw = T)\nModel  5: SHBG ~ poly(RIDAGEYR, 5, raw = T)\nModel  6: SHBG ~ poly(RIDAGEYR, 6, raw = T)\nModel  7: SHBG ~ poly(RIDAGEYR, 7, raw = T)\nModel  8: SHBG ~ poly(RIDAGEYR, 8, raw = T)\nModel  9: SHBG ~ poly(RIDAGEYR, 9, raw = T)\nModel 10: SHBG ~ poly(RIDAGEYR, 10, raw = T)\nModel 11: SHBG ~ poly(RIDAGEYR, 11, raw = T)\n   Res.Df     RSS Df Sum of Sq         F    Pr(&gt;F)    \n1    3388 4286690                                     \n2    3387 3276833  1   1009857 1351.0353 &lt; 2.2e-16 ***\n3    3386 2873485  1    403348  539.6180 &lt; 2.2e-16 ***\n4    3385 2618205  1    255281  341.5269 &lt; 2.2e-16 ***\n5    3384 2573535  1     44670   59.7615 1.403e-14 ***\n6    3383 2558283  1     15252   20.4048 6.482e-06 ***\n7    3382 2550263  1      8020   10.7289  0.001065 ** \n8    3381 2537447  1     12817   17.1467 3.545e-05 ***\n9    3380 2529858  1      7589   10.1524  0.001454 ** \n10   3379 2525059  1      4799    6.4198  0.011330 *  \n11   3378 2524950  1       109    0.1465  0.701973    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBased on these comparisons, it seems that a model with order 10 is considered the “best”.\n\n\nLack-of-Fit F-test\nWhen we have replicate observations at each level, we can also estimate the pure versus model error and use it to identify the number of polynomial terms. In the case of our NHANES data, age is only provided as a whole number and we could assume we have lots of categorical data:\n\n\nCode\n# fit saturated model\nmod_pure &lt;- lm(SHBG ~ as.factor(RIDAGEYR), data=dat)\n\n# save p-values from partial F-tests\np1 &lt;- anova(mod_pure, mod_lm)[\"Pr(&gt;F)\"][2,1]\np2 &lt;- anova(mod_pure, mod_poly2)[\"Pr(&gt;F)\"][2,1]\np3 &lt;- anova(mod_pure, mod_poly3)[\"Pr(&gt;F)\"][2,1]\np4 &lt;- anova(mod_pure, mod_poly4)[\"Pr(&gt;F)\"][2,1]\np5 &lt;- anova(mod_pure, mod_poly5)[\"Pr(&gt;F)\"][2,1]\np6 &lt;- anova(mod_pure, mod_poly6)[\"Pr(&gt;F)\"][2,1]\np7 &lt;- anova(mod_pure, mod_poly7)[\"Pr(&gt;F)\"][2,1]\np8 &lt;- anova(mod_pure, mod_poly8)[\"Pr(&gt;F)\"][2,1]\np9 &lt;- anova(mod_pure, mod_poly9)[\"Pr(&gt;F)\"][2,1]\np10 &lt;- anova(mod_pure, mod_poly10)[\"Pr(&gt;F)\"][2,1]\np11 &lt;- anova(mod_pure, mod_poly11)[\"Pr(&gt;F)\"][2,1]\n\n# print p-values\nround(c('Order 1'=p1, 'Order 2'=p2, 'Order 3'=p3, 'Order 4'=p4, 'Order 5'=p5, 'Order 6'=p6, 'Order 7'=p7, 'Order 8'=p8, 'Order 9'=p9, 'Order 10'=p10, 'Order 11'=p11),4)\n\n\n Order 1  Order 2  Order 3  Order 4  Order 5  Order 6  Order 7  Order 8 \n  0.0000   0.0000   0.0000   0.0000   0.0001   0.0044   0.0234   0.2086 \n Order 9 Order 10 Order 11 \n  0.4763   0.6655   0.6374 \n\n\nOur results indicate that an octic (order 8) polynomial regression is not significantly better than a septic (order 7) polynomial regression. However, the septic (order 7) is better than the sextic (order 6).\nThis differs from our approach using just raw polynomials, where we chose a more complex order 10 model as “best”.\n\n\nOrthogonal Polynomial Contrasts\nAn approach we alluded to, but didn’t delve into details, is the construction of orthogonal polynomial contrasts which remove the correlation between polynomial terms. While the math behind it is a bit lengthy, it is fairly easy to implement in R by removing our raw=T argument (or setting equal to the default FALSE) from poly():\n\n\nCode\nmod_orth1 &lt;- lm( SHBG~poly(RIDAGEYR,1, raw=F), data=dat)\nmod_orth2 &lt;- lm( SHBG~poly(RIDAGEYR,2, raw=F), data=dat)\nmod_orth3 &lt;- lm( SHBG~poly(RIDAGEYR,3, raw=F), data=dat)\nmod_orth4 &lt;- lm( SHBG~poly(RIDAGEYR,4, raw=F), data=dat)\nmod_orth5 &lt;- lm( SHBG~poly(RIDAGEYR,5, raw=F), data=dat)\nmod_orth6 &lt;- lm( SHBG~poly(RIDAGEYR,6, raw=F), data=dat)\nmod_orth7 &lt;- lm( SHBG~poly(RIDAGEYR,7, raw=F), data=dat)\nmod_orth8 &lt;- lm( SHBG~poly(RIDAGEYR,8, raw=F), data=dat)\nmod_orth9 &lt;- lm( SHBG~poly(RIDAGEYR,9, raw=F), data=dat)\nmod_orth10 &lt;- lm( SHBG~poly(RIDAGEYR,10, raw=F), data=dat)\nmod_orth11 &lt;- lm( SHBG~poly(RIDAGEYR,11, raw=F), data=dat)\n\n# print summary of Order 11 polynomial regression\nsummary(mod_orth11)\n\n\n\nCall:\nlm(formula = SHBG ~ poly(RIDAGEYR, 11, raw = F), data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-100.74  -15.81   -4.69   10.99  254.05 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     52.2391     0.4696 111.250  &lt; 2e-16 ***\npoly(RIDAGEYR, 11, raw = F)1  -125.7103    27.3399  -4.598 4.42e-06 ***\npoly(RIDAGEYR, 11, raw = F)2  1004.9164    27.3399  36.756  &lt; 2e-16 ***\npoly(RIDAGEYR, 11, raw = F)3  -635.0966    27.3399 -23.230  &lt; 2e-16 ***\npoly(RIDAGEYR, 11, raw = F)4   505.2531    27.3399  18.480  &lt; 2e-16 ***\npoly(RIDAGEYR, 11, raw = F)5  -211.3525    27.3399  -7.731 1.40e-14 ***\npoly(RIDAGEYR, 11, raw = F)6   123.4989    27.3399   4.517 6.48e-06 ***\npoly(RIDAGEYR, 11, raw = F)7    89.5518    27.3399   3.276  0.00107 ** \npoly(RIDAGEYR, 11, raw = F)8  -113.2105    27.3399  -4.141 3.54e-05 ***\npoly(RIDAGEYR, 11, raw = F)9    87.1125    27.3399   3.186  0.00145 ** \npoly(RIDAGEYR, 11, raw = F)10  -69.2721    27.3399  -2.534  0.01133 *  \npoly(RIDAGEYR, 11, raw = F)11   10.4627    27.3399   0.383  0.70197    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 27.34 on 3378 degrees of freedom\nMultiple R-squared:  0.4131,    Adjusted R-squared:  0.4112 \nF-statistic: 216.2 on 11 and 3378 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that the 11th order polynomial is the first to have a p-value &gt;0.05. This agrees with our raw polynomial calculation that Order 10 would be “best”.\nWith the correlation removed between our predictors, we can also see that a sequence of F-tests for adding each higher order term results in the same p-values:\n\n\nCode\n# print ANOVA comparisons adding 1 higher orthogonal term, notice p-values match those from the regression coefficient table for mod_orth11\nanova(mod_orth1,mod_orth2,mod_orth3,mod_orth4,mod_orth5,mod_orth6,mod_orth7,mod_orth8,mod_orth9,mod_orth10,mod_orth11)\n\n\nAnalysis of Variance Table\n\nModel  1: SHBG ~ poly(RIDAGEYR, 1, raw = F)\nModel  2: SHBG ~ poly(RIDAGEYR, 2, raw = F)\nModel  3: SHBG ~ poly(RIDAGEYR, 3, raw = F)\nModel  4: SHBG ~ poly(RIDAGEYR, 4, raw = F)\nModel  5: SHBG ~ poly(RIDAGEYR, 5, raw = F)\nModel  6: SHBG ~ poly(RIDAGEYR, 6, raw = F)\nModel  7: SHBG ~ poly(RIDAGEYR, 7, raw = F)\nModel  8: SHBG ~ poly(RIDAGEYR, 8, raw = F)\nModel  9: SHBG ~ poly(RIDAGEYR, 9, raw = F)\nModel 10: SHBG ~ poly(RIDAGEYR, 10, raw = F)\nModel 11: SHBG ~ poly(RIDAGEYR, 11, raw = F)\n   Res.Df     RSS Df Sum of Sq         F    Pr(&gt;F)    \n1    3388 4286690                                     \n2    3387 3276833  1   1009857 1351.0353 &lt; 2.2e-16 ***\n3    3386 2873485  1    403348  539.6180 &lt; 2.2e-16 ***\n4    3385 2618205  1    255281  341.5269 &lt; 2.2e-16 ***\n5    3384 2573535  1     44670   59.7615 1.403e-14 ***\n6    3383 2558283  1     15252   20.4048 6.482e-06 ***\n7    3382 2550263  1      8020   10.7289  0.001065 ** \n8    3381 2537447  1     12817   17.1467 3.545e-05 ***\n9    3380 2529858  1      7589   10.1524  0.001454 ** \n10   3379 2525059  1      4799    6.4198  0.011330 *  \n11   3378 2524950  1       109    0.1465  0.701973    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHowever, if we did a series of partial F-tests comparing to our saturated pure fit model (mod_pure), we’d arrive at the same conclusion as the lack-of-fit \\(F\\)-test that a model with order 7 is “best”:\n\n\nCode\no1 &lt;- anova(mod_pure, mod_orth1)[\"Pr(&gt;F)\"][2,1]\no2 &lt;- anova(mod_pure, mod_orth2)[\"Pr(&gt;F)\"][2,1]\no3 &lt;- anova(mod_pure, mod_orth3)[\"Pr(&gt;F)\"][2,1]\no4 &lt;- anova(mod_pure, mod_orth4)[\"Pr(&gt;F)\"][2,1]\no5 &lt;- anova(mod_pure, mod_orth5)[\"Pr(&gt;F)\"][2,1]\no6 &lt;- anova(mod_pure, mod_orth6)[\"Pr(&gt;F)\"][2,1]\no7 &lt;- anova(mod_pure, mod_orth7)[\"Pr(&gt;F)\"][2,1]\no8 &lt;- anova(mod_pure, mod_orth8)[\"Pr(&gt;F)\"][2,1]\no9 &lt;- anova(mod_pure, mod_orth9)[\"Pr(&gt;F)\"][2,1]\no10 &lt;- anova(mod_pure, mod_orth10)[\"Pr(&gt;F)\"][2,1]\no11 &lt;- anova(mod_pure, mod_orth11)[\"Pr(&gt;F)\"][2,1]\n\nround(c('Order 1'=o1, 'Order 2'=o2, 'Order 3'=o3, 'Order 4'=o4, 'Order 5'=o5, 'Order 6'=o6, 'Order 7'=o7, 'Order 8'=o8, 'Order 9'=o9, 'Order 10'=o10, 'Order 11'=o11),4)\n\n\n Order 1  Order 2  Order 3  Order 4  Order 5  Order 6  Order 7  Order 8 \n  0.0000   0.0000   0.0000   0.0000   0.0001   0.0044   0.0234   0.2086 \n Order 9 Order 10 Order 11 \n  0.4763   0.6655   0.6374 \n\n\nHere, since we are comparing to the saturated model, we can be fairly confident that order 7 may be best. If we did not have a saturated model (e.g., no replicates), we may have chosen the order 10 as optimal based on the orthogonal polynomials (and potentially decided a lower order based on parsimony).\n\n\nWhich Model to Use?\nWe saw that the raw polynomial approach suggested a model with Order 10, and that both saturated or orthogonal polynomial approaches identified a model with Order 7. Both of these are quite high orders. We may also wish to evaluate other summaries, like the adjusted \\(R^2\\) value:\n\n\nCode\nrawr2 &lt;- c('Order 1'=summary(mod_lm)$adj.r.squared, 'Order 2'=summary(mod_poly2)$adj.r.squared, 'Order 3'=summary(mod_poly3)$adj.r.squared, 'Order 4'=summary(mod_poly4)$adj.r.squared, 'Order 5'=summary(mod_poly5)$adj.r.squared, 'Order 6'=summary(mod_poly6)$adj.r.squared, 'Order 7'=summary(mod_poly7)$adj.r.squared, 'Order 8'=summary(mod_poly8)$adj.r.squared, 'Order 9'=summary(mod_poly9)$adj.r.squared, 'Order 10'=summary(mod_poly10)$adj.r.squared, 'Order 11'=summary(mod_poly11)$adj.r.squared)\n\nortr2 &lt;- c('Orth Order 1'=summary(mod_orth1)$adj.r.squared, 'Orth Order 2'=summary(mod_orth2)$adj.r.squared, 'Orth Order 3'=summary(mod_orth3)$adj.r.squared, 'Orth Order 4'=summary(mod_orth4)$adj.r.squared, 'Orth Order 5'=summary(mod_orth5)$adj.r.squared, 'Orth Order 6'=summary(mod_orth6)$adj.r.squared, 'Orth Order 7'=summary(mod_orth7)$adj.r.squared, 'Orth Order 8'=summary(mod_orth8)$adj.r.squared, 'Orth Order 9'=summary(mod_orth9)$adj.r.squared, 'Orth Order 10'=summary(mod_orth10)$adj.r.squared, 'Orth Order 11'=summary(mod_orth11)$adj.r.squared)\n\nrbind('Raw' = rawr2, 'Orthogonal' = ortr2)\n\n\n               Order 1   Order 2   Order 3   Order 4   Order 5   Order 6\nRaw        0.003378928 0.2379376 0.3315431 0.3907489 0.4009665 0.4043407\nOrthogonal 0.003378928 0.2379376 0.3315431 0.3907489 0.4009665 0.4043407\n             Order 7   Order 8   Order 9  Order 10  Order 11\nRaw        0.4060323 0.4088426 0.4104361 0.4113803 0.4112316\nOrthogonal 0.4060323 0.4088426 0.4104361 0.4113803 0.4112316\n\n\nWe can note 2 things from these \\(R^2_{adj}\\) results:\n\nThe estimated percent of variability explained by each raw and orthogonal polynomial model of the same order are identical! This is because while the orthogonal polynomials transform our predictors to remove collinearity, the same data/information is included in the model.\nIt appears that after Order 4, the jumps in \\(R^2_{adj}\\) get smaller and smaller. This may suggest a model with Order 4 may be sufficient.\n\nTo evaluate, let’s compare the some models while also illustrating the dangers of extrapolation:\n\n\nCode\nplot(x=dat$RIDAGEYR, y=dat$SHBG, xlab='Age (Years)', ylab='SHBG (nmol/L)', col='gray85', cex.lab=0.8, cex.axis=0.8, cex=0.5, xlim=c(-10,100))\nage &lt;- -10:100\nlines(x=age, y=predict(mod_poly4, newdata=data.frame(RIDAGEYR=age)), lwd=2, col='black', lty=1)\nlines(x=age, y=predict(mod_poly7, newdata=data.frame(RIDAGEYR=age)), lwd=2, col='orangered2', lty=2)\nlines(x=age, y=predict(mod_poly10, newdata=data.frame(RIDAGEYR=age)), lwd=2, col='blue', lty=4)\nlines(x=age, y=predict(mod_poly11, newdata=data.frame(RIDAGEYR=age)), lwd=2, col='purple', lty=5)\nlegend('top', horiz=T, bty='n', xpd=T, inset=-0.15, lty=c(1,2,4,5), lwd=c(2,2,2,2), legend=c('Order 4','Order 7','Order 10','Order 11'), col=c('black','orangered2','blue','purple'))\n\n\n\n\n\nFirst, note extrapolation beyond our observed data quickly gets crazy and shouldn’t be done for our polynomial models.\nSecond, I think the choice of “best” Order 4 or Order 7 depends on our own belief of how well it fits the data without being potentially overfit. Further, Orders 10 and 11 are quite similar to Order 7, so even if we had used a method and chosen Order 10, it wouldn’t be wrong (just overly complex)."
  },
  {
    "objectID": "recitation/r19/index.html#other-modeling-approaches",
    "href": "recitation/r19/index.html#other-modeling-approaches",
    "title": "Interpreting Polynomial Regression Models, Selecting Your Highest Order Polynomial Term, and Calculus in R",
    "section": "Other Modeling Approaches",
    "text": "Other Modeling Approaches\nThere are also other modeling strategies that may be helpful beyond polynomial regression and are covered in some other lectures:\n\nSpline modeling provides more flexibility for the trends (but is still hard to interpret)\nPiecewise regression models are more interpretable and identify break/changepoints where the trend changes/shifts"
  },
  {
    "objectID": "recitation/r17/index.html",
    "href": "recitation/r17/index.html",
    "title": "ANOVA versus Linear Regression",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nANOVA versus Regression\nDifferent disciplines have different ways to refer to different approaches to modeling continuous outcomes. While I think of most things as forms of regression, you may see the following terminology:\n\nANOVA (analysis of variance): predictor can only be categorical (one-way ANOVA which we considered has one predictor, but two-way ANOVA has two categorical predictors, etc.)\nRegression: predictors are only continuous\nANCOVA (analysis of covariance): predictors can be both categorical and continuous\n\nThere are some other subtle differences with the basics of the methods as we covered them:\n\n\n\n\n\n\n\n\nFeature\nANOVA\nANCOVA/Regression\n\n\n\n\n\\(H_0\\)\nCompares group means\nEvaluates if overall model predicts \\(Y\\) better than group mean (can test group means with contrasts and/or cell means)\n\n\nAssumptions\nEqual or unequal variances\nHomogeneity of variances\n\n\nPost-Hoc Testing\nLots of procedures and corrections for multiplicity\nDo not necessarily do corrections for multiple testing\n\n\nCovariates\nCan’t accommodate\nCan easily accommodate\n\n\n\nIn practice, the biggest limitation of using ANOVA more often in practice is the fact you cannot adjust for other variables. I often choose regression/ANCOVA approaches, even if I am initially fitting a model with only 1 categorical predictor, because I never know if I’ll need to expand my model to include other covariates in the future. If I have a very well-defined problem with a continuous outcome and single categorical predictor, I may use ANOVA to provide a little more flexibility with the unequal variance assumption.\nA more philosophical question is if regression should use the same explicit post-hoc corrections that ANOVA uses. There is no clear answer here with various viewpoints:\n\nYou should correct whenever doing multiple testing (e.g., fitting linear regression with a categorical predictor) to avoid type I errors\nYou should consider corrections that are appropriate to your setting if your research is confirmatory (e.g., pre-planned, specific comparisons with corrections to control a small set of comparisons)\nYou should only report nominal (i.e., uncorrected) p-values and note no corrections were taken\nYou should conduct simultaneous inference to more dynamically adjust the estimates, p-values, and confidence intervals (e.g., GLHT versus one-by-one comparisons)"
  },
  {
    "objectID": "recitation/r15/index.html",
    "href": "recitation/r15/index.html",
    "title": "Confidence Interval Differences in lm and glm",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r15/index.html#using-confint-with-lm-vs.-by-hand",
    "href": "recitation/r15/index.html#using-confint-with-lm-vs.-by-hand",
    "title": "Confidence Interval Differences in lm and glm",
    "section": "Using confint with lm vs. By Hand",
    "text": "Using confint with lm vs. By Hand\nOur summary output for our lm object is\n\n\nCode\nsummary(lm_ci)\n\n\n\nCall:\nlm(formula = y2 ~ x4)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5470 -6.5629 -0.0575  5.6679 12.3070 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -17.8352    17.0028  -1.049   0.3249  \nx4            0.5303     0.1627   3.260   0.0115 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.782 on 8 degrees of freedom\nMultiple R-squared:  0.5705,    Adjusted R-squared:  0.5168 \nF-statistic: 10.63 on 1 and 8 DF,  p-value: 0.01153\n\n\nand its calculated confidence intervals are\n\n\nCode\nconfint(lm_ci)\n\n\n                  2.5 %     97.5 %\n(Intercept) -57.0436575 21.3732652\nx4            0.1551655  0.9053396\n\n\nIf we calculate the 95% CI by hand from the output provided we arrive at:\n\\[ \\hat{\\beta_1} \\pm t_{1-\\alpha/2, n-p-1} SE(\\hat{\\beta}_1) = 0.5303 \\pm 2.306004 \\times 0.1627 \\]\nWe pull \\(\\hat{\\beta}_1\\) and \\(SE(\\hat{\\beta}_1)\\) directly from our regression output. We can calculate the value from our \\(t\\)-distribution corresponding to \\(t_{1-\\alpha/2, n-p-1} = t_{1-0.05/2, 10-1-1}\\) by using qt(0.975, df=8). The estimated 95% CI is therefore\n\\[ (0.1551131, \\; 0.9054869) \\]\nThis does NOT exactly match the confidence interval provided by confint for our lm model. No need to be overly concerned in this situation, because it is simply due to rounding. R is able to retain many, many more decimal places of information. If we instead stored the pieces of information and had R calculate our confidence interval “by hand” we find that we do achieve agreement:\n\n\nCode\nb1 &lt;- summary(lm_ci)$coefficients['x4','Estimate']\nb1_se &lt;- summary(lm_ci)$coefficients['x4','Std. Error']\ntval &lt;- qt(0.975, df=8)\nb1 + c(-1,1)*tval*b1_se\n\n\n[1] 0.1551655 0.9053396"
  },
  {
    "objectID": "recitation/r15/index.html#using-confint-with-glm-vs.-by-hand",
    "href": "recitation/r15/index.html#using-confint-with-glm-vs.-by-hand",
    "title": "Confidence Interval Differences in lm and glm",
    "section": "Using confint with glm vs. By Hand",
    "text": "Using confint with glm vs. By Hand\nWhat would happen if we used glm instead of lm to calculate our confidence intervals:\n\n\nCode\nsummary(glm_ci)\n\n\n\nCall:\nglm(formula = y2 ~ x4)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -17.8352    17.0028  -1.049   0.3249  \nx4            0.5303     0.1627   3.260   0.0115 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 60.55895)\n\n    Null deviance: 1128.05  on 9  degrees of freedom\nResidual deviance:  484.47  on 8  degrees of freedom\nAIC: 73.184\n\nNumber of Fisher Scoring iterations: 2\n\n\nCode\nconfint(glm_ci)\n\n\nWaiting for profiling to be done...\n\n\n                  2.5 %     97.5 %\n(Intercept) -51.1600159 15.4896236\nx4            0.2114512  0.8490538\n\n\nRuh-oh, looks like this interval is very different from our lm output! In this case, we need to remember that glm uses the standard normal distribution in its calculation of the confidence interval:\n\\[ \\hat{\\beta_1} \\pm Z_{1-\\alpha/2} SE(\\hat{\\beta}_1) = 0.5303 \\pm 1.959964 \\times 0.1627 = (0.2114139,\\; 0.849186) \\]\nAgain, our “by hand” and confint intervals are slightly different due to rounding. If we have R do the calculations for us the confidence intervals match:\n\n\nCode\nb1 &lt;- summary(glm_ci)$coefficients['x4','Estimate']\nb1_se &lt;- summary(glm_ci)$coefficients['x4','Std. Error']\nzval &lt;- qnorm(0.975)\nb1 + c(-1,1)*zval*b1_se\n\n\n[1] 0.2114512 0.8490538"
  },
  {
    "objectID": "recitation/r15/index.html#when-should-lm-and-glm-more-or-less-match",
    "href": "recitation/r15/index.html#when-should-lm-and-glm-more-or-less-match",
    "title": "Confidence Interval Differences in lm and glm",
    "section": "When Should lm and glm More-or-Less Match?",
    "text": "When Should lm and glm More-or-Less Match?\nWe know that the \\(t\\)-distribution looks increasingly normal as \\(n \\to \\infty\\). Therefore, as our sample size increases the differences in our confidence interval grow increasingly small. Let’s examine the confidence intervals across a range of increasing sample sizes to see this firsthand:\n\n\nCode\nset.seed(515)\n\nsize_vec &lt;- c(10,30,100,300,1000,3000,10000,30000)\nconf_mat &lt;- matrix(nrow=length(size_vec), ncol=5, dimnames = list(paste0('N=',size_vec), c('lm LCI','glm LCI','b1','glm UCI','lm UCI')) )\n\nfor( s in 1:length(size_vec) ){\n  x &lt;- rnorm(n=size_vec[s], mean=100, sd=50)\n  y &lt;- 100 + 3*x + rnorm(n=size_vec[s], mean=0, sd=45)\n  lms &lt;- lm(y ~ x)\n  glms &lt;- glm(y ~ x)\n  conf_mat[s,] &lt;- c( confint(lms)[2,1], confint(glms)[2,1], coef(lms)[2], confint(glms)[2,2], confint(lms)[2,2])\n}\n\nlibrary(kableExtra)\nkableExtra::kbl(conf_mat, col.names=c('lm LCI','glm LCI','$\\\\hat{\\\\beta}_{1}$','glm UCI','lm UCI'), align='ccccc', escape=F) %&gt;%\n      kable_styling(bootstrap_options = \"striped\", full_width = F, position = \"left\")\n\n\n\n\n\n\n\n\n\nlm LCI\n\n\nglm LCI\n\n\n\\(\\hat{\\beta}_{1}\\)\n\n\nglm UCI\n\n\nlm UCI\n\n\n\n\n\n\nN=10\n\n\n2.435313\n\n\n2.550756\n\n\n3.204622\n\n\n3.858487\n\n\n3.973930\n\n\n\n\nN=30\n\n\n2.652286\n\n\n2.669110\n\n\n3.041944\n\n\n3.414778\n\n\n3.431602\n\n\n\n\nN=100\n\n\n2.741768\n\n\n2.744375\n\n\n2.952861\n\n\n3.161347\n\n\n3.163953\n\n\n\n\nN=300\n\n\n2.903734\n\n\n2.904129\n\n\n3.000912\n\n\n3.097694\n\n\n3.098089\n\n\n\n\nN=1000\n\n\n2.903226\n\n\n2.903297\n\n\n2.962270\n\n\n3.021243\n\n\n3.021314\n\n\n\n\nN=3000\n\n\n2.952634\n\n\n2.952647\n\n\n2.985838\n\n\n3.019028\n\n\n3.019041\n\n\n\n\nN=10000\n\n\n2.979706\n\n\n2.979708\n\n\n2.997415\n\n\n3.015122\n\n\n3.015124\n\n\n\n\nN=30000\n\n\n2.989690\n\n\n2.989691\n\n\n2.999973\n\n\n3.010256\n\n\n3.010256\n\n\n\n\n\nWe see that at \\(N=30000\\) the confidence intervals are nearly identical (the LCI differs by 0.000001)."
  },
  {
    "objectID": "recitation/r13/index.html",
    "href": "recitation/r13/index.html",
    "title": "Hat and Design Matrices in Linear Regression",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r13/index.html#unique-properties-of-the-hat-matrix",
    "href": "recitation/r13/index.html#unique-properties-of-the-hat-matrix",
    "title": "Hat and Design Matrices in Linear Regression",
    "section": "Unique Properties of the Hat Matrix",
    "text": "Unique Properties of the Hat Matrix\nWhy might we care about the hat matrix? It turns out it has some nifty properties that can make it helpful to calculate certain regression quantities, evaluate certain model diagnostics, and complete matrix operations.\nLet’s start with some of the matrix properties:\n\n\\(\\mathbf{H}\\) is a square \\(n \\times n\\) matrix\n\\(\\mathbf{H}\\) is a symmetric matrix (i.e., a symmetric matrix is a square matrix that is equal to its transpose: \\(\\mathbf{H} = \\mathbf{H}^\\top\\))\n\\(\\mathbf{H}\\) is an idempotent matrix (i.e., an idempotent matrix is one where multiplying the matrix by itself results in the same matrix: \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\))\n\nFor regression-related benefits:\n\n\\(\\mathbf{H}\\) can be used to estimate our predicted values based on the observed outcomes (hence its nickname as the “hat” matrix since it can lead us directly to our predicted outcomes): \\(\\hat{\\mathbf{Y}} = \\mathbf{H} \\mathbf{Y}\\).\nWe can easily estimate our residuals using the hat matrix: \\(\\mathbf{e} = \\mathbf{Y} - \\hat{\\mathbf{Y}} = \\mathbf{Y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}} = \\mathbf{Y} - \\mathbf{H} \\mathbf{Y} = (\\mathbf{I} - \\mathbf{H}) \\mathbf{Y}\\)\nThe diagonal of \\(\\mathbf{H}\\) represents the leverage of each observation in the analysis. This can be used as a diagnostic to evaluate for potential issues with our linear regression (which we will talk about in a few weeks).\nThe trace of \\(\\mathbf{H}\\) represents the number of parameters being estimated (i.e., if \\(p=3\\) predictors, \\(tr(\\mathbf{H})=p+1\\), where the +1 comes from the intercept term).\n\nYou may also see the hat matrix referred to as a projection matrix or influence matrix depending on the context and reference. The influence matrix terminology comes from the ability to estimate the leverage of individual observations.\nThe projection matrix terminology is because \\(\\mathbf{H}\\) represents the orthogonal (i.e., perpendicular) projection of \\(\\mathbf{Y}\\) onto the column space of \\(\\mathbf{X}\\). From our least squares approach, we can think of this as representing the distance that is minimized from our observed to predicted outcome when averaged across all observations. From StackExchange we see an visual representation of this:\n\n\n\nProjection matrix visual example from StackExchange"
  },
  {
    "objectID": "recitation/r11/index.html",
    "href": "recitation/r11/index.html",
    "title": "Nonparametric Test Choices",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r11/index.html#choice-of-parametric-versus-non-parametric-test",
    "href": "recitation/r11/index.html#choice-of-parametric-versus-non-parametric-test",
    "title": "Nonparametric Test Choices",
    "section": "Choice of Parametric versus Non-Parametric Test",
    "text": "Choice of Parametric versus Non-Parametric Test\nThere are multiple things to consider when selecting which test to use for inference, and they may vary by priority and context:\n\nAre the assumptions met? Generally, we feel most comfortable running a test if its assumptions are met. If the assumptions are not met, it may still be a valid test if we can figure out the limitations (e.g., lower power, increased bias, etc.).\nWhat interpretation is desired? In addition to meeting our assumptions, we may wish to consider what the interpretation may be. For example, if we have ordinal data we most likely will use some form of categorical data methods (e.g., chi-squared test, ordinal logistic regression, etc.). However, we might wish to use means or medians (e.g., t-tests, Mann-Whitey U-test (i.e., Wilcoxon rank sum), linear regression, quantile regression, etc.). If we have a binary outcome, depending on the study design we may wish to interpret the risk difference, risk ratio, or odds ratio. In certain circumstances, we may sacrifice the model that meets the most assumptions for something that is more interpretable.\nAre there serious consequences? If we are analyzing the primary outcome of a Phase III trial, we’ll do our best to make sure we have the best method to properly measure our estimand. However, an exploratory study may not be as concerned about getting every single statistical test “right” based on assumptions. We can also employ simulation studies to evaluate how poorly a model may be (e.g., how much worse would my power be with my given sample size, effect size, etc.)."
  },
  {
    "objectID": "recitation/r1/index.html",
    "href": "recitation/r1/index.html",
    "title": "Variance, Consistency, and Bias of Estimators",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nVariance of Estimators\nFirst, let’s start by defining some nuanced statistical terminology for an estimand, estimator, and estimate:\n\nEstimand: a precise description of what is measured, it reflects the population-level (e.g., the average weight of all dogs)\nEstimator: a method of analysis (e.g., formula, algorithm) to compute an estimate of the estimand using observed data (e.g., the sample mean is \\(\\bar{X}= \\frac{\\sum_{i=1}^{n} X_i}{n}\\), adding the weights of all \\(n\\) dogs in our sample and dividing by the sample size \\(n\\))\nEstimate: the numeric value obtained when an estimator is applied to the observed data (e.g., the sample mean may be 21.2 kg)\n\nA humorous way to remember this might be (from Richard McElreath on Twitter):\n\n\n\nMeme of estimand, estimator, and estimate\n\n\nTo estimate the population mean, \\(\\mu\\), we can use the estimator of the sample mean, \\(\\bar{X}\\). We may be interested in identifying its sampling distribution for use in statistical inference (e.g., the summarize through an estimate, and potentially to compare between groups, calculate p-values, etc.). We know that based on the central limit theorem, for any distribution with a known mean and variance, \\(\\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})\\)).\nHowever, we can note that within our sampling distribution, we have the population-level estimate of the variance, \\(\\sigma^2\\). In some cases, we may assume this variance is “known” (e.g., a \\(Z\\)-test), but in practice we often do not have enough data or confidence in past studies to make this strong assumption. Instead, we can use \\(s^2\\) as our estimator. In our slides, we note that the unbiased estimator for the sample variance is \\(s^2 = \\frac{\\sum_{i=1}^{n} (X_i-\\bar{X})^2}{n-1}\\).\nGiven that \\(s^2\\) is an estimator, we can also work to derive the distribution of the sample variance. Here we have to make a few more assumptions than with \\(\\bar{X}\\) (since we don’t have a CLT for variances). If we assume \\(X_1,X_2,...,X_{n} \\overset{iid}{\\sim} N(\\mu,\\sigma^2)\\), then we can show \\(\\frac{(n-1) s^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\). This allows us to further estimate the variance of our sample variance to be \\(V(s^2) = \\frac{2 \\sigma^4}{n-1}\\) (in practice, replacing \\(\\sigma^4\\) with \\(s^4\\)).\nThis variance of the sample variance can be helpful to understand the reasonable range of variance values we may expect from the population for (1) observing another random sample of \\(n\\) or (2) if we could observe the entire population. In practice, we often do not look at the variance of the sample variance because we are most interested in the sample mean. We also know some general properties (e.g., larger sample sizes will be more consistent).\n\n\nConsistency and Bias\nBias is a property of the expected value of a parameter of interest. For example, if our estimate \\(\\hat{\\theta}\\) is unbiased, then \\(E(\\hat{\\theta}) = \\theta\\). For any given estimator (i.e., our equation/algorithm) we can estimate the bias to be \\(\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\\). This is really only possible in simulation studies where we can set the truth for \\(\\theta\\).\nConsistency relates to the sample size of your sample. It states that as you have more and more data, our estimate should converge in probability to the estimand/truth: \\(\\hat{\\theta}_{n} \\overset{p}{\\rightarrow} \\theta\\), where \\(n\\) is the sample size going to infinity.\nIt is possible to have situations where you have both, only one, or neither of these properties (although I’d argue some are strange settings). Let’s consider estimating the sample mean for a random sample of \\(X_1,X_2,...,X_n\\):\n\n\n\n\n\n\n\n\nUnbiased?\nConsistent?\nExample Estimator\n\n\n\n\nYes\nYes\n\\(\\bar{X}\\)\n\n\nYes\nNo\n\\(X_{1}\\)\n\n\nNo\nYes\n\\(\\sum_{i=1}^{n} \\frac{X_i}{n} + \\frac{1}{n}\\)\n\n\nNo\nNo\n\\(\\frac{1}{X_1^3}\\)\n\n\n\nFor bias, we could estimate the theoretical properties given our assumptions (e.g., \\(E(\\bar{X})=\\mu\\), \\(E(X_1)=\\mu\\), \\(E(\\bar{X} + \\frac{1}{n})=\\mu+\\frac{1}{n}\\) (although this will be asymptotically unbiased), \\(E(\\frac{1}{X_1^3}) =\\) not trivial and we ignore here, but it isn’t \\(\\mu\\)). Let’s explore some plots of these estimators by simulating across a range of sample sizes for \\(N(\\mu=10,\\sigma=5)\\) distribution for consistency:\n\n\nCode\nset.seed(515) # set seed for reproducibility\nnsim &lt;- 1:100 # sample sizes to explore\nmy_dat &lt;- data.frame( n=nsim, yy=NA, yn=NA, ny=NA, nn=NA) # create data frame to save simulation results in for each unbiased/consistent combination\n\nfor( i in 1:length(nsim) ){\n  n &lt;- nsim[i]\n  simdat &lt;- rnorm(n=nsim, mean=10, sd=5)\n  my_dat$yy[i] &lt;- mean(simdat)\n  my_dat$yn[i] &lt;- simdat[1]\n  my_dat$ny[i] &lt;- mean(simdat) + 1/n\n  my_dat$nn[i] &lt;- 1/(simdat[1]^3)\n}\n\npar(mfrow=c(2,2)) # create panel figure\nplot(x=nsim, y=my_dat$yy, type='l', ylim=c(-10, 30), main='Unbiased, Consistent'); abline(h=10, col='blue')\nplot(x=nsim, y=my_dat$yn, type='l', ylim=c(-10, 30), main='Unbiased, Not Consistent'); abline(h=10, col='blue')\nplot(x=nsim, y=my_dat$ny, type='l', ylim=c(-10, 30), main='Biased, Consistent'); abline(h=10, col='blue')\nplot(x=nsim, y=my_dat$nn, type='l', ylim=c(-10, 30), main='Biased, Not Consistent'); abline(h=10, col='blue')\n\n\n\n\n\nFrom these plots, we can see that the non-consistent examples never converge to the true parameter value of \\(\\mu=10\\)."
  },
  {
    "objectID": "labs/wTemplate/index.html",
    "href": "labs/wTemplate/index.html",
    "title": "TITLE HERE",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nFirst Header"
  },
  {
    "objectID": "labs/prac9/index.html",
    "href": "labs/prac9/index.html",
    "title": "Week 9 Practice Problems",
    "section": "",
    "text": "This page includes optional practice problems, many of which are structured to assist you on the homework with Solutions provided on a separate page. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises are focusing on implementing and interpreting a multiple linear regression (MLR) model, both using existing functions and by coding our own matrices."
  },
  {
    "objectID": "labs/prac9/index.html#a-the-true-regression-equation",
    "href": "labs/prac9/index.html#a-the-true-regression-equation",
    "title": "Week 9 Practice Problems",
    "section": "1a: The True Regression Equation",
    "text": "1a: The True Regression Equation\nWrite the the multiple linear regression model for the outcome of throat pain (i.e., dependent variable) and independent variables for ASA score, gender, age, and treatment status. Be sure to define all terms."
  },
  {
    "objectID": "labs/prac9/index.html#b-fitting-the-model",
    "href": "labs/prac9/index.html#b-fitting-the-model",
    "title": "Week 9 Practice Problems",
    "section": "1b: Fitting the Model",
    "text": "1b: Fitting the Model\nFit the multiple linear regression model for the outcome of throat pain (i.e., dependent variable) and independent variables for ASA score, gender, age, and treatment status. Print the summary table output for reference in the following questions."
  },
  {
    "objectID": "labs/prac9/index.html#c-predicted-regression-equation",
    "href": "labs/prac9/index.html#c-predicted-regression-equation",
    "title": "Week 9 Practice Problems",
    "section": "1c: Predicted Regression Equation",
    "text": "1c: Predicted Regression Equation\nWrite down the predicted regression equation that describes the relationship between throat pain and your predictors based on the output."
  },
  {
    "objectID": "labs/prac9/index.html#d-intercept-interpretation-and-hypothesis-test",
    "href": "labs/prac9/index.html#d-intercept-interpretation-and-hypothesis-test",
    "title": "Week 9 Practice Problems",
    "section": "1d: Intercept Interpretation and Hypothesis Test",
    "text": "1d: Intercept Interpretation and Hypothesis Test\nWrite the hypothesis being tested in the regression output for this coefficient. What is the estimated intercept and how would you interpret it (provide a brief, but complete, interpretation)?"
  },
  {
    "objectID": "labs/prac9/index.html#e-coefficient-interpretation-and-hypothesis-test-for-binary-predictor",
    "href": "labs/prac9/index.html#e-coefficient-interpretation-and-hypothesis-test-for-binary-predictor",
    "title": "Week 9 Practice Problems",
    "section": "1e: Coefficient Interpretation and Hypothesis Test for Binary Predictor",
    "text": "1e: Coefficient Interpretation and Hypothesis Test for Binary Predictor\nWrite the hypothesis being tested in the regression output for this coefficient. What is the estimated effect of treatment and how would you interpret it (provide a brief, but complete, interpretation)?"
  },
  {
    "objectID": "labs/prac9/index.html#f-coefficient-interpretation-and-hypothesis-test-for-continuous-predictor",
    "href": "labs/prac9/index.html#f-coefficient-interpretation-and-hypothesis-test-for-continuous-predictor",
    "title": "Week 9 Practice Problems",
    "section": "1f: Coefficient Interpretation and Hypothesis Test for Continuous Predictor",
    "text": "1f: Coefficient Interpretation and Hypothesis Test for Continuous Predictor\nWrite the hypothesis being tested in the regression output for this coefficient. What is the estimated effect of age and how would you interpret it (provide a brief, but complete, interpretation)?"
  },
  {
    "objectID": "labs/prac9/index.html#g-the-partial-f-test",
    "href": "labs/prac9/index.html#g-the-partial-f-test",
    "title": "Week 9 Practice Problems",
    "section": "1g: The Partial F-test",
    "text": "1g: The Partial F-test\nEvaluate if the addition of age and gender contribute significantly to the prediction of throat pain over and above that achieved by treatment group alone. Write out the null and alternative hypotheses being tested and your conclusion."
  },
  {
    "objectID": "labs/prac9/index.html#h-the-overall-f-test",
    "href": "labs/prac9/index.html#h-the-overall-f-test",
    "title": "Week 9 Practice Problems",
    "section": "1h: The Overall F-test",
    "text": "1h: The Overall F-test\nEvaluate if the the entire set of independent variables (i.e., predictors) contribute significantly to the prediction of throat pain. Write out the null and alternative hypotheses being tested and your conclusion."
  },
  {
    "objectID": "labs/prac9/index.html#i-multicollinearity",
    "href": "labs/prac9/index.html#i-multicollinearity",
    "title": "Week 9 Practice Problems",
    "section": "1i: Multicollinearity",
    "text": "1i: Multicollinearity\nCalculate the variance inflation factors (VIFs) for the independent variables in the model. Does it appear that multicollinearity may be a concern?"
  },
  {
    "objectID": "labs/prac9/index.html#j-diagnostic-plots",
    "href": "labs/prac9/index.html#j-diagnostic-plots",
    "title": "Week 9 Practice Problems",
    "section": "1j: Diagnostic Plots",
    "text": "1j: Diagnostic Plots\nEvaluate the assumptions of our multiple linear regression model by creating diagnostic plots."
  },
  {
    "objectID": "labs/prac9/index.html#a-the-design-matrix",
    "href": "labs/prac9/index.html#a-the-design-matrix",
    "title": "Week 9 Practice Problems",
    "section": "2a: The Design Matrix",
    "text": "2a: The Design Matrix\nCreate the design matrix that we will use for our regression calculations."
  },
  {
    "objectID": "labs/prac9/index.html#b-beta-coefficients",
    "href": "labs/prac9/index.html#b-beta-coefficients",
    "title": "Week 9 Practice Problems",
    "section": "2b: Beta Coefficients",
    "text": "2b: Beta Coefficients\nCalculate the estimated beta coefficients via matrix algebra."
  },
  {
    "objectID": "labs/prac9/index.html#c-standard-error-of-beta-coefficients",
    "href": "labs/prac9/index.html#c-standard-error-of-beta-coefficients",
    "title": "Week 9 Practice Problems",
    "section": "2c: Standard Error of Beta Coefficients",
    "text": "2c: Standard Error of Beta Coefficients\nCalculate the standard error of the beta coefficients via matrix algebra."
  },
  {
    "objectID": "labs/prac9/index.html#d-test-statistics-and-p-values",
    "href": "labs/prac9/index.html#d-test-statistics-and-p-values",
    "title": "Week 9 Practice Problems",
    "section": "2d: Test Statistics and p-values",
    "text": "2d: Test Statistics and p-values\nCalculate the \\(t\\)-statistic and associated p-value based on the previous estimates from 1b and 1c."
  },
  {
    "objectID": "labs/prac9/index.html#e-confidence-and-prediction-interval",
    "href": "labs/prac9/index.html#e-confidence-and-prediction-interval",
    "title": "Week 9 Practice Problems",
    "section": "2e: Confidence and Prediction Interval",
    "text": "2e: Confidence and Prediction Interval\nCalculate the 95% confidence and prediction interval for a male in the treatment group who is 50 years old. Compare this result to the calculation provided by R when using the predict function."
  },
  {
    "objectID": "labs/prac8/index.html",
    "href": "labs/prac8/index.html",
    "title": "Week 8 Practice Problems",
    "section": "",
    "text": "This page includes optional practice problems, many of which are structured to assist you on the homework with Solutions provided on a separate page. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises are focusing on examining the diagnostic plots for various regression models simulated to violate various regression assumptions. The interpretation of models with transformations are also explored for further practice."
  },
  {
    "objectID": "labs/prac8/index.html#a.-scenario-1",
    "href": "labs/prac8/index.html#a.-scenario-1",
    "title": "Week 8 Practice Problems",
    "section": "1a. Scenario 1",
    "text": "1a. Scenario 1"
  },
  {
    "objectID": "labs/prac8/index.html#b.-scenario-2",
    "href": "labs/prac8/index.html#b.-scenario-2",
    "title": "Week 8 Practice Problems",
    "section": "1b. Scenario 2",
    "text": "1b. Scenario 2"
  },
  {
    "objectID": "labs/prac8/index.html#c.-scenario-3",
    "href": "labs/prac8/index.html#c.-scenario-3",
    "title": "Week 8 Practice Problems",
    "section": "1c. Scenario 3",
    "text": "1c. Scenario 3"
  },
  {
    "objectID": "labs/prac8/index.html#d.-scenario-4",
    "href": "labs/prac8/index.html#d.-scenario-4",
    "title": "Week 8 Practice Problems",
    "section": "1d. Scenario 4",
    "text": "1d. Scenario 4"
  },
  {
    "objectID": "labs/prac8/index.html#e.-scenario-5",
    "href": "labs/prac8/index.html#e.-scenario-5",
    "title": "Week 8 Practice Problems",
    "section": "1e. Scenario 5",
    "text": "1e. Scenario 5"
  },
  {
    "objectID": "labs/prac8/index.html#a.-y-x",
    "href": "labs/prac8/index.html#a.-y-x",
    "title": "Week 8 Practice Problems",
    "section": "2a. \\(Y\\), \\(X\\)",
    "text": "2a. \\(Y\\), \\(X\\)\n\n\nCode\nsummary(lm(PVol ~ Age, data=dat))\n\n\n\nCall:\nlm(formula = PVol ~ Age, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.527 -15.817  -4.367   6.451 205.105 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -13.3823    14.1790  -0.944    0.346    \nAge           1.1443     0.2308   4.959 1.18e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.1 on 305 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.07461,   Adjusted R-squared:  0.07158 \nF-statistic: 24.59 on 1 and 305 DF,  p-value: 1.178e-06"
  },
  {
    "objectID": "labs/prac8/index.html#b.-logy-x",
    "href": "labs/prac8/index.html#b.-logy-x",
    "title": "Week 8 Practice Problems",
    "section": "2b. \\(log(Y)\\), \\(X\\)",
    "text": "2b. \\(log(Y)\\), \\(X\\)\n\n\nCode\nsummary(lm(log(PVol) ~ Age, data=dat))\n\n\n\nCall:\nlm(formula = log(PVol) ~ Age, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.85316 -0.25833 -0.02639  0.18992  1.56272 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.83345    0.18124   15.63  &lt; 2e-16 ***\nAge          0.01820    0.00295    6.17 2.17e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3719 on 305 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.111, Adjusted R-squared:  0.108 \nF-statistic: 38.07 on 1 and 305 DF,  p-value: 2.169e-09"
  },
  {
    "objectID": "labs/prac8/index.html#c.-sqrty-x",
    "href": "labs/prac8/index.html#c.-sqrty-x",
    "title": "Week 8 Practice Problems",
    "section": "2c. \\(\\sqrt{Y}\\), \\(X\\)",
    "text": "2c. \\(\\sqrt{Y}\\), \\(X\\)\n\n\nCode\nsummary(lm(sqrt(PVol) ~ Age, data=dat))\n\n\n\nCall:\nlm(formula = sqrt(PVol) ~ Age, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6025 -0.9659 -0.1915  0.5813  8.4549 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.05356    0.75625   4.038 6.83e-05 ***\nAge          0.07016    0.01231   5.700 2.82e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.552 on 305 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.09628,   Adjusted R-squared:  0.09332 \nF-statistic:  32.5 on 1 and 305 DF,  p-value: 2.819e-08"
  },
  {
    "objectID": "labs/prac8/index.html#d.-y-logx",
    "href": "labs/prac8/index.html#d.-y-logx",
    "title": "Week 8 Practice Problems",
    "section": "2d. \\(Y\\), \\(log(X)\\)",
    "text": "2d. \\(Y\\), \\(log(X)\\)\n\n\nCode\nsummary(lm(PVol ~ log(Age), data=dat))\n\n\n\nCall:\nlm(formula = PVol ~ log(Age), data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.901 -15.970  -4.164   6.732 206.188 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -215.75      55.63  -3.878 0.000129 ***\nlog(Age)       66.33      13.55   4.895 1.59e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.13 on 305 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.07285,   Adjusted R-squared:  0.06981 \nF-statistic: 23.97 on 1 and 305 DF,  p-value: 1.593e-06"
  },
  {
    "objectID": "labs/prac8/index.html#e.-logy-logx",
    "href": "labs/prac8/index.html#e.-logy-logx",
    "title": "Week 8 Practice Problems",
    "section": "2e. \\(log(Y)\\), \\(log(X)\\)",
    "text": "2e. \\(log(Y)\\), \\(log(X)\\)\n\n\nCode\nsummary(lm(log(PVol) ~ log(Age), data=dat))\n\n\n\nCall:\nlm(formula = log(PVol) ~ log(Age), data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8618 -0.2634 -0.0178  0.1913  1.5541 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.4363     0.7104  -0.614     0.54    \nlog(Age)      1.0673     0.1730   6.169 2.18e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3719 on 305 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.1109,    Adjusted R-squared:  0.108 \nF-statistic: 38.06 on 1 and 305 DF,  p-value: 2.181e-09"
  },
  {
    "objectID": "labs/prac7/index.html",
    "href": "labs/prac7/index.html",
    "title": "Week 7 Practice Problems",
    "section": "",
    "text": "This page includes optional practice problems, many of which are structured to assist you on the homework with Solutions provided on a separate page. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises are focusing on implementing a simple linear regression and using the output to address various questions."
  },
  {
    "objectID": "labs/prac7/index.html#a-fitting-the-model",
    "href": "labs/prac7/index.html#a-fitting-the-model",
    "title": "Week 7 Practice Problems",
    "section": "1a: Fitting the Model",
    "text": "1a: Fitting the Model\nFit the simple linear regression model for an outcome of surgery duration with a single predictor for BMI. Print the summary table output for reference in the following questions."
  },
  {
    "objectID": "labs/prac7/index.html#b-fitted-least-squares-regression-equation",
    "href": "labs/prac7/index.html#b-fitted-least-squares-regression-equation",
    "title": "Week 7 Practice Problems",
    "section": "1b: Fitted Least-Squares Regression Equation",
    "text": "1b: Fitted Least-Squares Regression Equation\nWrite down the least-squares regression equation that describes the relationship between surgery duration and BMI based on your output."
  },
  {
    "objectID": "labs/prac7/index.html#c-intercept-interpretation",
    "href": "labs/prac7/index.html#c-intercept-interpretation",
    "title": "Week 7 Practice Problems",
    "section": "1c: Intercept Interpretation",
    "text": "1c: Intercept Interpretation\nWhat is the estimated intercept and how would you interpret it?"
  },
  {
    "objectID": "labs/prac7/index.html#d-slope-interpretation",
    "href": "labs/prac7/index.html#d-slope-interpretation",
    "title": "Week 7 Practice Problems",
    "section": "1d: Slope Interpretation",
    "text": "1d: Slope Interpretation\nWhat is the estimated slope and how would you interpret it?"
  },
  {
    "objectID": "labs/prac7/index.html#e-slope-hypothesis-test",
    "href": "labs/prac7/index.html#e-slope-hypothesis-test",
    "title": "Week 7 Practice Problems",
    "section": "1e: Slope Hypothesis Test",
    "text": "1e: Slope Hypothesis Test\nTest the hypothesis that the true slope is 0."
  },
  {
    "objectID": "labs/prac7/index.html#f-ci-for-slope-with-t",
    "href": "labs/prac7/index.html#f-ci-for-slope-with-t",
    "title": "Week 7 Practice Problems",
    "section": "1f: CI for Slope with \\(t\\)",
    "text": "1f: CI for Slope with \\(t\\)\nFor the estimated slope, calculate a 95% confidence interval by hand based on the output with the \\(t\\)-distribution (i.e., the “correct” calculation)."
  },
  {
    "objectID": "labs/prac7/index.html#g-ci-for-slope-with-z",
    "href": "labs/prac7/index.html#g-ci-for-slope-with-z",
    "title": "Week 7 Practice Problems",
    "section": "1g: CI for Slope with \\(Z\\)",
    "text": "1g: CI for Slope with \\(Z\\)\nFor the estimated slope, calculate a 95% confidence interval by hand based on the output with the \\(Z\\)-distribution (i.e., pretend we forgot to use the \\(t\\)-distribution and went with the simpler standard normal distribution). How different is your estimated interval and why might it be more similar (or more different) for our given context?"
  },
  {
    "objectID": "labs/prac7/index.html#h-ci-for-slope-in-r",
    "href": "labs/prac7/index.html#h-ci-for-slope-in-r",
    "title": "Week 7 Practice Problems",
    "section": "1h: CI for Slope in R",
    "text": "1h: CI for Slope in R\nFor the estimated slope, calculate a 95% confidence interval using a function in R. Which approach (in f or g) is this confidence interval most like?"
  },
  {
    "objectID": "labs/prac7/index.html#i-summary-for-slope",
    "href": "labs/prac7/index.html#i-summary-for-slope",
    "title": "Week 7 Practice Problems",
    "section": "1i: Summary for Slope",
    "text": "1i: Summary for Slope\nWrite a brief, but complete, summary of the effect of BMI on surgery duration."
  },
  {
    "objectID": "labs/prac7/index.html#j-prediction",
    "href": "labs/prac7/index.html#j-prediction",
    "title": "Week 7 Practice Problems",
    "section": "1j: Prediction",
    "text": "1j: Prediction\nWhat is the estimated surgery duration for someone with a BMI of 27.5?"
  },
  {
    "objectID": "labs/prac7/index.html#k-confidence-interval-around-prediction",
    "href": "labs/prac7/index.html#k-confidence-interval-around-prediction",
    "title": "Week 7 Practice Problems",
    "section": "1k: Confidence Interval Around Prediction",
    "text": "1k: Confidence Interval Around Prediction\nCalculate the 95% confidence interval around the mean surgery duration for the population with a BMI of 27.5."
  },
  {
    "objectID": "labs/prac7/index.html#l-prediction-interval-around-prediction",
    "href": "labs/prac7/index.html#l-prediction-interval-around-prediction",
    "title": "Week 7 Practice Problems",
    "section": "1l: Prediction Interval Around Prediction",
    "text": "1l: Prediction Interval Around Prediction\nCalculate the 95% prediction interval around the mean surgery duration for a single individual with a BMI of 27.5."
  },
  {
    "objectID": "labs/prac7/index.html#m-scattplot-and-fitted-line",
    "href": "labs/prac7/index.html#m-scattplot-and-fitted-line",
    "title": "Week 7 Practice Problems",
    "section": "1m: Scattplot and Fitted Line",
    "text": "1m: Scattplot and Fitted Line\nCreate a scatterplot with the fitted linear regression line."
  },
  {
    "objectID": "labs/prac6/index.html",
    "href": "labs/prac6/index.html",
    "title": "Week 6 Practice Problems",
    "section": "",
    "text": "This page includes optional practice problems, many of which are structured to assist you on the homework with Solutions provided on a separate page. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercise focuses on showing the total sums of squares is indeed equal to the model sums of squares plus the error sums of squares.\n\nExercise 1: Deriving the Sums of Squares\nIn our lecture slides we noted that SStotal = SSmodel + SSerror. For this problem, you will work through the math and perform some algebraic acrobatics to “prove” this is true.\nConsider \\(\\hat{Y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}X_{i}\\). Using material from the lecture slides, we can rewrite this in terms of our residual (\\(\\hat{e}_{i} = Y_{i} - \\hat{Y}_{i}\\)):\n\\(\\begin{aligned} \\frac{\\partial}{\\partial \\beta_{0}} SS_{Error} =& \\frac{\\partial}{\\partial \\beta_{0}} \\left( \\sum_{i=1}^{n} (Y_{i} - \\beta_{0} - \\beta_{1}X_{i})^2 \\right) = 0 \\to \\sum_{i=1}^{n} -2 (Y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}X_{i}) = -2 \\sum_{i=1}^{n} \\hat{e}_{i} = 0 \\\\ \\frac{\\partial}{\\partial \\beta_{1}} SS_{Error} =& \\frac{\\partial}{\\partial \\beta_{1}} \\left( \\sum_{i=1}^{n} (Y_{i} - \\beta_{0} - \\beta_{1}X_{i})^2 \\right) = 0 \\to \\sum_{i=1}^{n} -2 X_{i} (Y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}X_{i}) = -2 \\sum_{i=1}^{n} X_{i} \\hat{e}_{i} = 0 \\end{aligned}\\)\nAdditionally, we can use the following properties/definitions and hint:\n\nNote 1: \\(\\sum_{i=1}^{n} \\hat{e}_{i} = \\sum_{i=1}^{n} (Y_{i} - \\hat{Y}_{i}) = 0\\)\nNote 2: \\(\\sum_{i=1}^{n} X_{i} \\hat{e}_{i} = 0\\)\nNote 3: \\(SS_{Error} = \\sum_{i=1}^{n} (Y_{i} - \\hat{Y}_{i})^2\\)\nNote 4: \\(SS_{Model} = \\sum_{i=1}^{n} (\\hat{Y}_{i} - \\bar{Y})^2\\)\nHint:\n\n\\[\\begin{align}\nSS_{Total} =& \\sum_{i=1}^{n} (Y_{i} - \\bar{Y})^2 \\\\\n=& \\sum_{i=1}^{n} \\left( (Y_{i} - \\hat{Y}_{i}) + (\\hat{Y}_{i} - \\bar{Y}) \\right)^2 \\\\\n=& \\sum_{i=1}^{n} (Y_{i} - \\hat{Y}_{i})^2 + \\sum_{i=1}^{n} (\\hat{Y}_{i} - \\bar{Y})^2 + 2 \\sum_{i=1}^{n} (Y_{i} - \\hat{Y}_{i})(\\hat{Y}_{i} - \\bar{Y})\n\\end{align}\\]"
  },
  {
    "objectID": "labs/prac5/index.html",
    "href": "labs/prac5/index.html",
    "title": "Week 5 Practice Problems",
    "section": "",
    "text": "This page includes optional practice problems, many of which are structured to assist you on the homework with Solutions provided on a separate page. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises are focusing on implementing bootstrap resampling and permutation testing to evaluate the odds ratio of an estimate."
  },
  {
    "objectID": "labs/prac5/index.html#exercise-1-bootstrap-confidence-intervals",
    "href": "labs/prac5/index.html#exercise-1-bootstrap-confidence-intervals",
    "title": "Week 5 Practice Problems",
    "section": "Exercise 1: Bootstrap Confidence Intervals",
    "text": "Exercise 1: Bootstrap Confidence Intervals\nEstimate the 95% normal percentile and bootstrap percentile confidence intervals with 10,000 bootstrap samples to describe the variability of our estimate and:\na. compare the resulting CIs to the estimate from epi.2by2\nb. evaluate if the normal percentile CI has acceptable coverage\nc. evaluate if the bootstrap percentile CI has acceptable accuracy"
  },
  {
    "objectID": "labs/prac5/index.html#exercise-2-permutation-test-p-value",
    "href": "labs/prac5/index.html#exercise-2-permutation-test-p-value",
    "title": "Week 5 Practice Problems",
    "section": "Exercise 2: Permutation Test P-value",
    "text": "Exercise 2: Permutation Test P-value\nImplement a permutation test with 10,000 resamples to estimate a p-value for if our observed OR is significantly different from its null value for:\na. a two-sided p-value.\nb. a one-sided p-value where we hypothesize that mornings have a lower odds of complications compared to the afternoon."
  },
  {
    "objectID": "labs/prac4/index.html",
    "href": "labs/prac4/index.html",
    "title": "Week 4 Practice Problems",
    "section": "",
    "text": "This page includes optional practice problems, many of which are structured to assist you on the homework with Solutions provided on a separate page. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises are focusing on the various diagnostic testing summaries and categorical data analyses covered in the lecture videos for this week. There is a lot of different material covered, so plenty of practice exercises are provided below to give you a wide range of exposure to the topics."
  },
  {
    "objectID": "labs/prac4/index.html#a",
    "href": "labs/prac4/index.html#a",
    "title": "Week 4 Practice Problems",
    "section": "1a",
    "text": "1a\nCreate a 2x2 table for arthroplasty knee (subset based on ahrq_ccs) that summarizes the in-hospital complication rate (complication) for surgeries by time of day (hour less than 12 for morning)."
  },
  {
    "objectID": "labs/prac4/index.html#b",
    "href": "labs/prac4/index.html#b",
    "title": "Week 4 Practice Problems",
    "section": "1b",
    "text": "1b\nCalculate the risk difference, risk ratio, and odds ratio with 95% CIs by hand comparing morning surgeries versus afternoon surgeries."
  },
  {
    "objectID": "labs/prac4/index.html#c",
    "href": "labs/prac4/index.html#c",
    "title": "Week 4 Practice Problems",
    "section": "1c",
    "text": "1c\nCheck your calculations by using a package in R (e.g., epi.2by2() in the epiR package)."
  },
  {
    "objectID": "labs/prac4/index.html#d",
    "href": "labs/prac4/index.html#d",
    "title": "Week 4 Practice Problems",
    "section": "1d",
    "text": "1d\nDiscuss with the similarity or the difference between the estimated RR and OR with respect to incidence of complications."
  },
  {
    "objectID": "labs/prac4/index.html#a-1",
    "href": "labs/prac4/index.html#a-1",
    "title": "Week 4 Practice Problems",
    "section": "2a",
    "text": "2a\nCreate a contingency table summarizing our data."
  },
  {
    "objectID": "labs/prac4/index.html#b-1",
    "href": "labs/prac4/index.html#b-1",
    "title": "Week 4 Practice Problems",
    "section": "2b",
    "text": "2b\nDiscuss what test(s) could be used to evaluate the potential association between tumor volume and recurrence."
  },
  {
    "objectID": "labs/prac4/index.html#c-1",
    "href": "labs/prac4/index.html#c-1",
    "title": "Week 4 Practice Problems",
    "section": "2c",
    "text": "2c\nSelect the test you believe is most appropriate. State the null hypothesis (\\(H_0\\)) you are testing."
  },
  {
    "objectID": "labs/prac4/index.html#d-1",
    "href": "labs/prac4/index.html#d-1",
    "title": "Week 4 Practice Problems",
    "section": "2d",
    "text": "2d\nImplement the test using R and interpret your result."
  },
  {
    "objectID": "labs/prac4/index.html#a-2",
    "href": "labs/prac4/index.html#a-2",
    "title": "Week 4 Practice Problems",
    "section": "3a",
    "text": "3a\nFirst, remove all rows from the data frame that are missing vitamin D (VitaminD). One function in R that may be useful is is.na() which returns TRUE or FALSE for each item in a vector if it is missing or not, respectively."
  },
  {
    "objectID": "labs/prac4/index.html#b-2",
    "href": "labs/prac4/index.html#b-2",
    "title": "Week 4 Practice Problems",
    "section": "3b",
    "text": "3b\nCreate an ROC curve for vitamin D (VitaminD) to predict SSI (SSI)."
  },
  {
    "objectID": "labs/prac4/index.html#c-2",
    "href": "labs/prac4/index.html#c-2",
    "title": "Week 4 Practice Problems",
    "section": "3c",
    "text": "3c\nCalculate the AUC, does it suggest any potential benefit?"
  },
  {
    "objectID": "labs/prac4/index.html#d-2",
    "href": "labs/prac4/index.html#d-2",
    "title": "Week 4 Practice Problems",
    "section": "3d",
    "text": "3d\nThe pROC package includes the ci.auc function that can calculate a confidence interval for your AUC. It does this in two ways, using either method=delong or method=bootstrap, where DeLong’s approach is based on asymptotics and the bootstrap is nonparametric. Calculate the 95% confidence interval for the AUC using either method. Based on this interval, if we are interested in testing \\(H_0: AUC=0.5\\) what would we conclude?"
  },
  {
    "objectID": "labs/prac4/index.html#a-3",
    "href": "labs/prac4/index.html#a-3",
    "title": "Week 4 Practice Problems",
    "section": "4a",
    "text": "4a\nSubset the data to only include the control group (treat=0)."
  },
  {
    "objectID": "labs/prac4/index.html#b-3",
    "href": "labs/prac4/index.html#b-3",
    "title": "Week 4 Practice Problems",
    "section": "4b",
    "text": "4b\nCreate a 2x2 table summarizing an overweight BMI (i.e., BMI &gt; 25) to predict any throat pain at 30 minutes (i.e., pain &gt; 0)."
  },
  {
    "objectID": "labs/prac4/index.html#c-3",
    "href": "labs/prac4/index.html#c-3",
    "title": "Week 4 Practice Problems",
    "section": "4c",
    "text": "4c\nCalculate the sensitivity and specificity by hand from our 2x2 table and interpret."
  },
  {
    "objectID": "labs/prac4/index.html#d-3",
    "href": "labs/prac4/index.html#d-3",
    "title": "Week 4 Practice Problems",
    "section": "4d",
    "text": "4d\nDiscuss if there is any potential use of BMI as a predictor of throat pain in those who are treated with a sugar-water placebo."
  },
  {
    "objectID": "labs/prac3/index.html",
    "href": "labs/prac3/index.html",
    "title": "Week 3 Practice Problems",
    "section": "",
    "text": "This page includes optional practice problems, many of which are structured to assist you on the homework with Solutions provided on a separate page. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises focus on subsetting data objects and identifying relationships between the various assumptions in power calculations."
  },
  {
    "objectID": "labs/prac3/index.html#a",
    "href": "labs/prac3/index.html#a",
    "title": "Week 3 Practice Problems",
    "section": "1a",
    "text": "1a\nRun the following code to combine a few state-related data sets that are part of R’s available data sets:\n\n\nCode\nstates &lt;- data.frame(state.x77, state.region, state.abb)"
  },
  {
    "objectID": "labs/prac3/index.html#b",
    "href": "labs/prac3/index.html#b",
    "title": "Week 3 Practice Problems",
    "section": "1b",
    "text": "1b\nCalculate the mean (SD) life expectancy by state region."
  },
  {
    "objectID": "labs/prac3/index.html#c",
    "href": "labs/prac3/index.html#c",
    "title": "Week 3 Practice Problems",
    "section": "1c",
    "text": "1c\nSubset the four corner states (Utah, Colorado, Arizona, and New Mexico) by row name. Which state has the largest population? Which state has the lowest high school graduation rate?"
  },
  {
    "objectID": "labs/prac3/index.html#d",
    "href": "labs/prac3/index.html#d",
    "title": "Week 3 Practice Problems",
    "section": "1d",
    "text": "1d\nSubset states that either have an area greater than 90,000 miles\\(^2\\) or a percent of high school graduates below 50%. How many states meet this criteria?"
  },
  {
    "objectID": "labs/prac3/index.html#e",
    "href": "labs/prac3/index.html#e",
    "title": "Week 3 Practice Problems",
    "section": "1e",
    "text": "1e\nSubset states that have an area greater than 90,000 miles\\(^2\\) and a percent of high school graduates below 50%. How many states meet this criteria?"
  },
  {
    "objectID": "labs/prac3/index.html#f",
    "href": "labs/prac3/index.html#f",
    "title": "Week 3 Practice Problems",
    "section": "1f",
    "text": "1f\nAre there any states where the mean number of days with a minimum temperature below freezing is above 100, the murder rate is greater than 10 per 100,000 population, and the rate of illiteracy is below 1%?"
  },
  {
    "objectID": "labs/prac2/index.html",
    "href": "labs/prac2/index.html",
    "title": "Week 2 Practice Problems",
    "section": "",
    "text": "This page includes optional practice problems, many of which are structured to assist you on the homework with Solutions provided on a separate page. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course."
  },
  {
    "objectID": "labs/prac2/index.html#a",
    "href": "labs/prac2/index.html#a",
    "title": "Week 2 Practice Problems",
    "section": "2a",
    "text": "2a\nReproducibly simulate a sample of 10,000 from each of the following distributions\n\nNB(\\(r=5, p=0.6\\)), a negative binomial distribution where \\(r\\) is the target number of successful trials and \\(p\\) is the probability of success within each trial (i.e., using the default parameterization in R, see ?dnbinom)\nWeibull(\\(\\lambda=2, k=4\\)) where \\(\\lambda\\) represents the scale parameter and \\(k\\) the shape parameter"
  },
  {
    "objectID": "labs/prac2/index.html#b",
    "href": "labs/prac2/index.html#b",
    "title": "Week 2 Practice Problems",
    "section": "2b",
    "text": "2b\nDetermine the theoretical mean and standard deviation for each distribution and verify that the generated numbers have approximately the correct mean and standard deviation. Note, you can derive or look-up and cite your source for the theoretical mean and standard deviation."
  },
  {
    "objectID": "labs/prac2/index.html#c",
    "href": "labs/prac2/index.html#c",
    "title": "Week 2 Practice Problems",
    "section": "2c",
    "text": "2c\nCreate a histogram and boxplot depicting each of the mock samples."
  },
  {
    "objectID": "labs/prac2/index.html#a-1",
    "href": "labs/prac2/index.html#a-1",
    "title": "Week 2 Practice Problems",
    "section": "3a",
    "text": "3a\nAssume we are interested in an outcome that has a chi-squared distribution with 2 degrees of freedom (i.e., \\(df=2\\)). Generate and save a vector with 500 sample means (i.e., the mean of five-hundred simulated “experiments”), where each sample mean is from a sample size of 10 simulated from rchisq() in R."
  },
  {
    "objectID": "labs/prac2/index.html#b-1",
    "href": "labs/prac2/index.html#b-1",
    "title": "Week 2 Practice Problems",
    "section": "3b",
    "text": "3b\nRepeat for sample sizes of n = 20, n = 30, n = 40, and n = 50. It may be helpful to use a for loop or apply statement to tackle parts a and b simultaneously."
  },
  {
    "objectID": "labs/prac2/index.html#c-1",
    "href": "labs/prac2/index.html#c-1",
    "title": "Week 2 Practice Problems",
    "section": "3c",
    "text": "3c\nCalculate the mean and standard deviation associated with each of the five sets of \\(\\bar{x}\\) values."
  },
  {
    "objectID": "labs/prac2/index.html#d",
    "href": "labs/prac2/index.html#d",
    "title": "Week 2 Practice Problems",
    "section": "3d",
    "text": "3d\nCreate histograms of the sampling distribution of the mean, for each sample size n. Provide meaningful labeling (i.e., include a title and label the relevant axes)."
  },
  {
    "objectID": "labs/prac2/index.html#e",
    "href": "labs/prac2/index.html#e",
    "title": "Week 2 Practice Problems",
    "section": "3e",
    "text": "3e\nIs there a value of the sample size n (i.e., 10, 20, 30, 40, or 50) where the distributions begin to look normal?"
  },
  {
    "objectID": "labs/prac14/index.html",
    "href": "labs/prac14/index.html",
    "title": "Week 14 Practice Problems",
    "section": "",
    "text": "This page includes optional practice problems, many of which are structured to assist you on the homework with Solutions provided on a separate page. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises are focusing on implementing and interpreting a linear regression model using Bayesian approaches. We leverage the same data set from the MLR practice problems to help compare results with our frequentist approaches."
  },
  {
    "objectID": "labs/prac13/index.html",
    "href": "labs/prac13/index.html",
    "title": "Week 13 Practice Problems",
    "section": "",
    "text": "This page includes optional practice problems, many of which are structured to assist you on the homework with Solutions provided on a separate page. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises are focusing on some of our “advanced” topics including segmented (piecewise) regression, quantile regression, and splines to model nonlinear trends."
  },
  {
    "objectID": "labs/prac12/index.html",
    "href": "labs/prac12/index.html",
    "title": "Week 12 Practice Problems",
    "section": "",
    "text": "This page includes optional practice problems, many of which are structured to assist you on the homework with Solutions provided on a separate page. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises focus on model/variable selection and examining the potential for influential points."
  },
  {
    "objectID": "labs/prac12/index.html#a-replicating-the-lecture-results",
    "href": "labs/prac12/index.html#a-replicating-the-lecture-results",
    "title": "Week 12 Practice Problems",
    "section": "1a: Replicating the Lecture Results",
    "text": "1a: Replicating the Lecture Results\nUsing the observed data, recreate the results from our slides."
  },
  {
    "objectID": "labs/prac12/index.html#b-modification-1",
    "href": "labs/prac12/index.html#b-modification-1",
    "title": "Week 12 Practice Problems",
    "section": "1b: Modification 1",
    "text": "1b: Modification 1\nReplace first row of data with (X,Y)=(150, 115). Confirm the first observation is not an outlier, has little influence, and has high leverage."
  },
  {
    "objectID": "labs/prac12/index.html#c-modification-2",
    "href": "labs/prac12/index.html#c-modification-2",
    "title": "Week 12 Practice Problems",
    "section": "1c: Modification 2",
    "text": "1c: Modification 2\nReplace first row of data with (X,Y)=(114, 115). Confirm the first observation is an outlier, has little influence, and has low leverage."
  },
  {
    "objectID": "labs/prac12/index.html#d-modification-3",
    "href": "labs/prac12/index.html#d-modification-3",
    "title": "Week 12 Practice Problems",
    "section": "1d: Modification 3",
    "text": "1d: Modification 3\nRemove the first row of data. Confirm there are no outliers, influential points or leverage points."
  },
  {
    "objectID": "labs/prac12/index.html#a-model-selection-approaches",
    "href": "labs/prac12/index.html#a-model-selection-approaches",
    "title": "Week 12 Practice Problems",
    "section": "2a: Model Selection Approaches",
    "text": "2a: Model Selection Approaches\nUsing all subsets regression, calculate the AIC, AICc, BIC, adjusted R2, and Mallows’ Cp. Identify the optimal model for each approach. Hint: you may need to use expand.grid, for loops, or other manual coding to generate all possible models and calculate each summary."
  },
  {
    "objectID": "labs/prac12/index.html#b-variable-selection-approaches",
    "href": "labs/prac12/index.html#b-variable-selection-approaches",
    "title": "Week 12 Practice Problems",
    "section": "2b: Variable Selection Approaches",
    "text": "2b: Variable Selection Approaches\nIdentify the best model using forward selection, backward selection, and stepwise selection based on BIC."
  },
  {
    "objectID": "labs/prac12/index.html#c-picking-a-model",
    "href": "labs/prac12/index.html#c-picking-a-model",
    "title": "Week 12 Practice Problems",
    "section": "2c: Picking a Model",
    "text": "2c: Picking a Model\nBased on your results from the prior questions, what model would you propose to use in modeling PSA?"
  },
  {
    "objectID": "labs/prac12/index.html#a-power",
    "href": "labs/prac12/index.html#a-power",
    "title": "Week 12 Practice Problems",
    "section": "3a: Power",
    "text": "3a: Power\nSimulate the data set and fit the full model 1,000 times using set.seed(12521). Summarize the power/type I error for each variable."
  },
  {
    "objectID": "labs/prac12/index.html#b-the-best-model",
    "href": "labs/prac12/index.html#b-the-best-model",
    "title": "Week 12 Practice Problems",
    "section": "3b: The “Best” Model",
    "text": "3b: The “Best” Model\nSimulate the data set 1,000 times using set.seed(12521). For each data set identify the variables included in the optimal model for AIC, AICc, BIC, adjusted R2, backward selection, forward selection, stepwise selection from the null model, and stepwise selection from the full model. For model selection criteria, select the model that minimizes the criterion (i.e., we will ignore if other models might have fewer predictors but only a slightly larger criterion for sake of automating the simulation). For automatic selection models use BIC.\nSummarize both how often the “true” model is selected from each approach (i.e., with the 3 continuous and 3 categorical predictors that are non-null), as well as how often each variable was selected more generally. Does one approach seem better overall? On average?"
  },
  {
    "objectID": "labs/prac11/index.html",
    "href": "labs/prac11/index.html",
    "title": "Week 11 Practice Problems",
    "section": "",
    "text": "This page includes optional practice problems, many of which are structured to assist you on the homework with Solutions provided on a separate page. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises focus on the wonderfully wide world of applications for MLR: confounding, mediation, interactions, general linear hypothesis testing, and polynomial regression."
  },
  {
    "objectID": "labs/prac11/index.html#a-unadjusted-model-and-interpretation",
    "href": "labs/prac11/index.html#a-unadjusted-model-and-interpretation",
    "title": "Week 11 Practice Problems",
    "section": "1a: Unadjusted Model and Interpretation",
    "text": "1a: Unadjusted Model and Interpretation\nWhat is the unadjusted (crude) estimate for the association between RSI and day of the week? Write a brief, but complete, summary of the relationship between RSI and day of the week. Hint: you will need to create a new variable for day of the week."
  },
  {
    "objectID": "labs/prac11/index.html#b-adjusted-model-and-interpretation",
    "href": "labs/prac11/index.html#b-adjusted-model-and-interpretation",
    "title": "Week 11 Practice Problems",
    "section": "1b: Adjusted Model and Interpretation",
    "text": "1b: Adjusted Model and Interpretation\nAdjusting for the effect of moon phase, what is the adjusted estimate for the association between RSI and day of the week? Write a brief, but complete, summary of the relationship between RSI and day of the week adjusting for moon phase."
  },
  {
    "objectID": "labs/prac11/index.html#c-moon-phase-confounding",
    "href": "labs/prac11/index.html#c-moon-phase-confounding",
    "title": "Week 11 Practice Problems",
    "section": "1c: Moon Phase Confounding",
    "text": "1c: Moon Phase Confounding\nIs moon phase a confounder of the association between RSI and day of the week based on the operational criterion? Should you report the results from (A) or (B)? Justify your answer."
  },
  {
    "objectID": "labs/prac11/index.html#a-mediation-dag",
    "href": "labs/prac11/index.html#a-mediation-dag",
    "title": "Week 11 Practice Problems",
    "section": "2a: Mediation DAG",
    "text": "2a: Mediation DAG\nFit the three fundamental models of mediation analysis and fill in the following DAG:"
  },
  {
    "objectID": "labs/prac11/index.html#b-percent-mediated",
    "href": "labs/prac11/index.html#b-percent-mediated",
    "title": "Week 11 Practice Problems",
    "section": "2b: Percent Mediated",
    "text": "2b: Percent Mediated\nWhat is the proportion/percent mediated by age?"
  },
  {
    "objectID": "labs/prac11/index.html#c-95-ci-for-percent-mediated",
    "href": "labs/prac11/index.html#c-95-ci-for-percent-mediated",
    "title": "Week 11 Practice Problems",
    "section": "2c: 95% CI for Percent Mediated",
    "text": "2c: 95% CI for Percent Mediated\nWhat is the 95% CI and corresponding p-value for the proportion/percent mediated by age using the normal approximation to estimate the standard error (i.e., Sobel’s test)?"
  },
  {
    "objectID": "labs/prac11/index.html#a-fitted-regression-equation-with-interaction",
    "href": "labs/prac11/index.html#a-fitted-regression-equation-with-interaction",
    "title": "Week 11 Practice Problems",
    "section": "3a: Fitted Regression Equation with Interaction",
    "text": "3a: Fitted Regression Equation with Interaction\nWrite down the fitted regression equation for the regression of RSI on BMI, diabetes, and the interaction between the two. Provide an interpretation for each of the coefficients in the model (including the intercept)."
  },
  {
    "objectID": "labs/prac11/index.html#b-interaction-test",
    "href": "labs/prac11/index.html#b-interaction-test",
    "title": "Week 11 Practice Problems",
    "section": "3b: Interaction Test",
    "text": "3b: Interaction Test\nTest whether the relationship between RSI and BMI depends on whether the patient had diabetes."
  },
  {
    "objectID": "labs/prac11/index.html#c-fitted-regression-model-for-patients-without-diabetes",
    "href": "labs/prac11/index.html#c-fitted-regression-model-for-patients-without-diabetes",
    "title": "Week 11 Practice Problems",
    "section": "3c: Fitted Regression Model for Patients without Diabetes",
    "text": "3c: Fitted Regression Model for Patients without Diabetes\nWhat is the regression equation for patients who don’t have diabetes?"
  },
  {
    "objectID": "labs/prac11/index.html#d-fitted-regression-model-for-patients-with-diabetes",
    "href": "labs/prac11/index.html#d-fitted-regression-model-for-patients-with-diabetes",
    "title": "Week 11 Practice Problems",
    "section": "3d: Fitted Regression Model for Patients with Diabetes",
    "text": "3d: Fitted Regression Model for Patients with Diabetes\nWhat is the regression equation for patients with diabetes?"
  },
  {
    "objectID": "labs/prac11/index.html#e-interaction-visualization",
    "href": "labs/prac11/index.html#e-interaction-visualization",
    "title": "Week 11 Practice Problems",
    "section": "3e: Interaction Visualization",
    "text": "3e: Interaction Visualization\nCreate a scatterplot of RSI versus BMI, using different symbols and separate regression lines for patients with and without diabetes."
  },
  {
    "objectID": "labs/prac11/index.html#f-hypothesis-test-for-bmi-without-diabetes",
    "href": "labs/prac11/index.html#f-hypothesis-test-for-bmi-without-diabetes",
    "title": "Week 11 Practice Problems",
    "section": "3f: Hypothesis Test for BMI without Diabetes",
    "text": "3f: Hypothesis Test for BMI without Diabetes\nTest if the slope for BMI for those who don’t have diabetes is significantly different from 0."
  },
  {
    "objectID": "labs/prac11/index.html#g-hypothesis-test-for-bmi-with-diabetes",
    "href": "labs/prac11/index.html#g-hypothesis-test-for-bmi-with-diabetes",
    "title": "Week 11 Practice Problems",
    "section": "3g: Hypothesis Test for BMI with Diabetes",
    "text": "3g: Hypothesis Test for BMI with Diabetes\nTest if the slope for BMI for those who do have diabetes is significantly different from 0."
  },
  {
    "objectID": "labs/prac11/index.html#h-interactions-and-brief-but-complete-summaries",
    "href": "labs/prac11/index.html#h-interactions-and-brief-but-complete-summaries",
    "title": "Week 11 Practice Problems",
    "section": "3h: Interactions and Brief, but Complete, Summaries",
    "text": "3h: Interactions and Brief, but Complete, Summaries\nProvide a brief, but complete, summary of the relationship between RSI and BMI, accounting for any observed interaction with diabetes (i.e., if there is a significant interaction, interpret those who do and don’t have diabetes separately)."
  },
  {
    "objectID": "labs/prac11/index.html#a-slr",
    "href": "labs/prac11/index.html#a-slr",
    "title": "Week 11 Practice Problems",
    "section": "4a: SLR",
    "text": "4a: SLR\nFit a simple linear regression model for the outcome of RSI for in-hospital complications with age as the predictor."
  },
  {
    "objectID": "labs/prac11/index.html#b-polynomial-regression",
    "href": "labs/prac11/index.html#b-polynomial-regression",
    "title": "Week 11 Practice Problems",
    "section": "4b: Polynomial Regression",
    "text": "4b: Polynomial Regression\nFit a polynomial regression model that adds a squared term for age."
  },
  {
    "objectID": "labs/prac11/index.html#c-polynomial-plots",
    "href": "labs/prac11/index.html#c-polynomial-plots",
    "title": "Week 11 Practice Problems",
    "section": "4c: Polynomial Plots",
    "text": "4c: Polynomial Plots\nCreate a scatterplot of the data and add the predicted regression lines for each model."
  },
  {
    "objectID": "labs/prac11/index.html#d-polynomial-vs-slr",
    "href": "labs/prac11/index.html#d-polynomial-vs-slr",
    "title": "Week 11 Practice Problems",
    "section": "4d: Polynomial vs SLR",
    "text": "4d: Polynomial vs SLR\nBased on the model output and figure, is there evidence that a quadratic model may be more appropriate than the simple linear regression model?"
  },
  {
    "objectID": "labs/prac11/index.html#a-fitting-the-model",
    "href": "labs/prac11/index.html#a-fitting-the-model",
    "title": "Week 11 Practice Problems",
    "section": "5a: Fitting The Model",
    "text": "5a: Fitting The Model\nFit a linear regression model for the outcome of RSI for in-hospital complication with day of the week as a predictor for all days."
  },
  {
    "objectID": "labs/prac11/index.html#b-glht-for-two-coefficients",
    "href": "labs/prac11/index.html#b-glht-for-two-coefficients",
    "title": "Week 11 Practice Problems",
    "section": "5b: GLHT for Two Coefficients",
    "text": "5b: GLHT for Two Coefficients\nUsing a general linear hypothesis, test if the mean RSI on Tuesday and Wednesday are equal to each other."
  },
  {
    "objectID": "labs/prac11/index.html#c-glht-for-combination-of-coefficients",
    "href": "labs/prac11/index.html#c-glht-for-combination-of-coefficients",
    "title": "Week 11 Practice Problems",
    "section": "5c: GLHT for Combination of Coefficients",
    "text": "5c: GLHT for Combination of Coefficients\nUsing a general linear hypothesis, test if the mean RSI on Monday is equal to two times the RSI on Thursday."
  },
  {
    "objectID": "labs/prac11/index.html#d-glht-for-simultaneous-testing",
    "href": "labs/prac11/index.html#d-glht-for-simultaneous-testing",
    "title": "Week 11 Practice Problems",
    "section": "5d: GLHT for Simultaneous Testing",
    "text": "5d: GLHT for Simultaneous Testing\nUsing a general linear hypothesis, test both 5b and 5c simultaneously."
  },
  {
    "objectID": "labs/prac10/index.html",
    "href": "labs/prac10/index.html",
    "title": "Week 10 Practice Problems",
    "section": "",
    "text": "This page includes optional practice problems, many of which are structured to assist you on the homework with Solutions provided on a separate page. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises focus on ANOVA and categorical variables."
  },
  {
    "objectID": "labs/prac10/index.html#a-testing-homogeneity-of-the-variances-assumption",
    "href": "labs/prac10/index.html#a-testing-homogeneity-of-the-variances-assumption",
    "title": "Week 10 Practice Problems",
    "section": "1a: Testing Homogeneity of the Variances Assumption",
    "text": "1a: Testing Homogeneity of the Variances Assumption\nUse both Levene’s test and Bartlett’s test to evaluate if the variances are homogeneous (i.e., equal) across our three surgery size groups. Write the null and alternative hypothesis being tested."
  },
  {
    "objectID": "labs/prac10/index.html#b-one-way-anova-with-equal-variances",
    "href": "labs/prac10/index.html#b-one-way-anova-with-equal-variances",
    "title": "Week 10 Practice Problems",
    "section": "1b: One-Way ANOVA with Equal Variances",
    "text": "1b: One-Way ANOVA with Equal Variances\nAssume that the variances are equal across groups and test the hypothesis that the mean age between groups is equal across the three surgery size groups. State the null and alternative hypothesis being tested."
  },
  {
    "objectID": "labs/prac10/index.html#c-one-way-anova-with-unequal-variances",
    "href": "labs/prac10/index.html#c-one-way-anova-with-unequal-variances",
    "title": "Week 10 Practice Problems",
    "section": "1c: One-Way ANOVA with Unequal Variances",
    "text": "1c: One-Way ANOVA with Unequal Variances\nAssume that the variances are unequal across groups and test the hypothesis that the mean age between groups is equal across the three surgery size groups. State the null and alternative hypothesis being tested."
  },
  {
    "objectID": "labs/prac10/index.html#d-nonparametric-kruskal-wallis-anova",
    "href": "labs/prac10/index.html#d-nonparametric-kruskal-wallis-anova",
    "title": "Week 10 Practice Problems",
    "section": "1d: Nonparametric Kruskal-Wallis ANOVA",
    "text": "1d: Nonparametric Kruskal-Wallis ANOVA\nAssume that we think our normality assumption for one-way ANOVA is violated. Implement the nonparametric Kruskal-Wallis test and interpret our result. State the null and alternative hypothesis being tested."
  },
  {
    "objectID": "labs/prac10/index.html#e-post-hoc-testing",
    "href": "labs/prac10/index.html#e-post-hoc-testing",
    "title": "Week 10 Practice Problems",
    "section": "1e: Post-Hoc Testing",
    "text": "1e: Post-Hoc Testing\nRegardless of our earlier results in 1b, compare the means of each pair of groups with the Tukey HSD method and summarize the results. Was post-hoc testing necessary in this case?"
  },
  {
    "objectID": "labs/prac10/index.html#a-reference-cell-model",
    "href": "labs/prac10/index.html#a-reference-cell-model",
    "title": "Week 10 Practice Problems",
    "section": "2a: Reference Cell Model",
    "text": "2a: Reference Cell Model\nThe reference cell model is our more common approach to regression modeling in many biostatistics applications. Write down the true regression equation and any assumptions for a model where ASA status 1 and surgery size small are the reference categories."
  },
  {
    "objectID": "labs/prac10/index.html#b-partial-f-test-for-asa-status",
    "href": "labs/prac10/index.html#b-partial-f-test-for-asa-status",
    "title": "Week 10 Practice Problems",
    "section": "2b: Partial \\(F\\)-test for ASA Status",
    "text": "2b: Partial \\(F\\)-test for ASA Status\nEvaluate if ASA status contributes significantly to the model from 2a. Write the null and alternative hypotheses, test the null hypothesis, and state your conclusion."
  },
  {
    "objectID": "labs/prac10/index.html#c-partial-f-test-for-surgery-size",
    "href": "labs/prac10/index.html#c-partial-f-test-for-surgery-size",
    "title": "Week 10 Practice Problems",
    "section": "2c: Partial \\(F\\)-test for Surgery Size",
    "text": "2c: Partial \\(F\\)-test for Surgery Size\nEvaluate if surgery size contributes significantly to the model from 2a. Write the null and alternative hypotheses, test the null hypothesis, and state your conclusion."
  },
  {
    "objectID": "labs/prac10/index.html#d-overall-f-test",
    "href": "labs/prac10/index.html#d-overall-f-test",
    "title": "Week 10 Practice Problems",
    "section": "2d: Overall \\(F\\)-test",
    "text": "2d: Overall \\(F\\)-test\nEvaluate if ASA status and surgery size contribute significantly to the prediction of \\(Y\\). Write the null and alternative hypotheses, test the null hypothesis, and state your conclusion."
  },
  {
    "objectID": "labs/prac10/index.html#e-asa-status-iii-significance",
    "href": "labs/prac10/index.html#e-asa-status-iii-significance",
    "title": "Week 10 Practice Problems",
    "section": "2e: ASA Status III Significance",
    "text": "2e: ASA Status III Significance\nEvaluate the significance of an ASA status of III and provide an interpretation for the beta coefficient."
  },
  {
    "objectID": "labs/prac10/index.html#f-asa-status-iii-removal",
    "href": "labs/prac10/index.html#f-asa-status-iii-removal",
    "title": "Week 10 Practice Problems",
    "section": "2f: ASA Status III Removal",
    "text": "2f: ASA Status III Removal\nRegardless of your conclusion in 2e, if \\(p&gt;0.05\\) and we failed to reject the null hypothesis that our beta coefficient was equal to 0, should we consider removing just ASA III from our regression model and refitting (i.e., still have ASA II in the model)?"
  },
  {
    "objectID": "labs/lab8/index.html",
    "href": "labs/lab8/index.html",
    "title": "Week 8 Lab",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Labs collection, the mini-lecture content delivered at the start of class before breaking out into small groups to work on the homework assignment."
  },
  {
    "objectID": "labs/lab8/index.html#make-an-anova-function",
    "href": "labs/lab8/index.html#make-an-anova-function",
    "title": "Week 8 Lab",
    "section": "Make an ANOVA Function",
    "text": "Make an ANOVA Function\nBetter than having to recreate the same code manually each time, let’s write a function to generate a kable table of the ANOVA table for any glm or lm object we provide:\n\n\nCode\nlinreg_anova_func &lt;- function(mod, ndigits=2, p_ndigits=3, format='kable'){\n### Function to create an ANOVA table linear regression results from lm or glm\n# mod: an object with the fitted model results\n# ndigits: number of digits to round to for most values, default is 2\n# p_digits: number of digits to round the p-value to, default is 3\n# format: desired format output (default is kable):\n## \"kable\" for kable table\n## \"df\" for data frame as table\n\n  # extract outcome from the object produced by the glm or lm function\n  if( class(mod)[1] == 'glm' ){\n    y &lt;- mod$y\n  }\n  if( class(mod)[1] == 'lm' ){\n    y &lt;- mod$model[,1] # first column contains outcome data\n  }  \n  \n  ybar &lt;- mean(y)\n  yhat &lt;- predict(mod)\n  p &lt;- length(mod$coefficients)-1\n  n &lt;- length(y)\n\n  ssm &lt;- sum( (yhat-ybar)^2 )\n  sse &lt;- sum( (y-yhat)^2 )\n  sst &lt;- sum( (y-ybar)^2 )\n  \n  msm &lt;- ssm/p\n  mse &lt;- sse/(n-p-1)\n  \n  f_val &lt;- msm/mse\n  p_val &lt;- pf(f_val, df1=p, df2=n-p-1, lower.tail=FALSE)\n  \n  # Create an ANOVA table to summarize all our results:\n  p_digits &lt;- (10^(-p_ndigits))\n  p_val_tab &lt;- if(p_val&lt;p_digits){paste0('&lt;',p_digits)}else{round(p_val,p_ndigits)}\n  \n  anova_table &lt;- data.frame( 'Source' = c('Model','Error','Total'),\n                          'Sums of Squares' = c(round(ssm,ndigits), round(sse,ndigits), round(sst,ndigits)),\n                          'Degrees of Freedom' = c(p, n-p-1, n-1),\n                          'Mean Square' = c(round(msm,ndigits), round(mse,ndigits),''),\n                          'F Value' = c(round(f_val,ndigits),'',''),\n                          'p-value' = c(p_val_tab,'',''))\n\n  if( format == 'kable' ){  \n    library(kableExtra)\n    kbl(anova_table, col.names=c('Source','Sums of Squares','Degrees of Freedom','Mean Square','F-value','p-value'), align='lccccc', escape=F) %&gt;%\n      kable_styling(bootstrap_options = \"striped\", full_width = F, position = \"left\")\n  }else{\n    anova_table\n  }\n}\n\n\nNow let’s try applying the function to two data sets (our previous one, and one null), with both glm and lm:\n\n\nCode\nset.seed(515)\nx &lt;- rnorm(n=100, mean=10, sd=3) # simulate a single continuous predictor with mean=10 and SD=3\nreg &lt;- 10 + 3 * x # set the regression equation so the intercept=10 and the slope=3\ny &lt;- rnorm(n=100, mean=reg, sd=8) # simulate the outcome based on the conditional mean\nmod1 &lt;- glm(y ~ x) # fit linear regression model with glm\nlm1 &lt;- lm(y ~ x) # fit linear regression model with lm\n\nset.seed(303)\nx0 &lt;- rnorm(n=100, mean=10, sd=3) # simulate a single continuous predictor with mean=10 and SD=3\nreg0 &lt;- 10 + 0 * x0 # set the regression equation so the intercept=10 and the slope=0 (i.e., null)\ny0 &lt;- rnorm(n=100, mean=reg0, sd=8) # simulate the outcome based on the conditional mean\nmod0 &lt;- glm(y0 ~ x0) # fit linear regression model with glm\nlm0 &lt;- lm(y0 ~ x0) # fit linear regression model with lm\n\n\nFirst let’s check out our non-null scenario:\n\n\nCode\nlinreg_anova_func(mod=mod1)\n\n\n\n\n\nSource\nSums of Squares\nDegrees of Freedom\nMean Square\nF-value\np-value\n\n\n\n\nModel\n9532.85\n1\n9532.85\n113.94\n&lt;0.001\n\n\nError\n8198.93\n98\n83.66\n\n\n\n\nTotal\n17731.78\n99\n\n\n\n\n\n\n\n\n\n\nCode\nlinreg_anova_func(mod=lm1)\n\n\n\n\n\nSource\nSums of Squares\nDegrees of Freedom\nMean Square\nF-value\np-value\n\n\n\n\nModel\n9532.85\n1\n9532.85\n113.94\n&lt;0.001\n\n\nError\n8198.93\n98\n83.66\n\n\n\n\nTotal\n17731.78\n99\n\n\n\n\n\n\n\n\n\n\nNow let’s check out the null scenario and tweak our number of digits (both as a raw and formatted output version):\n\n\nCode\nlinreg_anova_func(mod=mod0, ndigits=3, p_ndigits=4, format='df')\n\n\n  Source Sums.of.Squares Degrees.of.Freedom Mean.Square F.Value p.value\n1  Model         150.943                  1     150.943   2.596  0.1104\n2  Error        5698.200                 98      58.145                \n3  Total        5849.143                 99                            \n\n\nCode\nlinreg_anova_func(mod=lm0, ndigits=3, p_ndigits=4, format='kable')\n\n\n\n\n\nSource\nSums of Squares\nDegrees of Freedom\nMean Square\nF-value\np-value\n\n\n\n\nModel\n150.943\n1\n150.943\n2.596\n0.1104\n\n\nError\n5698.200\n98\n58.145\n\n\n\n\nTotal\n5849.143\n99\n\n\n\n\n\n\n\n\n\n\nAnd as a final check, let’s compare the summary(lm) to our \\(F\\)-test result above:\n\n\nCode\nsummary(lm0)\n\n\n\nCall:\nlm(formula = y0 ~ x0)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5065  -5.0881  -0.5393   6.5040  15.8947 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.4344     2.7142   4.950 3.09e-06 ***\nx0           -0.4132     0.2564  -1.611     0.11    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.625 on 98 degrees of freedom\nMultiple R-squared:  0.02581,   Adjusted R-squared:  0.01587 \nF-statistic: 2.596 on 1 and 98 DF,  p-value: 0.1104"
  },
  {
    "objectID": "labs/lab5/index.html",
    "href": "labs/lab5/index.html",
    "title": "Week 5 Lab",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Labs collection, the mini-lecture content delivered at the start of class before breaking out into small groups to work on the homework assignment."
  },
  {
    "objectID": "labs/lab5/index.html#using-only-if",
    "href": "labs/lab5/index.html#using-only-if",
    "title": "Week 5 Lab",
    "section": "Using Only if",
    "text": "Using Only if\nThe if statement has a very simple structure:\n\n\nCode\nif( condition ){\n  run code if condition met\n}\n\n\nThe condition must be something that is either TRUE or FALSE. If the condition is met, then R will run whatever code is contained between our curly brackets.\nAs an example, let’s write three if statements that generate a sample from either a normal or exponential distribution based on an object we defined called dist as either normal or exponential:\n\n\nCode\nset.seed(561326)\ndist &lt;- 'exponential' # set desired distribution to simulate from\n\nif(dist == 'exponential'){ rexp(n=10, rate=0.5) }\nif(dist == 'normal'){ rnorm(n=10, mean=10, sd=4) }\n\n\n [1] 0.46504343 2.52015675 4.16642691 1.00602212 0.63401942 0.01873791\n [7] 2.80259386 1.18373900 5.37115794 4.67971633\n\n\nWe could also make it more informative by also printing a character string that reminds us what distribution we selected:\n\n\nCode\nset.seed(561326)\ndist &lt;- 'exponential' # set desired distribution to simulate from\n\nif(dist == 'exponential'){ \n  print('You requested the exponential distribution.')\n  rexp(n=10, rate=0.5) \n}\nif(dist == 'normal'){ \n  print('You requested the normal distribution.')  \n  rnorm(n=10, mean=10, sd=4) \n}\n\n\n[1] \"You requested the exponential distribution.\"\n [1] 0.46504343 2.52015675 4.16642691 1.00602212 0.63401942 0.01873791\n [7] 2.80259386 1.18373900 5.37115794 4.67971633\n\n\nWhile this example includes two if statements, we could add as many as desired to complete a given task."
  },
  {
    "objectID": "labs/lab5/index.html#adding-in-else",
    "href": "labs/lab5/index.html#adding-in-else",
    "title": "Week 5 Lab",
    "section": "Adding in else",
    "text": "Adding in else\nThe if statement will only run a set of code if the condition is TRUE, however we often will want to implement another set of code if the condition is FALSE. This is where the else statement comes in handy:\n\n\nCode\nif( condition ){\n  run code if condition met\n}else{\n  run this code instead if condition is NOT met\n}\n\n\nIt is also possible to add additional if statements:\n\n\nCode\nif( condition ){\n  run code if condition met\n}else if( condition2 ){\n  run this code instead if condition IS NOT met but condition2 IS met\n}else{\n  run this code only if condition and condition2 are both NOT met\n}\n\n\nFor example, perhaps we wish to simulate 100 standard normal data sets of \\(n=10\\) and want to count the number of times the mean is less than -0.5, between -0.5 and 0.5, and greater than 0.5. While there are multiple ways we could achieve this, let’s practice with if and else to achieve this:\n\n\nCode\nset.seed(98512) # set seed for reproducibility\n\nnsim &lt;- 100\ncat_vec &lt;- rep(NA, length=nsim) # initialize object to store results in\n\nfor( i in 1:nsim ){\n  asd &lt;- rnorm(n=10)\n\n  if( mean(asd) &lt; -0.5){\n    cat_vec[i] &lt;- 'mean &lt; -0.5'\n  }else if(mean(asd) &gt; 0.5){\n    cat_vec[i] &lt;- 'mean &gt; 0.5'\n  }else{\n    cat_vec[i] &lt;- 'mean between -0.5 and 0.5'\n  }\n}\n\ntable(cat_vec) #create a table summarizing results\n\n\ncat_vec\n              mean &lt; -0.5                mean &gt; 0.5 mean between -0.5 and 0.5 \n                        6                         6                        88"
  },
  {
    "objectID": "labs/lab5/index.html#subset-by-element-or-rowcolumn",
    "href": "labs/lab5/index.html#subset-by-element-or-rowcolumn",
    "title": "Week 5 Lab",
    "section": "Subset by Element or Row/Column",
    "text": "Subset by Element or Row/Column\nFor example, we have seen using the index for both vectors and matrices. Let’s create a vector of length 5 and a 5x5 (row x column) matrix to illustrate this, where the elements are the location of the vector or row/column of the matrix from 1 to 5:\n\n\nCode\nvec1 &lt;- 1:5 # create a vector of length 5\nvec1 # view the vector\n\n\n[1] 1 2 3 4 5\n\n\nCode\nmat1 &lt;- matrix( as.numeric(paste0( 1:5, matrix( rep(1:5,each=5), nrow=5))), nrow=5) #create a matrix with each coordinate being its combination of row and column\nmat1 # print the matrix\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   11   12   13   14   15\n[2,]   21   22   23   24   25\n[3,]   31   32   33   34   35\n[4,]   41   42   43   44   45\n[5,]   51   52   53   54   55\n\n\nFor example, if we wanted to see the 4th element of our vector:\n\n\nCode\nvec1[4]\n\n\n[1] 4\n\n\nWe see that it is the expected value 4.\nIf we wanted to see the 4th column of our matrix:\n\n\nCode\nmat1[,4] # select entire fourth column\n\n\n[1] 14 24 34 44 54\n\n\nWe see we have the values 14, 24, 34, 44, and 54 to reflect the 5 different rows and our column.\nLikewise, we can do the same for the entire 4th row of the our matrix:\n\n\nCode\nmat1[4,]\n\n\n[1] 41 42 43 44 45\n\n\nAdditionally, we can select a single element of the matrix by providing both a row and column coordinate. For example, we can call the 2nd row and 4th column:\n\n\nCode\nmat1[2,4]\n\n\n[1] 24\n\n\nHere we see the value is 24, as expected for the requested location."
  },
  {
    "objectID": "labs/lab5/index.html#subset-by-name",
    "href": "labs/lab5/index.html#subset-by-name",
    "title": "Week 5 Lab",
    "section": "Subset by Name",
    "text": "Subset by Name\nData frames always have column names (but can also be selected by index). Unique row names are optional for data frames (that aren’t just the number of the row). For vectors and matrices names are also optional, but can be helpful.\nWhen we do have a name, we can specifically request the data by name. Let’s create a small data frame of the dogs in the Kaizer family to see this behavior:\n\n\nCode\nmy_df &lt;- data.frame( \n  dog_name=c('Baisy','Kaizer','Yahtzee','Sheba'), \n  state=c('CO','IA','IA','VA'), \n  size=c('med','large','small','med'), \n  est_age=c(7,10,2,11)\n)\n\nmy_df # print data frame\n\n\n  dog_name state  size est_age\n1    Baisy    CO   med       7\n2   Kaizer    IA large      10\n3  Yahtzee    IA small       2\n4    Sheba    VA   med      11\n\n\nWe the data frame contains information in both numeric and character formats. If we wanted to extract the column of dog names we could request the data in three different ways:\n\n\nCode\nmy_df[,1] # by column number\n\n\n[1] \"Baisy\"   \"Kaizer\"  \"Yahtzee\" \"Sheba\"  \n\n\nCode\nmy_df[,'dog_name'] # by column name in the [row,column] structure similar to a matrix\n\n\n[1] \"Baisy\"   \"Kaizer\"  \"Yahtzee\" \"Sheba\"  \n\n\nCode\nmy_df$dog_name # by column name using only the \"$\" operator\n\n\n[1] \"Baisy\"   \"Kaizer\"  \"Yahtzee\" \"Sheba\"  \n\n\nWe can see that all 3 approaches correctly pull the column containing the names of the puppers."
  },
  {
    "objectID": "labs/lab5/index.html#subset-by-condition",
    "href": "labs/lab5/index.html#subset-by-condition",
    "title": "Week 5 Lab",
    "section": "Subset by Condition",
    "text": "Subset by Condition\nOne way we may wish to subset data is by applying a logical rule, where values with TRUE are retained and FALSE removed. For example, let’s say we wish to subset by dogs living in Iowa:\n\n\nCode\nmy_df[ my_df$state=='IA', ] # extract the dogs who live in Iowa\n\n\n  dog_name state  size est_age\n2   Kaizer    IA large      10\n3  Yahtzee    IA small       2\n\n\nWe see that Kaizer and Yahtzee were correctly subset.\nAnother similar approach is to use the which() function, which returns the indices where the statement is true:\n\n\nCode\nwhich( my_df$state == 'IA' ) # check which indices match this value\n\n\n[1] 2 3\n\n\nCode\nmy_df[ which( my_df$state == 'IA' ), ] # extract the dogs who live in Iowa\n\n\n  dog_name state  size est_age\n2   Kaizer    IA large      10\n3  Yahtzee    IA small       2\n\n\nThe use of which() resulted in the same subset that was desired.\nWe can also subset based on conditions involving our numeric variable estimated age:\n\n\nCode\nmy_df[ which( my_df$est_age &lt;= 8),] # subset to dogs less than or equal to 8 years of age\n\n\n  dog_name state  size est_age\n1    Baisy    CO   med       7\n3  Yahtzee    IA small       2\n\n\nI generally prefer using which() because I have occasionally run into issues without it."
  },
  {
    "objectID": "labs/lab5/index.html#subset-by-multiple-values",
    "href": "labs/lab5/index.html#subset-by-multiple-values",
    "title": "Week 5 Lab",
    "section": "Subset by Multiple Values",
    "text": "Subset by Multiple Values\nAnother consideration is that we can match multiple elements in a column or have multiple conditions. For multiple elements instead of == we can use %in%. An example of subsetting based on a dog living in Iowa or Virginia is\n\n\nCode\nmy_df[ which( my_df$state %in% c('IA','VA') ),] # subset dogs living in Iowa or Virginia\n\n\n  dog_name state  size est_age\n2   Kaizer    IA large      10\n3  Yahtzee    IA small       2\n4    Sheba    VA   med      11\n\n\nIf we accidentally used == we see an incorrect subset:\n\n\nCode\nmy_df[ which( my_df$state == c('IA','VA') ),] # note the incorrect subset when using ==\n\n\n  dog_name state  size est_age\n3  Yahtzee    IA small       2\n4    Sheba    VA   med      11\n\n\nWhere did Kaizer go?! It turns out the R repeated the c('IA','VA') vector so it was really matching c('IA','VA','IA','VA') to our vector of c('CO','IA','IA','VA')."
  },
  {
    "objectID": "labs/lab5/index.html#subset-with-multiple-conditions",
    "href": "labs/lab5/index.html#subset-with-multiple-conditions",
    "title": "Week 5 Lab",
    "section": "Subset with Multiple Conditions",
    "text": "Subset with Multiple Conditions\nFor multiple conditions we can either use & (and) to indicate both conditions are met or | (or) to indicate either condition is met. An example of matching Iowa and small dogs is:\n\n\nCode\nmy_df[ which( my_df$state=='IA' & my_df$size=='small' ),] # subset rows where both conditions are TRUE\n\n\n  dog_name state  size est_age\n3  Yahtzee    IA small       2\n\n\nAn example of matching Colorado or medium size is:\n\n\nCode\nmy_df[ which( my_df$state=='CO' | my_df$size=='med'),]\n\n\n  dog_name state size est_age\n1    Baisy    CO  med       7\n4    Sheba    VA  med      11"
  },
  {
    "objectID": "labs/lab5/index.html#null-scenario",
    "href": "labs/lab5/index.html#null-scenario",
    "title": "Week 5 Lab",
    "section": "Null Scenario",
    "text": "Null Scenario\nFirst we’ll start with the null scenario, where each group will have a simulated mean change of 0 over the course of the study. For each study we will save the uncorrected p-value, the Bonferroni corrected p-value, and the false discovery rate (FDR) corrected p-value. From our simulation study we will then be able to calculate:\n\nthe marginal type I error rate (i.e., the type I error rate for each individual test)\nthe family-wise type I error rate (i.e., the type I error rate within our simulated trial across all 10 tests, where even 1 falsely reject \\(H_0\\) is a failure)\nthe power (which we’ll see in the alternative scenario)\n\n\n\nCode\n# Step 1: set the seed for reproducibility\nset.seed(1117)\n\n# Step 2: define variables of interest (that will be easy to change for different simulations)\nnsim &lt;- 1000 # number of simulations\nnhyp &lt;- 10 # number of hypothesis tests\nalpha &lt;- 0.05 # type I error rate\n\n# set simulation parameters for data:\nmean_grp1 &lt;- 0\nmean_grp2 &lt;- 0\nsd_grp1 &lt;- 15\nsd_grp2 &lt;- 15\nn_grp1 &lt;- 37\nn_grp2 &lt;- 37\n\n# Step 3: initialize a matrix to store our results in for each test based on our 3 approaches\nnull_mat &lt;- matrix(nrow=nsim, ncol=nhyp*3)\ncolnames( null_mat ) &lt;- c( paste0('original_p_',1:nhyp), paste0('fdr_p_',1:nhyp), paste0('bonf_p_',1:nhyp) )\n\n# Step 4: simulate the trials and hypotheses, store the p-value results\nfor(i in 1:nsim){\n  simdat_grp1 &lt;- matrix( rnorm( n=n_grp1*nhyp, mean=mean_grp1, sd=sd_grp1 ), ncol=nhyp, nrow=n_grp1, byrow=FALSE)\n  simdat_grp2 &lt;- matrix( rnorm( n=n_grp2*nhyp, mean=mean_grp2, sd=sd_grp2 ), ncol=nhyp, nrow=n_grp2, byrow=FALSE)\n\n  pvec_orig &lt;- sapply( 1:nhyp, function(x) t.test(x=simdat_grp1[,x], y=simdat_grp2[,x])$p.value )\n  pvec_fdr &lt;- p.adjust( pvec_orig, method='fdr')  \n  pvec_bonf &lt;- p.adjust( pvec_orig, method='bonferroni')  \n\n  null_mat[i,] &lt;- c(pvec_orig, pvec_fdr, pvec_bonf)\n}\n\n\nBefore summarizing the type I error rates, let’s first examine some histograms of our overall collection of p-values from the 10 tests:\n\n\nCode\npar(mfrow=c(1,3))\nhist(null_mat[,paste0('original_p_',1:nhyp)], main='Uncorrected p-value', xlab='p-value', ylim=c(0,10000))\nhist(null_mat[,paste0('fdr_p_',1:nhyp)], main='FDR Corrected p-value', xlab='p-value', ylim=c(0,10000))\nhist(null_mat[,paste0('bonf_p_',1:nhyp)], main='Bonferroni Corrected p-Value', xlab='p-value', ylim=c(0,10000))\n\n\n\n\n\nThe uncorrected p-value from our \\(10 \\times 1000 = 10000\\) total tests reflects the expected behavior for a null response, a uniform distribution. For both corrections, FDR and Bonferroni, we see that the distribution is skewed towards higher p-values, representing that post-hoc correction being applied to our tests.\nNow let’s summarize our marginal and family-wise type I error rates:\n\n\nCode\nlibrary(kableExtra)\n\norig_marg_t1e &lt;- mean( null_mat[,paste0('original_p_',1:nhyp)] &lt;= alpha )\nfdr_marg_t1e &lt;- mean( null_mat[,paste0('fdr_p_',1:nhyp)] &lt;= alpha )\nbonf_marg_t1e &lt;- mean( null_mat[,paste0('bonf_p_',1:nhyp)] &lt;= alpha )\n\norig_fw_t1e &lt;- mean( rowSums(null_mat[,paste0('original_p_',1:nhyp)] &lt;= alpha) &gt; 0 )\nfdr_fw_t1e &lt;- mean( rowSums(null_mat[,paste0('fdr_p_',1:nhyp)] &lt;= alpha) &gt; 0  )\nbonf_fw_t1e &lt;- mean( rowSums(null_mat[,paste0('bonf_p_',1:nhyp)] &lt;= alpha) &gt; 0  )\n\nres_tab &lt;- matrix( c(orig_marg_t1e,fdr_marg_t1e,bonf_marg_t1e, orig_fw_t1e, fdr_fw_t1e, bonf_fw_t1e), ncol=2, byrow=F, \n  dimnames = list(c('Uncorrected','FDR','Bonferroni'), c('Marginal T1E','Family-wise T1E')) )\n\nkbl(res_tab, align='cc', escape=F) %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, position = \"left\")\n\n\n\n\n\n\nMarginal T1E\nFamily-wise T1E\n\n\n\n\nUncorrected\n0.0520\n0.422\n\n\nFDR\n0.0054\n0.050\n\n\nBonferroni\n0.0050\n0.050\n\n\n\n\n\n\n\nWe can also compare our estimated family-wise type I error rate for the uncorrected p-values through the formula of \\(1-(1-\\alpha)^{10} = 1-(0.95)^{10} = 0.401\\). In other words, we’d expect among 10 hypothesis tests the probability of incorrectly rejecting at least 1 test is expected to be 40.1% (or 42.2% from our simulation)."
  },
  {
    "objectID": "labs/lab5/index.html#alternative-scenario",
    "href": "labs/lab5/index.html#alternative-scenario",
    "title": "Week 5 Lab",
    "section": "Alternative Scenario",
    "text": "Alternative Scenario\nWe can use the same code above with a few tweaks to the specified effect size (i.e., 10) and also the object names in our code for some of our results:\n\n\nCode\n# Step 1: set the seed for reproducibility\nset.seed(1117)\n\n# Step 2: define variables of interest (that will be easy to change for different simulations)\nnsim &lt;- 1000 # number of simulations\nnhyp &lt;- 10 # number of hypothesis tests\nalpha &lt;- 0.05 # type I error rate\n\n# set simulation parameters for data:\nmean_grp1 &lt;- 0\nmean_grp2 &lt;- 10 # alternative scenario\nsd_grp1 &lt;- 15\nsd_grp2 &lt;- 15\nn_grp1 &lt;- 37\nn_grp2 &lt;- 37\n\n# Step 3: initialize a matrix to store our results in for each test based on our 3 approaches\nalt_mat &lt;- matrix(nrow=nsim, ncol=nhyp*3)\ncolnames( alt_mat ) &lt;- c( paste0('original_p_',1:nhyp), paste0('fdr_p_',1:nhyp), paste0('bonf_p_',1:nhyp) )\n\n# Step 4: simulate the trials and hypotheses, store the p-value results\nfor(i in 1:nsim){\n  simdat_grp1 &lt;- matrix( rnorm( n=n_grp1*nhyp, mean=mean_grp1, sd=sd_grp1 ), ncol=nhyp, nrow=n_grp1, byrow=FALSE)\n  simdat_grp2 &lt;- matrix( rnorm( n=n_grp2*nhyp, mean=mean_grp2, sd=sd_grp2 ), ncol=nhyp, nrow=n_grp2, byrow=FALSE)\n\n  pvec_orig &lt;- sapply( 1:nhyp, function(x) t.test(x=simdat_grp1[,x], y=simdat_grp2[,x])$p.value )\n  pvec_fdr &lt;- p.adjust( pvec_orig, method='fdr')  \n  pvec_bonf &lt;- p.adjust( pvec_orig, method='bonferroni')  \n\n  alt_mat[i,] &lt;- c(pvec_orig, pvec_fdr, pvec_bonf)\n}\n\n\nBefore summarizing the power, let’s examine some histograms of our overall collection of p-values from the 10 tests:\n\n\nCode\npar(mfrow=c(1,3))\nhist(alt_mat[,paste0('original_p_',1:nhyp)], main='Uncorrected p-value', xlab='p-value', ylim=c(0,10000))\nhist(alt_mat[,paste0('fdr_p_',1:nhyp)], main='FDR Corrected p-value', xlab='p-value', ylim=c(0,10000))\nhist(alt_mat[,paste0('bonf_p_',1:nhyp)], main='Bonferroni Corrected p-Value', xlab='p-value', ylim=c(0,10000))\n\n\n\n\n\nFor our alternative scenario we see the approximate behavior we’d expect, where the distribution is strongly concentrated towards lower p-values and is right skewed. One interesting observation is for our Bonferroni correct p-value, where we also see the spike for corrected p-values between 0.95 and 1.0.\nNow let’s summarize the power and add it to our results table from above:\n\n\nCode\norig_marg_pwr &lt;- mean( alt_mat[,paste0('original_p_',1:nhyp)] &lt;= alpha )\nfdr_marg_pwr &lt;- mean( alt_mat[,paste0('fdr_p_',1:nhyp)] &lt;= alpha )\nbonf_marg_pwr &lt;- mean( alt_mat[,paste0('bonf_p_',1:nhyp)] &lt;= alpha )\n\nres_tab &lt;- cbind(res_tab, 'Power'=c(orig_marg_pwr, fdr_marg_pwr, bonf_marg_pwr))\n\nkbl(res_tab, align='ccc', escape=F) %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, position = \"left\")\n\n\n\n\n\n\nMarginal T1E\nFamily-wise T1E\nPower\n\n\n\n\nUncorrected\n0.0520\n0.422\n0.8110\n\n\nFDR\n0.0054\n0.050\n0.7804\n\n\nBonferroni\n0.0050\n0.050\n0.4844\n\n\n\n\n\n\n\nHere we see the trade-offs between our different approaches:\n\nOur uncorrected approach has our desired marginal type I error rate and power, but the family-wise type I error rate is very large.\nThe FDR has a conservative marginal type I error rate, but achieves the desired \\(\\alpha\\) for our family-wise type I error rate. The power is slightly lower than the uncorrected, but not by much in this case.\nThe Bonferroni has similar type I error rates as the FDR, but it is far more conservative since its power is only 48.4%."
  },
  {
    "objectID": "labs/lab5/index.html#upping-the-number-of-tests-to-100",
    "href": "labs/lab5/index.html#upping-the-number-of-tests-to-100",
    "title": "Week 5 Lab",
    "section": "Upping the Number of Tests to 100",
    "text": "Upping the Number of Tests to 100\nLet’s just see the results for our study if we up the number of tests to 100 from 10:\n\n\nCode\n# Step 1: set the seed for reproducibility\nset.seed(11172020)\n\n# Step 2: define variables of interest (that will be easy to change for different simulations)\nnsim &lt;- 1000 # number of simulations\nnhyp &lt;- 100 # number of hypothesis tests\nalpha &lt;- 0.05 # type I error rate\n\n# set simulation parameters for data:\nmean_grp1 &lt;- 0\nmean_grp2_null &lt;- 0 # null scenario\nmean_grp2_alt &lt;- 10 # alternative scenario\nsd_grp1 &lt;- 15\nsd_grp2 &lt;- 15\nn_grp1 &lt;- 37\nn_grp2 &lt;- 37\n\n# Step 3: initialize a matrix to store our results in for each test based on our 3 approaches\nnull_mat &lt;- matrix(nrow=nsim, ncol=nhyp*3)\ncolnames( null_mat ) &lt;- c( paste0('original_p_',1:nhyp), paste0('fdr_p_',1:nhyp), paste0('bonf_p_',1:nhyp) )\n\nalt_mat &lt;- null_mat # create alt_mat to save results in for alternative scenario\n\n# Step 4: simulate the trials and hypotheses, store the p-value results\nfor(i in 1:nsim){\n  simdat_grp1 &lt;- matrix( rnorm( n=n_grp1*nhyp, mean=mean_grp1, sd=sd_grp1 ), ncol=nhyp, nrow=n_grp1, byrow=FALSE)\n  simdat_grp2 &lt;- matrix( rnorm( n=n_grp2*nhyp, mean=mean_grp2_null, sd=sd_grp2 ), ncol=nhyp, nrow=n_grp2, byrow=FALSE)\n  simdat_grp2_alt &lt;- matrix( rnorm( n=n_grp2*nhyp, mean=mean_grp2_alt, sd=sd_grp2 ), ncol=nhyp, nrow=n_grp2, byrow=FALSE)\n\n  # null scenario\n  pvec_orig &lt;- sapply( 1:nhyp, function(x) t.test(x=simdat_grp1[,x], y=simdat_grp2[,x])$p.value )\n  pvec_fdr &lt;- p.adjust( pvec_orig, method='fdr')  \n  pvec_bonf &lt;- p.adjust( pvec_orig, method='bonferroni')  \n\n  null_mat[i,] &lt;- c(pvec_orig, pvec_fdr, pvec_bonf)\n\n  # alternative scenario\n  pvec_orig_alt &lt;- sapply( 1:nhyp, function(x) t.test(x=simdat_grp1[,x], y=simdat_grp2_alt[,x])$p.value )\n  pvec_fdr_alt &lt;- p.adjust( pvec_orig_alt, method='fdr')  \n  pvec_bonf_alt &lt;- p.adjust( pvec_orig_alt, method='bonferroni')  \n\n  alt_mat[i,] &lt;- c(pvec_orig_alt, pvec_fdr_alt, pvec_bonf_alt)\n}  \n  \n# Make Table for Results\norig_marg_t1e &lt;- mean( null_mat[,paste0('original_p_',1:nhyp)] &lt;= alpha )\nfdr_marg_t1e &lt;- mean( null_mat[,paste0('fdr_p_',1:nhyp)] &lt;= alpha )\nbonf_marg_t1e &lt;- mean( null_mat[,paste0('bonf_p_',1:nhyp)] &lt;= alpha )\n\norig_fw_t1e &lt;- mean( rowSums(null_mat[,paste0('original_p_',1:nhyp)] &lt;= alpha) &gt; 0 )\nfdr_fw_t1e &lt;- mean( rowSums(null_mat[,paste0('fdr_p_',1:nhyp)] &lt;= alpha) &gt; 0  )\nbonf_fw_t1e &lt;- mean( rowSums(null_mat[,paste0('bonf_p_',1:nhyp)] &lt;= alpha) &gt; 0  )\n\norig_marg_pwr &lt;- mean( alt_mat[,paste0('original_p_',1:nhyp)] &lt;= alpha )\nfdr_marg_pwr &lt;- mean( alt_mat[,paste0('fdr_p_',1:nhyp)] &lt;= alpha )\nbonf_marg_pwr &lt;- mean( alt_mat[,paste0('bonf_p_',1:nhyp)] &lt;= alpha )\n\nres_tab &lt;- matrix( c(orig_marg_t1e,fdr_marg_t1e,bonf_marg_t1e, orig_fw_t1e, fdr_fw_t1e, bonf_fw_t1e, orig_marg_pwr, fdr_marg_pwr, bonf_marg_pwr)\n                   , ncol=3, byrow=F, dimnames = list(c('Uncorrected','FDR','Bonferroni'), c('Marginal T1E','Family-wise T1E','Power')) )\n\nkbl(res_tab, align='ccc', escape=F) %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, position = \"left\")\n\n\n\n\n\n\nMarginal T1E\nFamily-wise T1E\nPower\n\n\n\n\nUncorrected\n0.05007\n0.992\n0.80844\n\n\nFDR\n0.00042\n0.039\n0.77744\n\n\nBonferroni\n0.00040\n0.038\n0.23273"
  },
  {
    "objectID": "labs/lab5/index.html#some-thoughts",
    "href": "labs/lab5/index.html#some-thoughts",
    "title": "Week 5 Lab",
    "section": "Some Thoughts",
    "text": "Some Thoughts\nLike almost everything in statistics, there is not a single right answer for how to approach the question of multiple comparisons. However, a few considerations are highlighted below.\n\nIs the study “exploratory” or “confirmatory”? One way to distinguish these ideas is that exploratory research should be hypothesis generating, whereas confirmatory research should have a well-defined hypothesis you are attempting to evaluate. It is possible things fall along this spectrum as well, but this should be identified at the start of the study design and/or data collection process.\nSpecifying a single primary outcome, a small set of secondary outcomes, and everything else as exploratory outcomes. In this case, the primary outcome would stand on its own, and you wouldn’t have to correct for multiple comparisons. For the secondary outcomes, you may want to correct for multiple comparisons within that set, but it will partly depend on the outcomes in question and the hypotheses being tested. We rarely correct for “exploratory” outcomes in practice.\nInstead of the Bonferroni correction, use the Holm (also known as Bonferroni-Holm) correction, which is still conservative but generally has greater power. The challenge with Bonferroni-Holm or FDR, like with many other correction strategies, is it is more complex and it is not easy to generalize for a power/sample size calculation (although you can always do simulation studies!)."
  },
  {
    "objectID": "labs/lab3/index.html",
    "href": "labs/lab3/index.html",
    "title": "Week 3 Lab",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Labs collection, the mini-lecture content delivered at the start of class before breaking out into small groups to work on the homework assignment."
  },
  {
    "objectID": "labs/lab3/index.html#make-our-own-meanaverage-function",
    "href": "labs/lab3/index.html#make-our-own-meanaverage-function",
    "title": "Week 3 Lab",
    "section": "Make Our Own Mean/Average Function",
    "text": "Make Our Own Mean/Average Function\nTo get some practice at making functions, I think it is nice to start with things where we can directly check our work with existing functions or where we know what the answer should be. For this example we will write a function to calculate the mean, and then check our work with the actual mean function.\nLet’s start by writing our function to calculate a sample mean. I like to start my functions with a little commented header to (1) describe what the function does and (2) define what the argument are. This helps me whenever I come back to functions after some time has passed:\n\n\nCode\nsample_mean &lt;- function(dat){\n### This is a function to calculate the sample mean\n# dat: a vector of values to calculate the mean for\n  n &lt;- length(dat) # calculate the sample size\n  sum_x &lt;- sum(dat) # sum up all values in dat\n  sample_mean &lt;- sum_x / n # calculate the sample mean\n  return( sample_mean ) # this isn't necessary here since R should return the last line of code, but I like to include it to be explicit about what I want returned\n}\n\n\nNow that we’ve written a function, let’s generate a sample of data to compare our calculation to the existing mean function:\n\n\nCode\nset.seed(515)\nsamp_dat &lt;- rnorm( n=50, mean=14, sd=10 )\n\nsample_mean( dat = samp_dat )\n\n\n[1] 15.64743\n\n\nCode\nmean( samp_dat )\n\n\n[1] 15.64743\n\n\nThankfully, the results match! In future weeks we will discuss some more advanced pieces of building a function (e.g., returning multiple pieces of output), but for now we can use this as a foundation for writing functions."
  },
  {
    "objectID": "labs/lab3/index.html#additional-function-notes",
    "href": "labs/lab3/index.html#additional-function-notes",
    "title": "Week 3 Lab",
    "section": "Additional Function Notes",
    "text": "Additional Function Notes\nOur function for the mean above was pretty simple in that we only had one argument (dat) and one piece of information returned (sample_mean). We can write functions that have multiple arguments for a given task and/or return multiple objects. Further, you can also return objects including more than just a vector of information, such as a list object where multiple types of objects can be stored (more on this in a few weeks)."
  },
  {
    "objectID": "labs/lab3/index.html#the-apply-function",
    "href": "labs/lab3/index.html#the-apply-function",
    "title": "Week 3 Lab",
    "section": "The apply() function",
    "text": "The apply() function\nThe apply function operates on arrays, which for our purposes will generally mean 2-dimension matrices. It has three major arguments you can review in the help documentation for greater detail, but they include:\n\nX: the array to do some calculation on\nMARGIN: either 1 to indicate applying the function to each row, 2 to indicate applying the function to each column, or c(1,2) to indicate applying the function to both rows and columns (i.e., each element of the array)\nFUN: the function you wish to apply the rows and/or columns in your array\n\nFor example, let’s simulate 50 observations from a random exponential distribution and place them into a matrix with 10 rows and 5 columns:\n\n\nCode\nset.seed(515)\nmy_mat &lt;- matrix( rexp(n=50, rate=0.2), nrow=10, ncol=5) # here we are simulating the data using the \"rexp\" function within the \"matrix\" function itself so that we can include this as one line of code\n\nmy_mat # let's see what our data looks like\n\n\n           [,1]       [,2]        [,3]       [,4]        [,5]\n [1,] 12.217353 17.2375973  0.14967508  1.4993488  1.49949970\n [2,]  8.658003  4.7737718  1.18769738  3.3486665  9.63063688\n [3,] 19.308967  0.5294249  0.08559512  2.2290962 10.03797446\n [4,]  3.589007  1.8239578  2.38238175  2.4613284  0.05722932\n [5,]  3.008182  2.5826566  0.79034221  3.5895691  0.90203442\n [6,]  5.557042  1.4607429  3.96179643  0.9797761  0.38814744\n [7,]  1.783477  3.7350945  3.17351118 20.2186521  6.46094538\n [8,]  5.765822  4.0633058  4.18369018  3.3436216  6.23468792\n [9,]  2.993530  0.4073399  7.43696442  1.0924128  5.81720596\n[10,]  3.288153  2.0768742 23.82502088  9.3102057  2.14733849\n\n\nNow let’s calculate the mean value of each row (MARGIN=1) and column (MARGIN=2):\n\n\nCode\napply( X=my_mat, MARGIN=1, FUN=mean) # calculate the mean for each row\n\n\n [1] 6.520695 5.519755 6.438212 2.062781 2.174557 2.469501 7.074336 4.718225\n [9] 3.549491 8.129518\n\n\nCode\napply( X=my_mat, MARGIN=2, FUN=mean) # calculate the mean for each column\n\n\n[1] 6.616954 3.869077 4.717667 4.807268 4.317570\n\n\nWe see that we have 10 means for the 10 rows, and 5 means for the 5 columns.\n\nFor Real, What Good is MARGIN=c(1,2)\nFor MARGIN=c(1,2) there admittedly may not be as many uses with a 2-dimensional array. But if we wanted to concatenate some string to each value we could consider using the function.\nAs an example, let’s say we wanted to concatenate a “$” as though these were prices. To do this we may want to consider using the paste0() function, but we have to remember that it needs to be written in a way that can apply to each element of my_mat separately. To do this, we may first want to write a function that concatenates a dollar sign to whatever string or number is given, then use the apply() function:\n\n\nCode\nmoney_money_money &lt;- function(x){ paste0('$',x) }\napply( X=my_mat, MARGIN=c(1,2), FUN = money_money_money)\n\n\n      [,1]                [,2]                 [,3]                 \n [1,] \"$12.2173525666105\" \"$17.2375972560587\"  \"$0.149675077271274\" \n [2,] \"$8.65800315572815\" \"$4.77377176263785\"  \"$1.1876973762149\"   \n [3,] \"$19.308967021175\"  \"$0.529424920678139\" \"$0.0855951215619483\"\n [4,] \"$3.58900716041631\" \"$1.82395784649998\"  \"$2.38238175399601\"  \n [5,] \"$3.00818230491132\" \"$2.58265662286664\"  \"$0.790342205933128\" \n [6,] \"$5.55704173116321\" \"$1.46074291623291\"  \"$3.96179643438614\"  \n [7,] \"$1.78347686072811\" \"$3.73509448742561\"  \"$3.17351118428633\"  \n [8,] \"$5.76582155635065\" \"$4.0633058367693\"   \"$4.18369018451726\"  \n [9,] \"$2.9935301374644\"  \"$0.40733985370025\"  \"$7.43696442082858\"  \n[10,] \"$3.28815302578732\" \"$2.07687421934679\"  \"$23.8250208819721\"  \n      [,4]                 [,5]                 \n [1,] \"$1.49934881060734\"  \"$1.49949970077926\"  \n [2,] \"$3.34866651333869\"  \"$9.63063687975419\"  \n [3,] \"$2.22909616772085\"  \"$10.0379744633557\"  \n [4,] \"$2.46132835591099\"  \"$0.0572293228469789\"\n [5,] \"$3.58956913397824\"  \"$0.902034419123083\" \n [6,] \"$0.979776070453227\" \"$0.388147439807653\" \n [7,] \"$20.2186521411616\"  \"$6.46094538250423\"  \n [8,] \"$3.34362160181627\"  \"$6.23468791579312\"  \n [9,] \"$1.09241279074922\"  \"$5.81720596158362\"  \n[10,] \"$9.31020567151618\"  \"$2.14733849463794\"  \n\n\nWe could also define the function directly within the apply statement, or consider rounding each value to 2 decimal places before adding the dollar sign, or do both!\n\n\nCode\n# Define the function within the apply statement:\napply(  X=my_mat, MARGIN=c(1,2), FUN = function(x){ paste0('$',x) })\n\n\n      [,1]                [,2]                 [,3]                 \n [1,] \"$12.2173525666105\" \"$17.2375972560587\"  \"$0.149675077271274\" \n [2,] \"$8.65800315572815\" \"$4.77377176263785\"  \"$1.1876973762149\"   \n [3,] \"$19.308967021175\"  \"$0.529424920678139\" \"$0.0855951215619483\"\n [4,] \"$3.58900716041631\" \"$1.82395784649998\"  \"$2.38238175399601\"  \n [5,] \"$3.00818230491132\" \"$2.58265662286664\"  \"$0.790342205933128\" \n [6,] \"$5.55704173116321\" \"$1.46074291623291\"  \"$3.96179643438614\"  \n [7,] \"$1.78347686072811\" \"$3.73509448742561\"  \"$3.17351118428633\"  \n [8,] \"$5.76582155635065\" \"$4.0633058367693\"   \"$4.18369018451726\"  \n [9,] \"$2.9935301374644\"  \"$0.40733985370025\"  \"$7.43696442082858\"  \n[10,] \"$3.28815302578732\" \"$2.07687421934679\"  \"$23.8250208819721\"  \n      [,4]                 [,5]                 \n [1,] \"$1.49934881060734\"  \"$1.49949970077926\"  \n [2,] \"$3.34866651333869\"  \"$9.63063687975419\"  \n [3,] \"$2.22909616772085\"  \"$10.0379744633557\"  \n [4,] \"$2.46132835591099\"  \"$0.0572293228469789\"\n [5,] \"$3.58956913397824\"  \"$0.902034419123083\" \n [6,] \"$0.979776070453227\" \"$0.388147439807653\" \n [7,] \"$20.2186521411616\"  \"$6.46094538250423\"  \n [8,] \"$3.34362160181627\"  \"$6.23468791579312\"  \n [9,] \"$1.09241279074922\"  \"$5.81720596158362\"  \n[10,] \"$9.31020567151618\"  \"$2.14733849463794\"  \n\n\nCode\n# Let's add some rounding to the mix:\napply(  X=my_mat, MARGIN=c(1,2), FUN = function(x){ paste0('$', round(x,digits=2) ) })\n\n\n      [,1]     [,2]     [,3]     [,4]     [,5]    \n [1,] \"$12.22\" \"$17.24\" \"$0.15\"  \"$1.5\"   \"$1.5\"  \n [2,] \"$8.66\"  \"$4.77\"  \"$1.19\"  \"$3.35\"  \"$9.63\" \n [3,] \"$19.31\" \"$0.53\"  \"$0.09\"  \"$2.23\"  \"$10.04\"\n [4,] \"$3.59\"  \"$1.82\"  \"$2.38\"  \"$2.46\"  \"$0.06\" \n [5,] \"$3.01\"  \"$2.58\"  \"$0.79\"  \"$3.59\"  \"$0.9\"  \n [6,] \"$5.56\"  \"$1.46\"  \"$3.96\"  \"$0.98\"  \"$0.39\" \n [7,] \"$1.78\"  \"$3.74\"  \"$3.17\"  \"$20.22\" \"$6.46\" \n [8,] \"$5.77\"  \"$4.06\"  \"$4.18\"  \"$3.34\"  \"$6.23\" \n [9,] \"$2.99\"  \"$0.41\"  \"$7.44\"  \"$1.09\"  \"$5.82\" \n[10,] \"$3.29\"  \"$2.08\"  \"$23.83\" \"$9.31\"  \"$2.15\" \n\n\nWe see from the above output, that the first statement recreates our result with the function defined within apply. In the second result we see we successfully rounded to 2 decimal places (although R has this nasty default habit of rounding up to the desired number of digits, so extra steps are needed if we want exactly 2 decimal places)."
  },
  {
    "objectID": "labs/lab3/index.html#the-lapply-and-sapply-functions",
    "href": "labs/lab3/index.html#the-lapply-and-sapply-functions",
    "title": "Week 3 Lab",
    "section": "The lapply() and sapply() Functions",
    "text": "The lapply() and sapply() Functions\nThe lapply function applies a given function to every element of a list (or vector or data frame, etc.) and returns a list as a result. The sapply function is a wrapper to the lapply function that is a “user-friendly version” (see the help documentation for this quote) that attempts to return a vector or matrix. The “right” function to use will depend on what format you desire the output to be in (potentially to help complete some next step for plotting, summarizing, etc.). Or, in other words, there is no one right function…just different ones that are more useful in certain situations!\nA lot of the set-up for loops that we discussed in the lab for last week can be similarly set-up for sapply. For example, we could use an index approach and recreate our column-specific means from apply in the previous section:\n\n\nCode\nsapply( X=1:5, FUN = function(x) mean( my_mat[,x]) ) # notice how we define the function within sapply here to keep it on one line\n\n\n[1] 6.616954 3.869077 4.717667 4.807268 4.317570\n\n\nCode\napply( X=my_mat, MARGIN=2, FUN=mean) # check our results\n\n\n[1] 6.616954 3.869077 4.717667 4.807268 4.317570\n\n\nAnd, for completeness, we can see that lapply returns the means in a list:\n\n\nCode\nlapply( X=1:5, FUN = function(x) mean( my_mat[,x]) )\n\n\n[[1]]\n[1] 6.616954\n\n[[2]]\n[1] 3.869077\n\n[[3]]\n[1] 4.717667\n\n[[4]]\n[1] 4.807268\n\n[[5]]\n[1] 4.31757\n\n\nWe can also define X as values to be entered into a function. For example, perhaps we want to simulate sample sizes of 5, 10, and 15 from an F-distribution. Here we will use lapply so we can see the results are actually 5, 10, and 15 in length:\n\n\nCode\nlapply( X=c(5,10,15), FUN = function(x) rf(n=x, df1=15, df2=45) )\n\n\n[[1]]\n[1] 1.4579813 1.0352903 1.3878726 0.9470038 0.9811442\n\n[[2]]\n [1] 0.8471400 1.3279725 1.1581030 0.7450090 0.8182207 1.4929248 0.2743359\n [8] 0.3669082 0.7668954 1.1297963\n\n[[3]]\n [1] 1.3295476 0.9085403 0.5368008 0.9059368 1.0249046 1.1086250 0.4956291\n [8] 1.1187265 0.7603356 0.5230847 1.6806868 1.1895109 0.7753558 1.0400509\n[15] 0.9721089\n\n\nIn fact, if we tried using sapply we’d see the results are also returned as a list since this is R’s best guess as to the most appropriate object type!"
  },
  {
    "objectID": "labs/lab3/index.html#for-loops-solution",
    "href": "labs/lab3/index.html#for-loops-solution",
    "title": "Week 3 Lab",
    "section": "for loops solution",
    "text": "for loops solution\nIf we wish to use for loops, we will likely need to consider nesting two of the loops so we can (1) go through our 3 sample sizes and (2) go through the 500 simulations. For the sample sizes we will need to define a vector to reference, and also index the simulation so we can more easily store the results in a matrix we initialize for the results:\n\n\nCode\nset.seed(515)\nnsim &lt;- 500\nsizeVec &lt;- c(10,30,50)\nmeanMatrix &lt;- matrix(NA, nrow=nsim, ncol=length(sizeVec))\n\nfor( j in 1:length(sizeVec) ){ # our outer loop will go through the sample sizes\n  for( i in 1:nsim){ # our inner loop will go through the number of sims we desire\n    poisData &lt;- rpois(n=sizeVec[j], lambda=3) # simulate the data\n    meanMatrix[i,j] &lt;- mean(poisData) # save the sample mean in the ith row (1 to 500) and jth column (1 to 3)\n  }\n}\n\n\nLet’s first take a peek at the first few rows of our matrix using the head() function, then use the apply() function to calculate the column mean for each sample size:\n\n\nCode\nhead(meanMatrix) # see the top few rows of our matrix\n\n\n     [,1]     [,2] [,3]\n[1,]  2.6 3.033333 3.54\n[2,]  3.6 2.833333 2.98\n[3,]  3.6 2.666667 2.90\n[4,]  3.1 3.166667 2.92\n[5,]  2.7 2.500000 2.94\n[6,]  3.0 2.733333 2.90\n\n\nCode\napply(X=meanMatrix, MARGIN=2, FUN=mean)\n\n\n[1] 2.982200 2.975933 3.010560\n\n\nCode\napply(X=meanMatrix, MARGIN=2, FUN=sd)\n\n\n[1] 0.5317788 0.3067211 0.2472593\n\n\nWe see that the average of the sample means is pretty close to the true mean of \\(\\lambda=3\\) across all sample sizes, but that as the sample size increases the standard deviation of the sample mean (also known as the standard error) decreases. In other words, there is less variability among all estimated means when we have a larger sample size."
  },
  {
    "objectID": "labs/lab3/index.html#sapply-solution",
    "href": "labs/lab3/index.html#sapply-solution",
    "title": "Week 3 Lab",
    "section": "sapply Solution",
    "text": "sapply Solution\nWe can also nest our sapply functions within one another, and it will automatically create a matrix with 3 columns and 500 rows to store the results in. WARNING, this gets a bit nesty… (get it, nasty…but with nesting…I’ll show myself out):\n\n\nCode\nset.seed(515)\nnsim &lt;- 500\nsizeVec &lt;- c(10,30,50)\n\nmeanMatrix_sapply &lt;- sapply( X=sizeVec, FUN= function(x) sapply(X=1:500, FUN=function(y) mean(rpois(n=x,lambda=3)) ) )\n\n\nBefore we look at the first few rows and check that the mean and SD are the same as above, let’s break down what the heck is happening here:\nsapply( X=sizeVec, FUN= function(x) sapply(X=1:500, FUN=function(y) mean(rpois(n=x,lambda=3)) ) )\nThe red text is our “outer” sapply statement, which is essentially going through our sample size vector. It is applying its function(x) to the blue “inner” sapply statement. This inner statement is then going through our vector 1:500 to generate 500 samples where is it applying the function(y) (note the use of y instead of x as our function’s argument to differentiate it) to the actual generation of the Poisson sample of size x (i.e., \\(n\\)) and calculate its mean.\nPHEW!! That is…a lot. In cases like this, it may just be easier (and potentially more intuitive) to use two loops (or a different approach you’ve stumbled upon that involves neither!).\nLet’s now double check our results to see they match the above for loop results:\n\n\nCode\nhead(meanMatrix_sapply)\n\n\n     [,1]     [,2] [,3]\n[1,]  2.6 3.033333 3.54\n[2,]  3.6 2.833333 2.98\n[3,]  3.6 2.666667 2.90\n[4,]  3.1 3.166667 2.92\n[5,]  2.7 2.500000 2.94\n[6,]  3.0 2.733333 2.90\n\n\nCode\napply(X=meanMatrix_sapply, MARGIN=2, FUN=mean)\n\n\n[1] 2.982200 2.975933 3.010560\n\n\nCode\napply(X=meanMatrix_sapply, MARGIN=2, FUN=sd)\n\n\n[1] 0.5317788 0.3067211 0.2472593\n\n\nCha-ching! They do match, so even this more potentially chaotic approach worked out."
  },
  {
    "objectID": "labs/lab14/index.html",
    "href": "labs/lab14/index.html",
    "title": "Week 14 Lab",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Labs collection, the mini-lecture content delivered at the start of class before breaking out into small groups to work on the homework assignment.\n\nTBD\nContent for this week is to be decided."
  },
  {
    "objectID": "labs/lab12/index.html",
    "href": "labs/lab12/index.html",
    "title": "Week 12 Lab",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Labs collection, the mini-lecture content delivered at the start of class before breaking out into small groups to work on the homework assignment."
  },
  {
    "objectID": "labs/lab12/index.html#the-regression-equation-for-smokers-vs.-non-smokers",
    "href": "labs/lab12/index.html#the-regression-equation-for-smokers-vs.-non-smokers",
    "title": "Week 12 Lab",
    "section": "The Regression Equation for Smokers vs. Non-smokers",
    "text": "The Regression Equation for Smokers vs. Non-smokers\nGiven that smoking status is a categorical predictor, we may be interested in using the fitted regression model above and simplifying it to be from the perspective of nonsmokers and smokers. This can help us more clearly identify the intercept and slope(s) for the two groups.\nFor non-smokers we can think of this as taking our fitted regression equation from above, plugging in “0” for “Smoker”, and focusing on simplified regression equation:\n\\(\\begin{aligned} \\hat{Y}_{nonsmk} =& 0.25 + 1.94 \\times 0 + 0.24 \\times \\text{Age} + (-0.16 \\times 0 \\times \\text{Age} ) \\\\ =& 0.25 + 0.24 \\times \\text{Age} \\end{aligned}\\)\nLikewise, for smokers we can think of this as plugging in “1” for “Smoker”:\n\\(\\begin{aligned} \\hat{Y}_{smk} =& 0.25 + 1.94 \\times 1 + 0.24 \\times \\text{Age} + (-0.16 \\times 1 \\times \\text{Age} ) \\\\ =& (0.25+1.94) + (0.24-0.16) \\times \\text{Age} \\\\ =& 2.19 + 0.08 \\times \\text{Age} \\end{aligned}\\)\nWith these two regression equations, we can see that smokers and nonsmokers have different intercepts and slopes for age. We can also see that the intercept and slope for smokers is based on the sum of different \\(\\hat\\beta\\)’s. Visually this would look something like:\n\n\nCode\nfev &lt;- read.csv('../../.data/FEV_rosner.csv')\nfev$pch &lt;- 0; fev$pch[which(fev$smoke=='smoker')] &lt;- 15\nfev$col &lt;- 'green3'; fev$col[which(fev$smoke=='smoker')] &lt;- 'blue'\n\n# Code to generate picture\n#FEV figure with regression lines for FEV ~ smoking status + age + smoke*age (slide 11)\nlmfi &lt;- lm( fev ~ smoke * age, data=fev)\n\nplot(x=fev$age, y=fev$fev, pch=fev$pch,col=fev$col,cex=0.8, xlab='Age (years)', ylab='FEV (liters)', xlim=c(0,20), ylim=c(0,6))\npoints(x=fev$age[which(fev$smoke=='smoker')], y=fev$fev[which(fev$smoke=='smoker')], pch=fev$pch[which(fev$smoke=='smoker')],col=fev$col[which(fev$smoke=='smoker')],cex=0.8)\nlines(x=c(3,19), y=( coef(lmfi)[1]+coef(lmfi)[3]*c(3,19) ), lwd=3, col='green4' )\nlines(x=c(3,19), y=( coef(lmfi)[1]+coef(lmfi)[2] + coef(lmfi)[3]*c(3,19) +coef(lmfi)[4]*c(3,19) ) , lty=2, lwd=3, col='blue4')\n\nlines(x=c(-1,3), y=( coef(lmfi)[1]+coef(lmfi)[3]*c(-1,3) ), lty=4, col='green4' )\nlines(x=c(-1,3), y=( coef(lmfi)[1]+coef(lmfi)[2] + coef(lmfi)[3]*c(-1,3)+coef(lmfi)[4]*c(-1,3) ) , lty=4, col='blue4')\nabline(v=0, col='gray65', lty=1)\n\nlegend('topleft',inset=0.005,bg='white',box.col='white',pch=c(0,15),col=c('green3','blue'),legend=c('Non-Smoker','Smoker'))\n\nmtext(  expression(hat(Y) == hat(beta)[0] + hat(beta)[1] * Smoke + hat(beta)[2] * Age + hat(beta)[3] * Smoke %*% Age) )\ntext(x = 15, y = 1, expression(hat(Y)[nonsmk] == (hat(beta)[0]+0) + (hat(beta)[2] + 0) %*% Age), col='green4', cex=1.2)\ntext(x = 5.5, y = 4.4, expression(hat(Y)[smk] == (hat(beta)[0]+hat(beta)[1]) + (hat(beta)[2] + hat(beta)[3]) %*% Age), col='blue4', cex=1.2)\n\nlines(x=c(0,0), y=c(coef(lmfi)[1],coef(lmfi)[1]+coef(lmfi)[2]), col='purple', lwd=3)\ntext(x=-.25, y=1.3, expression(hat(beta)[1]), col='purple', pos=4, cex=1.2)\n\ntext(x=-.1, y=0.15, expression(hat(beta)[0]), col='green3', pos=4, cex=1.2); points(x=0,y=coef(lmfi)[1], pch=16, col='green3', cex=1.2)\ntext(x=-.1, y=2, expression(hat(beta)[0] + hat(beta)[1]), col='blue', pos=4, cex=1.2); points(x=0,y=coef(lmfi)[1]+coef(lmfi)[2], pch=16, col='blue', cex=1.2)"
  },
  {
    "objectID": "labs/lab12/index.html#interpreting-the-beta-coefficients",
    "href": "labs/lab12/index.html#interpreting-the-beta-coefficients",
    "title": "Week 12 Lab",
    "section": "Interpreting the Beta Coefficients",
    "text": "Interpreting the Beta Coefficients\nThe meaning of each \\(\\hat{\\beta}\\) is:\n\n\\(\\hat\\beta_0\\): estimated average FEV for non-smokers at age 0 (i.e., all \\(X=0\\))\n\\(\\hat\\beta_1\\): estimated difference in FEV at age 0 between smokers and non-smokers (i.e., difference in intercepts)\n\\(\\hat\\beta_2\\): estimated slope for age for non-smokers (increase in FEV per year of age for non-smokers)\n\\(\\hat\\beta_3\\): estimated difference in slope between smokers and non-smokers\n\\(\\hat\\beta_0 + \\hat\\beta_1\\): estimated average FEV for smokers at age 0 (i.e., here \\(X_1=1\\), but \\(X_1=0\\) still)\n\\(\\hat\\beta_2 + \\hat\\beta_3\\): estimated slope for age for smokers (increase in FEV per year of age for smokers)\n\nBased on our interpretation above, we may be interested in formally testing the hypothesis that any one of our estimates is significantly different from 0. Fortunately, if we wish to test any of the \\(\\hat\\beta\\)’s on their own (i.e., not \\(\\hat\\beta_i + \\hat\\beta_j\\)), we can simply use the regression output:\n\n\nCode\nmod1 &lt;- lm( fev ~ smoke * age, data=fev)\nsummary(mod1)$coefficients\n\n\n                  Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept)      0.2533955 0.08265075  3.065859  2.260474e-03\nsmokesmoker      1.9435707 0.41428463  4.691390  3.309624e-06\nage              0.2425584 0.00833154 29.113274 6.504859e-120\nsmokesmoker:age -0.1627027 0.03073753 -5.293290  1.645078e-07"
  },
  {
    "objectID": "labs/lab12/index.html#inference-for-non-smokers-i.e.-the-reference-category",
    "href": "labs/lab12/index.html#inference-for-non-smokers-i.e.-the-reference-category",
    "title": "Week 12 Lab",
    "section": "Inference for Non-Smokers (i.e., the Reference Category)",
    "text": "Inference for Non-Smokers (i.e., the Reference Category)\nFor example, for \\(\\hat\\beta_2\\) we would be testing if the estimated slope for age for non-smokers is significantly different from 0. We see that \\(p&lt;0.001\\), so we would reject \\(H_0\\) and conclude that the slope for age for non-smokers is significantly different from 0.\nThe point estimate, interval estimate, and uncertainty for the association between FEV and age for non-smokers is:\n\nPoint Estimate: \\(\\hat{\\beta}_2=\\) 0.24256 liters/year\nInterval Estimate (95% CI): \\(\\hat{\\beta}_2 \\pm t_{n-p-1,\\alpha/2} SE(\\hat{\\beta}_2) = 0.24256 \\pm 1.96(0.00833) = (0.2262, 0.2589)\\), where \\(t_{650,0.975}=1.96\\)\nUncertainty: \\(t=29.11\\) with \\(p&lt;0.001\\)\n\nOn average, FEV increases by 0.24 liters for a one year increase in age (95% CI: 0.23, 0.26 L) for non-smokers, which is significantly different from 0 (p&lt;0.001)."
  },
  {
    "objectID": "labs/lab12/index.html#inference-for-smokers",
    "href": "labs/lab12/index.html#inference-for-smokers",
    "title": "Week 12 Lab",
    "section": "Inference for Smokers",
    "text": "Inference for Smokers\nBut if we are interested in testing the slope for age for smokers we need to do a little more work! There are 3 approaches that may be of use depending on the information you have available:\n\nCalculate the standard error for \\(\\hat\\beta_2 + \\hat\\beta_3\\) by hand from the variance-covariance matrix to use in calculating 95% CI, p-values, etc.\nUse some form of general linear hypothesis testing to do this work of combining beta coefficients.\nReverse code the categorical variable so that “Age” is now the estimated slope for age for smokers."
  },
  {
    "objectID": "labs/lab12/index.html#option-1-use-the-variance-covariance-matrix",
    "href": "labs/lab12/index.html#option-1-use-the-variance-covariance-matrix",
    "title": "Week 12 Lab",
    "section": "Option 1: Use the Variance-Covariance Matrix",
    "text": "Option 1: Use the Variance-Covariance Matrix\nRecall, the variance for two random variables that are added together is \\(Var(X+Y) = Var(X) + Var(Y) + 2 \\times Cov(X,Y)\\). The standard error is then the square root of this: \\(SE(X+Y) = \\sqrt{Var(X+Y)}\\).\nWe can replace \\(X\\) and \\(Y\\) with our \\(\\hat\\beta\\)’s of interest and calculate the standard error by extracting the relevant information from the variance-covariance matrix. The variance-covariance matrix is a square matrix (i.e., same number of rows and columns) which has the variances along the main diagonal (i.e., from upper left to lower right) and the covariances between any two beta coefficients on the off-diagonal:\n\n\nCode\nvcov(mod1)\n\n\n                  (Intercept)   smokesmoker           age smokesmoker:age\n(Intercept)      0.0068311473 -0.0068311473 -6.618543e-04    6.618543e-04\nsmokesmoker     -0.0068311473  0.1716317529  6.618543e-04   -1.249970e-02\nage             -0.0006618543  0.0006618543  6.941456e-05   -6.941456e-05\nsmokesmoker:age  0.0006618543 -0.0124997012 -6.941456e-05    9.447957e-04\n\n\nFrom this matrix we see that\n\n\\(Var(\\hat\\beta_2)=6.941456e-05=0.0000694146\\)\n\\(Var(\\hat\\beta_3)=9.447957e-04=0.0009447957\\)\n\\(Cov(\\hat\\beta_2,\\hat\\beta_3)=Cov(\\hat\\beta_3,\\hat\\beta_2)=-6.941456e-05=−0.000069415\\)\n\nPlugging these values into our equation for the standard error results in:\n\\(\\begin{aligned} SE(\\hat{\\beta}_2+\\hat{\\beta}_3) =& \\sqrt{Var(\\hat{\\beta}_2) + Var(\\hat{\\beta}_3) + 2\\times Cov(\\hat{\\beta}_2,\\hat{\\beta}_3)} \\\\ =& \\sqrt{0.0000694146 + 0.0009447957 + 2(-0.000069415)} \\\\ =& \\sqrt{0.0008753803} \\\\ =& 0.029587 \\end{aligned}\\)\nThe point estimate, interval estimate, and uncertainty for the association between FEV and age for smokers is:\n\nPoint Estimate: \\(\\hat{\\beta}_2 + \\hat{\\beta}_3=0.24256+(-0.16270)=\\) 0.07986 liters/year\nInterval Estimate (95% CI): \\(0.07986 \\pm 1.96(0.029587) = (0.0219, 0.1378)\\)\nUncertainty: \\(t=\\frac{0.07986}{0.029587}=2.699\\) with \\(p=\\)2*pt(2.699,650,lower.tail=F)\\(=0.007\\)\n\nOn average, FEV increases by 0.08 liters for a one year increase in age (95% CI: 0.02, 0.14 L) for smokers, which is significantly different from 0 (p=0.007)."
  },
  {
    "objectID": "labs/lab12/index.html#option-2-use-general-linear-hypothesis-tests",
    "href": "labs/lab12/index.html#option-2-use-general-linear-hypothesis-tests",
    "title": "Week 12 Lab",
    "section": "Option 2: Use General Linear Hypothesis Tests",
    "text": "Option 2: Use General Linear Hypothesis Tests\nWe can also use the glht function in the multcomp package in R to calculate the interval estimate and uncertainty:\n\n\nCode\nlibrary(multcomp)\nK &lt;- matrix(c(0,0,1,1),nrow=1) #beta2+beta3\nsummary(glht(mod1, linfct=K))\n\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: lm(formula = fev ~ smoke * age, data = fev)\n\nLinear Hypotheses:\n       Estimate Std. Error t value Pr(&gt;|t|)   \n1 == 0  0.07986    0.02959   2.699  0.00713 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nWe see that the results match those of Option 1.\nNOTE: For glm models the normal approximation is used, whereas for lm models the t-distribution is used."
  },
  {
    "objectID": "labs/lab12/index.html#option-3-use-reverse-coding-of-the-categorical-predictor",
    "href": "labs/lab12/index.html#option-3-use-reverse-coding-of-the-categorical-predictor",
    "title": "Week 12 Lab",
    "section": "Option 3: Use Reverse Coding of the Categorical Predictor",
    "text": "Option 3: Use Reverse Coding of the Categorical Predictor\nOur third option is to reverse the coding for smoke, then evaluate the estimate of the new \\(\\hat\\beta_2^*\\):\n\n\nCode\nfev$nonsmoke &lt;- factor(fev$smoke, levels=c('smoker','nonsmoker'))\nmod1_reverse &lt;- lm( fev ~ nonsmoke*age, data=fev )\nsummary(mod1_reverse)$coefficients\n\n\n                         Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)            2.19696626 0.40595641  5.411828 8.782811e-08\nnonsmokenonsmoker     -1.94357074 0.41428463 -4.691390 3.309624e-06\nage                    0.07985574 0.02958684  2.699029 7.134971e-03\nnonsmokenonsmoker:age  0.16270267 0.03073753  5.293290 1.645078e-07\n\n\nWe see based on these results that the estimate, standard error, t-value, and p-value for age match our estimates from Options 1 and 2 above.\nThe caveat with this approach is that we would still have to fit the “original” model to estimate the effect for non-smokers without extra work, or we could do Option 1 or Option 2 for non-smokers with this model."
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "BIOS 6618 Labs and Practice Problems",
    "section": "",
    "text": "This page includes our class’ weekly labs and practice problems relating to the lecture material. The practice problems are meant to provide additional examples of methods and coding approaches that can be used in completing the homework assignments and projects. The BIOS 6618 Recitation page also includes responses to specific questions submitted by students that may provide additional examples and information.\n\n\n\n\n\n\n\n\n\nWeek/Topics\nWeek 2 Normal Distribution and Central Limit Theorem\nLab and Practice Problem Description\nThree ways you can create vectors, different examples of using for loops, and the derivation of E(X) for an exponential distribution.\nThe practice problems examine estimating the sample mean and sample variance for a Poisson distribution across a range of sample sizes, the theoretical and asymptotic results for some distributions, and the central limit theorem for chi-squared distributed data.\nLinks\nLab; Practice Problems; Solutions\n\n\n\nWeek 3 Hypothesis Testing and Power\nAn introduction to writing your own functions in R and using the apply family of functions in place of for loops.\nThe practice problems work on subsetting a data frame of US states by various criteria, as well as an exercise identifying the relationship of how the various quantities in a power calculation change based on our lectures.\nLab; Practice Problems; Solutions\n\n\n\nWeek 4 Diagnostic Testing and 2x2 Tables\nExploring power calculations with known and unknown SD and creating plots across a range of scenarios using ggplot2.\nPractice problems examine RD/RR/OR, categorical data hypothesis testing, ROC curves, and sensitivity/specificity.\nLab; Practice Problems; Solutions\n\n\n\nWeek 5 Bootstrap Sampling and Nonparametric Tests\nSubsetting data and if/else statements are covered before exploring a simulation study to evaluate the challenges of multiple comparisons.\nThe practice problems give more exposure to bootstrap and permutation testing.\nLab; Practice Problems; Solutions\n\n\n\nWeek 6 Simple Linear Regression Intro and Derivations\nA visual example of bootstrap and permutation resampling, as well as for the distribution of the ratio of sampling variances.\nThe practice problem focuses on the derivation of the sums of squares for our linear regression model.\nLab; Practice Problems; Solutions\n\n\n\nWeek 7 SLR Diagnostics, Confidence/Prediction Intervals\nLab walks through the Midterm Simulation Project template files.\nPractice problems relate to simple linear regression modeling and interpretation.\nPractice Problems; Solutions\n\n\n\nWeek 8 SLR Examples, Log(Y) Transformation\nPROC REG provides a beautiful ANOVA table in its output, but R’s lm and glm do not. We discuss the ANOVA table in lab and how to create a function to generate your own.\nThe practice problems cover diagnostic plot interpretation and simple linear regression models with different data transformations.\nLab; Practice Problems; Solutions\n\n\n\nWeek 9 Multiple Linear Regression and Matrix Approaches\nDiagnostic plots can be one of the most efficient ways to evaluate if your model assumptions for regression are met. In this lab we dive into some examples of good (and bad) diagnostic plots.\nThe practice problems focus on a multiple linear regression problem with two binary and one continuous predictors using both existing functions and by coding your own matrices.\nLab; Practice Problems; Solutions\n\n\n\nWeek 10 Categorical Predictors and ANOVA\nLab introduces the Final Data Analysis Project.\nThe practice problems are on the ANOVA lectures and including categorical variables in your regression models.\nPractice Problems; Solutions\n\n\n\nWeek 11 Confounding, Mediation, Interactions, and Polynomials\nThe partial F-test can be used for both an overall F-test of the model or to evaluate a single predictor (instead of the t-test), in lab we walk through an example contrasting these approaches.\nThe practice problems touch on special topics relating to this week (confounding, mediation, interactions, polynomials, and GLHT).\nLab; Practice Problems; Solutions\n\n\n\nWeek 12 Model and Variable Selection, Outliers/Influential Points\nInteractions can be tricky to interpret, so we review an example using the FEV data set during lab.\nThe practice problems examine influential points/outliers, model and variable selection, and a simulation study for model selection approaches.\nLab; Practice Problems; Solutions\n\n\n\nWeek 13 Segmented Regression, Quantile Regression, Splines, and Advanced Bootstraps\nThere are so many cool things we can do with extensions to the topics we’ve covered this semester. This week we explore quantile regression, segmented regression, splines, and advanced bootstrap topics, with a focus on some of these topics during lab.\nThe practice problems focus on quantile regression, splines, and segmented regression models.\nLab TBD; Practice Problems; Solutions\n\n\n\nWeek 14 Bayesian Linear Regression\nThis week we introduce the Bayesian framework and see a linear regression example.\nPractice problems focus on re-analyzing data from our MLR examples with a Bayesian approach.\nLab TBD; Practice Problems; Solutions"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIOS 6618",
    "section": "",
    "text": "Description\nStatistical thinking can help you become a critical consumer of information while challenging you to examine the world in new ways. Advanced Biostatistical Methods I (BIOS 6618) is the first of a two-semester applied statistics sequence designed to equip students with a practical knowledge of the quantitative methods most frequently used in medical research. This course is an introduction to applied biostatistics, including probability models, simulation, sampling distributions, hypothesis testing, resampling methods, linear regression and the use of the R and SAS statistical packages. Concepts will be illustrated using examples in the fields of medicine, biology, epidemiology and public health. Written and graphical presentation and interpretation of methods and results will be emphasized.\n\n\nPrerequisites\nDifferential and integral calculus is a prerequisite for the course, along with an introductory course in applied probability and statistics. Previous experience with a statistical package (e.g. R, SAS, Stata, SPSS) is also assumed. Matrix/linear algebra will also be introduced and past coursework or experience is beneficial.\n\n\nLecture Videos, Slides, and Other Materials\nAll BIOS 6618 lecture videos have been broken down into modular sections and posted to YouTube. To view the list of videos, download slides, and access other materials, please visit the BIOS 6618 Materials page.\nI’ve also added all my responses to recitation questions that students submit each week. The landing page includes all questions broken out by weekly topics, but you may also wish to use the built-in Quarto search functionality located near the top-right of the webpage.\nFinally, we also have all of our weekly labs and practice problems (with solutions) provided on the labs page. This includes some deeper dives into lecture material or coding tips and tricks for the labs, as well as practice problems designed to assist on completing course homework assignments and having more practice with the different methods covered in class.\n\n\nJournal of BIOS 6618\nThe Journal of BIOS 6618 is the official publication for Advanced Biostatistical Methods I. It has two annual issues relating to simulation studies and applied data analysis. To learn more about the journal’s requirements for submission, please visit the the Guide for Authors page."
  },
  {
    "objectID": "bios_6618_materials/index.html",
    "href": "bios_6618_materials/index.html",
    "title": "BIOS 6618 Materials",
    "section": "",
    "text": "This page includes the asynchronous lecture videos, PDF slides, and other materials for Advanced Biostatistical Methods I (BIOS 6618). It builds and expands upon material from its predecessor course Biostatistical Methods I (BIOS 6611). The material is broadly broken down by theme with links provided to download materials or view the videos. A brief overview of the material covered in a given lecture can be viewed by hovering over or clicking the given “Lecture Topic”."
  },
  {
    "objectID": "bios_6618_materials/index.html#prerequisite-materials",
    "href": "bios_6618_materials/index.html#prerequisite-materials",
    "title": "BIOS 6618 Materials",
    "section": "Prerequisite Materials",
    "text": "Prerequisite Materials\nWe assume some material has been covered in previous statistics coursework. No worries if it isn’t ringing a bell, feel free to review the material below.\n\n\n\n\n\nLecture Topic\n\n\nSlides\n\n\nVideo Link\n\n\n\n\n\n\nProbability and Bayes Theorem\n\n\nPDF\n\n\nYouTube\n\n\n\n\nDiscrete Distributions\n\n\nPDF\n\n\nYouTube\n\n\n\n\nCommon Discrete Distributions\n\n\nPDF\n\n\nYouTube\n\n\n\n\nContinuous Distributions\n\n\nPDF\n\n\nYouTube\n\n\n\n\nOther Continuous Distributions\n\n\nPDF\n\n\nYouTube\n\n\n\n\nMethods of Estimation (MoM, MLE, LSE)\n\n\nPDF\n\n\nYouTube"
  },
  {
    "objectID": "bios_6618_materials/index.html#probability-and-distributions",
    "href": "bios_6618_materials/index.html#probability-and-distributions",
    "title": "BIOS 6618 Materials",
    "section": "Probability and Distributions",
    "text": "Probability and Distributions\nThe first theme in BIOS 6618 discusses core concepts in statistics (e.g., simulation studies, estimators, estimation), the normal distribution, and the central limit theorem.\n\n\n\n\n\nLecture Topic\n\n\nSlides\n\n\nVideo Link\n\n\n\n\n\n\nIntro to Statistical Topics\n\n\nPDF\n\n\nYouTube\n\n\n\n\nProperties of Estimators\n\n\nPDF\n\n\nYouTube\n\n\n\n\nSimulations Studies\n\n\nPDF\n\n\nYouTube\n\n\n\n\nBrief, but Complete, Interpretations\n\n\nPDF\n\n\nYouTube\n\n\n\n\nThe Normal Distribution\n\n\nPDF\n\n\nYouTube\n\n\n\n\nAssessing Normality in Samples\n\n\nPDF\n\n\nYouTube\n\n\n\n\nThe Central Limit Theorem\n\n\nPDF\n\n\nYouTube\n\n\n\n\nDeriving the Sampling Distribution of the Sample Mean and Sample Variance\n\n\nPDF\n\n\nYouTube"
  },
  {
    "objectID": "bios_6618_materials/index.html#hypothesis-testing-power-sample-size",
    "href": "bios_6618_materials/index.html#hypothesis-testing-power-sample-size",
    "title": "BIOS 6618 Materials",
    "section": "Hypothesis Testing, Power, Sample Size",
    "text": "Hypothesis Testing, Power, Sample Size\nThe second theme of BIOS 6618 introduces the framework for hypothesis testing based on null hypothesis significance testing (NHST). We then discuss the concept of power calculations and work through practical examples by hand and using R to estimate power, sample size, or a detectable difference.\n\n\n\n\n\nLecture Topic\n\n\nSlides\n\n\nVideo Link\n\n\n\n\n\n\nFisher’s p-value\n\n\nPDF\n\n\nYouTube\n\n\n\n\nThe Neyman-Pearson Tradeoffs\n\n\nPDF\n\n\nYouTube\n\n\n\n\nNull Hypothesis Significance Testing (Fisher-Neyman-Pearson Hybrid)\n\n\nPDF\n\n\nYouTube\n\n\n\n\nStatistical Power and Derivations for a Two-Sided, One-Sample Z-test\n\n\nPDF\n\n\nYouTube\n\n\n\n\nPower Calculation Examples in R\n\n\nPDF\n\n\nYouTube\n\n\n\n\nMultiple Testing/Comparisons\n\n\nPDF\n\n\nYouTube"
  },
  {
    "objectID": "bios_6618_materials/index.html#conditional-probability-effect-measures-inference-for-2x2-tables",
    "href": "bios_6618_materials/index.html#conditional-probability-effect-measures-inference-for-2x2-tables",
    "title": "BIOS 6618 Materials",
    "section": "Conditional Probability, Effect Measures, Inference for 2x2 Tables",
    "text": "Conditional Probability, Effect Measures, Inference for 2x2 Tables\nOur third theme in BIOS 6618 explores conditional probability and its application for diagnostic testing as well as studies with 2x2 tables and inference based on risk differences, risk ratios, or odds ratios.\n\n\n\n\n\nLecture Topic\n\n\nSlides\n\n\nVideo Link\n\n\n\n\n\n\nDiagnostic Testing: Sensitivity and Specificity\n\n\nPDF\n\n\nYouTube\n\n\n\n\nDiagnostic Testing: Predictive Values and Odds\n\n\nPDF\n\n\nYouTube\n\n\n\n\nDiagnostic Testing: ROC Curves\n\n\nPDF\n\n\nYouTube\n\n\n\n\nObservational Study Designs\n\n\nPDF\n\n\nYouTube\n\n\n\n\n2x2 Tables Measures of Effect: RD, RR, OR\n\n\nPDF\n\n\nYouTube\n\n\n\n\n2x2 Tables and Tests of Association\n\n\nPDF\n\n\nYouTube"
  },
  {
    "objectID": "bios_6618_materials/index.html#bootstrap-sampling-and-nonparametric-methods",
    "href": "bios_6618_materials/index.html#bootstrap-sampling-and-nonparametric-methods",
    "title": "BIOS 6618 Materials",
    "section": "Bootstrap Sampling and Nonparametric Methods",
    "text": "Bootstrap Sampling and Nonparametric Methods\nThe fourth theme of BIOS 6618 explores bootstrap resampling, permutation testing, and other nonparametric methods for hypothesis testing.\n\n\n\n\n\nLecture Topic\n\n\nSlides\n\n\nVideo Link\n\n\n\n\n\n\nBootstrap Resample Introduction\n\n\nPDF\n\n\nYouTube\n\n\n\n\nConfidence Intervals and One-Sample Bootstraps\n\n\nPDF\n\n\nYouTube\n\n\n\n\nTwo-Sample Bootstraps\n\n\nPDF\n\n\nYouTube\n\n\n\n\nPermutation Tests\n\n\nPDF\n\n\nYouTube\n\n\n\n\nNonparametric Tests\n\n\nPDF\n\n\nYouTube"
  },
  {
    "objectID": "bios_6618_materials/index.html#simple-linear-regression",
    "href": "bios_6618_materials/index.html#simple-linear-regression",
    "title": "BIOS 6618 Materials",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nIn our fifth theme of BIOS 6618 we introduce, derive, and apply methods for linear regression with a single predictor for a continuous outcome. The lectures include a mix of applied and theoretical content.\n\n\n\n\n\nLecture Topic\n\n\nSlides\n\n\nVideo Link\n\n\n\n\n\n\nSimple Linear Regression (SLR) Introduction\n\n\nPDF\n\n\nYouTube\n\n\n\n\nSLR: Find the “Best” Fit and Summary Formulas\n\n\nPDF\n\n\nYouTube\n\n\n\n\nSLR: Simple Application and Hypothesis Testing\n\n\nPDF\n\n\nYouTube\n\n\n\n\nSLR: Quality of Fit, F-test, and ANOVA Table\n\n\nPDF\n\n\nYouTube\n\n\n\n\nSLR: Prediction vs. Confidence Intervals\n\n\nPDF\n\n\nYouTube\n\n\n\n\nDerivation of SLR Regression Coefficients\n\n\nPDF\n\n\nYouTube\n\n\n\n\nVariance of Regression Coefficients\n\n\nPDF\n\n\nYouTube\n\n\n\n\nCoefficient of Determination and Correlation Connection\n\n\nPDF\n\n\nYouTube\n\n\n\n\nResiduals\n\n\nPDF\n\n\nYouTube\n\n\n\n\nDiagnostic Plots\n\n\nPDF\n\n\nYouTube\n\n\n\n\nSLR Example: Continuous Predictor\n\n\nPDF\n\n\nYouTube\n\n\n\n\nSLR Example: Categorical Predictor\n\n\nPDF\n\n\nYouTube\n\n\n\n\nTransformations to Remove Heteroscedasticity\n\n\nPDF\n\n\nYouTube"
  },
  {
    "objectID": "bios_6618_materials/index.html#multiple-linear-regression",
    "href": "bios_6618_materials/index.html#multiple-linear-regression",
    "title": "BIOS 6618 Materials",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nThe penultimate theme of BIOS 6618 expands our linear regression framework to account for more than one predictor. This results in a variety of different applications and topics that are broken down further into subthemes below.\n\nMLR Introduction\n\n\n\n\n\nLecture Topic\n\n\nSlides\n\n\nVideo Link\n\n\n\n\n\n\nMultiple Linear Regression (MLR): Motivation, Assumptions, Example\n\n\nPDF\n\n\nYouTube\n\n\n\n\nMLR: Diagnostic Plots and Multicollinearity\n\n\nPDF\n\n\nYouTube\n\n\n\n\nMLR: Inference on Independent Variables\n\n\nPDF\n\n\nYouTube\n\n\n\n\nLinear Regression in Matrix Format\n\n\nPDF\n\n\nYouTube\n\n\n\n\nLinear Regression with Matrices in SAS\n\n\nSAS\n\n\nYouTube\n\n\n\n\nLinear Regression with Matrices in R\n\n\nHTML\n\n\nYouTube\n\n\n\n\n\n\n\nAnalysis of Variance (ANOVA) and Post-hoc Testing\n\n\n\n\n\nLecture Topic\n\n\nSlides\n\n\nVideo Link\n\n\n\n\n\n\nCoding Categorical Variables\n\n\nPDF\n\n\nYouTube\n\n\n\n\nOne-Way ANOVA and Connections to Regression\n\n\nPDF\n\n\nYouTube\n\n\n\n\nPost-hoc Testing for ANOVA\n\n\nPDF\n\n\nYouTube\n\n\n\n\nKruskal-Wallis: A Nonparametric ANOVA\n\n\nPDF\n\n\nYouTube\n\n\n\n\n\n\n\nSpecial Topics\n\n\n\n\n\nLecture Topic\n\n\nSlides\n\n\nVideo Link\n\n\n\n\n\n\nGeneral Linear Hypothesis Testing\n\n\nPDF\n\n\nYouTube\n\n\n\n\nConfounders and Precision Variables\n\n\nPDF\n\n\nYouTube\n\n\n\n\nMediation Models\n\n\nPDF\n\n\nYouTube\n\n\n\n\nPolynomial Regression\n\n\nPDF\n\n\nYouTube\n\n\n\n\nInteraction/Effect Modification\n\n\nPDF\n\n\nYouTube\n\n\n\n\nModel Selection Procedures\n\n\nPDF\n\n\nYouTube\n\n\n\n\nVariable Selection Considerations\n\n\nPDF\n\n\nYouTube\n\n\n\n\nDiagnostics for Outliers and Influential Points\n\n\nPDF\n\n\nYouTube\n\n\n\n\nSupplemental Material on Contrasts\n\n\nPDF\n\n\n–"
  },
  {
    "objectID": "bios_6618_materials/index.html#advanced-topics",
    "href": "bios_6618_materials/index.html#advanced-topics",
    "title": "BIOS 6618 Materials",
    "section": "Advanced Topics",
    "text": "Advanced Topics\nThe final theme of BIOS 6618 explores advanced topics that build what we have covered this semester. It includes segmented/piecewise regression, quantile regression, Bayesian regression, splines, and advanced bootstrap approaches. We close with material that builds a bridge to BIOS 6619 with exponential families and generalized linear models.\n\n\n\n\n\nLecture Topic\n\n\nSlides\n\n\nVideo Link\n\n\n\n\n\n\nSegmented/Piecewise Regression\n\n\nPDF\n\n\nYouTube\n\n\n\n\nQuantile Regression\n\n\nPDF\n\n\nYouTube\n\n\n\n\nSplines\n\n\nPDF\n\n\nYouTube\n\n\n\n\nBootstrap p-values\n\n\nPDF\n\n\nYouTube\n\n\n\n\nAlternative Bootstraps\n\n\nPDF\n\n\nYouTube\n\n\n\n\nIntro to Bayes\n\n\nPDF\n\n\nYouTube\n\n\n\n\nBayesian Multiple Linear Regression\n\n\nPDF\n\n\nYouTube\n\n\n\n\nMLE and Regression\n\n\nPDF\n\n\nYouTube\n\n\n\n\nExponential Families and Generalized Linear Models\n\n\nPDF\n\n\nYouTube"
  },
  {
    "objectID": "journal_of_bios_6618/index.html",
    "href": "journal_of_bios_6618/index.html",
    "title": "Journal of BIOS 6618 Guide for Authors",
    "section": "",
    "text": "The Journal of BIOS 6618 is the official internal publication of the Advanced Biostatistical Methods I course at the University of Colorado Anschutz Medical Campus. Each year (volume) of the journal encompasses two distinct issues: one focusing on simulation studies and another on applied data analysis. Specific details as to project deadlines, rubrics, and more can be found for current students on the Canvas course page. The details below are meant to mimic those of real journal websites for submission to provide practice for future submissions.\n\n\nTwo types of articles are accepted:\n\nSimulation studies: these articles focus on the use of simulation studies to explore research questions that do not involve “real” data. Articles may include exploration of theorems discussed in class, evaluation of test properties under various assumptions, illustration of power calculations for unique problems, or other applications where simulation studies may be of interest.\nBrief reports: these articles are short analyses to answer a research question from real world data. Articles must include at least one advanced topic including:\n\n\nBootstrap resampling\nPermutation test\nQuantile regression\nSegmented regression\nBayesian regression\nNon-linear models (e.g., using polynomial terms, splines, etc.)\nOther advanced topic (discuss with Alex for approval)"
  },
  {
    "objectID": "journal_of_bios_6618/index.html#types-of-articles",
    "href": "journal_of_bios_6618/index.html#types-of-articles",
    "title": "Journal of BIOS 6618 Guide for Authors",
    "section": "",
    "text": "Two types of articles are accepted:\n\nSimulation studies: these articles focus on the use of simulation studies to explore research questions that do not involve “real” data. Articles may include exploration of theorems discussed in class, evaluation of test properties under various assumptions, illustration of power calculations for unique problems, or other applications where simulation studies may be of interest.\nBrief reports: these articles are short analyses to answer a research question from real world data. Articles must include at least one advanced topic including:\n\n\nBootstrap resampling\nPermutation test\nQuantile regression\nSegmented regression\nBayesian regression\nNon-linear models (e.g., using polynomial terms, splines, etc.)\nOther advanced topic (discuss with Alex for approval)"
  },
  {
    "objectID": "journal_of_bios_6618/index.html#file-formats",
    "href": "journal_of_bios_6618/index.html#file-formats",
    "title": "Journal of BIOS 6618 Guide for Authors",
    "section": "File Formats",
    "text": "File Formats\nArticles should be submitted in a format compatible with Canvas SpeedGrader (e.g., DOCX or PDF).\nPreferably, articles will be written using R Markdown. All files will ultimately be converted to PDF for inclusion in the journal issue disseminated internally within the class."
  },
  {
    "objectID": "journal_of_bios_6618/index.html#page-limits",
    "href": "journal_of_bios_6618/index.html#page-limits",
    "title": "Journal of BIOS 6618 Guide for Authors",
    "section": "Page Limits",
    "text": "Page Limits\nSimulation studies have an expected page length of at least 1.5 pages and no more than 2 pages.\nBrief reports have an expected page length of at least 2.5 pages and no more than 3 pages.\nThese pages limits include the required figure(s) and table(s), but not references."
  },
  {
    "objectID": "journal_of_bios_6618/index.html#font-size-and-spacing",
    "href": "journal_of_bios_6618/index.html#font-size-and-spacing",
    "title": "Journal of BIOS 6618 Guide for Authors",
    "section": "Font Size and Spacing",
    "text": "Font Size and Spacing\nUse a standard font size and any standard font. Articles should be single spaced."
  },
  {
    "objectID": "journal_of_bios_6618/index.html#title-and-author-information",
    "href": "journal_of_bios_6618/index.html#title-and-author-information",
    "title": "Journal of BIOS 6618 Guide for Authors",
    "section": "Title and Author Information",
    "text": "Title and Author Information\nThe article should have an informative title. Additionally, author information should include:\n\nAuthor name\nAuthor degrees\nAuthor affiliation (e.g., BIOS MS Student, EPID MPH Student, Baisy’s Lab, etc.)"
  },
  {
    "objectID": "journal_of_bios_6618/index.html#abstract-and-keywords",
    "href": "journal_of_bios_6618/index.html#abstract-and-keywords",
    "title": "Journal of BIOS 6618 Guide for Authors",
    "section": "Abstract and Keywords",
    "text": "Abstract and Keywords\nA short abstract should describe the problem, data, and results.\nImmediately after the abstract, provide a maximum of 5 keywords, avoiding general and plural terms and multiple concepts (avoid, for example, ‘and’, ‘of’). These keywords will be used for indexing purposes."
  },
  {
    "objectID": "journal_of_bios_6618/index.html#article-body-structure",
    "href": "journal_of_bios_6618/index.html#article-body-structure",
    "title": "Journal of BIOS 6618 Guide for Authors",
    "section": "Article Body Structure",
    "text": "Article Body Structure\nArticles should follow a similar structure to that outlined below:\n\nIntroduction, Background\nMotivating Problem, Theorem Description (for simulation studies); Materials and Methods, Study Design (for brief reports)\nSimulation Set-up, Statistical Methods, Statistical Analyses\nResults\nDiscussion, Conclusion\n\nNote, these exact names do not need to be used and can be customized to your given article, analysis, and context."
  },
  {
    "objectID": "journal_of_bios_6618/index.html#tables-and-figures",
    "href": "journal_of_bios_6618/index.html#tables-and-figures",
    "title": "Journal of BIOS 6618 Guide for Authors",
    "section": "Tables and Figures",
    "text": "Tables and Figures\nAll tables and figures should be “publication ready” with informative titles and labels. Where applicable, units of measurement should be included. Variable names from code should be replaced with meaningful labels (e.g., instead of “outcome_variable4” write the actual outcome).\nSimulation studies must include at least one table and one figure, but more may be included. These count towards the two page limit.\nBrief reports must include a “Table 1” that describes the study population as well as at least one figure. Additional tables and figures may be included as necessary to present and summarize results. These count towards the three page limit."
  },
  {
    "objectID": "journal_of_bios_6618/index.html#decimal-numerals",
    "href": "journal_of_bios_6618/index.html#decimal-numerals",
    "title": "Journal of BIOS 6618 Guide for Authors",
    "section": "Decimal Numerals",
    "text": "Decimal Numerals\nTo enhance readability and clarity of the text as well as tables and figures, decimal numerals should - with the obvious exception of P-values - be rounded to the unit whenever possible (i.e. in all cases in which the rounding procedure does not change the meaning)."
  },
  {
    "objectID": "journal_of_bios_6618/index.html#acknowledgments",
    "href": "journal_of_bios_6618/index.html#acknowledgments",
    "title": "Journal of BIOS 6618 Guide for Authors",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nCollate acknowledgements in a separate section at the end of the article before the references and do not, therefore, include them on the title page, as a footnote to the title or otherwise. List here those individuals who provided help during the research (e.g., providing language help, writing assistance or proof reading the article, etc.)."
  },
  {
    "objectID": "journal_of_bios_6618/index.html#references",
    "href": "journal_of_bios_6618/index.html#references",
    "title": "Journal of BIOS 6618 Guide for Authors",
    "section": "References",
    "text": "References\nAny style of reference is acceptable as long as formatting is consistent."
  },
  {
    "objectID": "journal_of_bios_6618/index.html#supplemental-materials",
    "href": "journal_of_bios_6618/index.html#supplemental-materials",
    "title": "Journal of BIOS 6618 Guide for Authors",
    "section": "Supplemental Materials",
    "text": "Supplemental Materials\nGiven the small page limits, it is acceptable to include additional tables and figures beyond the required minimum of one table and one figure in a Supplemental Materials section if needed. These figures and tables should be referenced in the text as Figure S1, Table S1, etc.\nWhile you should direct the reader to the Supplemental Materials to access the additional tables, figures, or information, the paper itself should be able to be largely understood as though a reader did not have access to the supplement."
  },
  {
    "objectID": "journal_of_bios_6618/index.html#reproducible-research-principles",
    "href": "journal_of_bios_6618/index.html#reproducible-research-principles",
    "title": "Journal of BIOS 6618 Guide for Authors",
    "section": "Reproducible Research Principles",
    "text": "Reproducible Research Principles\nAn appendix with R, SAS, or other code should be included starting on a new page after your references with code that is clearly commented for the simulations, analyses, figures, and tables ultimately included in the report. Code that was ultimately not used in the final report should be excluded from this appendix."
  },
  {
    "objectID": "labs/lab11/index.html",
    "href": "labs/lab11/index.html",
    "title": "Week 11 Lab",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Labs collection, the mini-lecture content delivered at the start of class before breaking out into small groups to work on the homework assignment."
  },
  {
    "objectID": "labs/lab11/index.html#the-partial-f-test-for-the-overall-model",
    "href": "labs/lab11/index.html#the-partial-f-test-for-the-overall-model",
    "title": "Week 11 Lab",
    "section": "The Partial F-test for the Overall Model",
    "text": "The Partial F-test for the Overall Model\nWe can also use the partial \\(F\\)-test to address this question. It will be equivalent to our overall \\(F\\)-test because \\(SS_{model} = \\sum_{i=1}^{n} (\\hat{Y} - \\overline{Y})^2\\), but with no predictors \\(\\hat{Y} = \\overline{Y} \\implies \\sum_{i=1}^{n} (\\hat{Y} - \\overline{Y})^2 = 0\\).\nFor notation, let\n\n\\(n\\) be the number of observations\n\\(p\\) be the number of IVs in the reduced model\n\\(k\\) the number of IVs removed from the full model\n\\(p+k\\) the number of IVs in the full model\n\nThe partial \\(F\\)-test is \\[ F = \\frac{[SS_{model}(full) - SS_{model}(reduced)]/k}{MS_{error}(full)} \\sim F_{k,n-p-k-1} \\]\nTherefore, our partial \\(F\\)-test for the overall model becomes \\[ F = \\frac{[SS_{model}(full) - 0]/k}{MS_{error}(full)} = \\frac{MS_{model}(full)}{MS_{error}(full)}\\sim F_{k,n-k-1} \\]"
  },
  {
    "objectID": "labs/lab11/index.html#the-partial-f-test-for-one-variable",
    "href": "labs/lab11/index.html#the-partial-f-test-for-one-variable",
    "title": "Week 11 Lab",
    "section": "The Partial F-test for One Variable",
    "text": "The Partial F-test for One Variable\nWe could also use the partial \\(F\\)-test to evaluate if age on its own contributes to the explanation of the variability observed in our outcome above and beyond BMI, the study arm, or ASA status. In this case we’d fit our reduced model which excludes age and calculate the difference either by hand or using the anova function:\n\n\nCode\n# Fit the reduced model\nmod_red1 &lt;- glm(ease ~ BMI + intervention + asa3 + asa4, data=dat)\n\n# Partial F-test with anova\nanova(mod_full, mod_red1, test='F')\n\n\nAnalysis of Deviance Table\n\nModel 1: ease ~ BMI + age + intervention + asa3 + asa4\nModel 2: ease ~ BMI + intervention + asa3 + asa4\n  Resid. Df Resid. Dev Df Deviance      F  Pr(&gt;F)  \n1        91      76539                             \n2        92      78961 -1  -2422.6 2.8803 0.09309 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# ANOVA table to calculate partial F by hand\nlinreg_anova_func(mod_red1)\n\n\n\n\n\nSource\nSums of Squares\nDegrees of Freedom\nMean Square\nF-value\np-value\n\n\n\n\nModel\n9787.24\n4\n2446.81\n2.85\n0.028\n\n\nError\n78961.25\n92\n858.27\n\n\n\n\nTotal\n88748.49\n96\n\n\n\n\n\n\n\n\n\n\nBased on our ANOVA tables we would have:\n\\[ F = \\frac{[SS_{model}(full) - SS_{model}(reduced)]/k}{MS_{error}(full)} = \\frac{[12209.82 - 9787.24]/1}{841.08} = 2.88 \\]\nHere we have \\(n=97\\), \\(k=1\\), and \\(p=4\\). So we would reference an \\(F_{k,n-p-k-1} = F_{1,97-4-1-1} = F_{1,91}\\) distribution, where our critical value is qf(0.95,1,91)=3.946 and our p-value is pf(2.88,1,91,lower.tail=F)=0.093.\nThese estimates match our anova function results and the estimate from the table of coefficients. We can also note that our partial \\(F\\) statistic is equal to \\(t^2\\) for the variable being considered."
  },
  {
    "objectID": "labs/lab13/index.html",
    "href": "labs/lab13/index.html",
    "title": "Week 13 Lab",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Labs collection, the mini-lecture content delivered at the start of class before breaking out into small groups to work on the homework assignment.\n\nTBD\nContent for this week is to be decided."
  },
  {
    "objectID": "labs/lab2/index.html",
    "href": "labs/lab2/index.html",
    "title": "Week 2 Lab",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Labs collection, the mini-lecture content delivered at the start of class before breaking out into small groups to work on the homework assignment."
  },
  {
    "objectID": "labs/lab2/index.html#calculating-the-expected-value-ex-of-the-exponential-distribution",
    "href": "labs/lab2/index.html#calculating-the-expected-value-ex-of-the-exponential-distribution",
    "title": "Week 2 Lab",
    "section": "Calculating the Expected Value (\\(E(X)\\)) of the Exponential Distribution",
    "text": "Calculating the Expected Value (\\(E(X)\\)) of the Exponential Distribution\nAssume that \\(X \\sim \\text{Exp}(\\lambda)\\), then the probability density function is given by\n\\[\n    f(x)=\n\\begin{cases}\n    \\lambda e^{-\\lambda x},& \\text{if } x\\geq 0 \\\\\n    0,              & \\text{otherwise}\n\\end{cases}\n\\]\nTo calculate the expected value of a continuous variable we know that \\(E(X) = \\int_{-\\infty}^{\\infty} x f(x) \\, dx\\) (i.e., we multiply our probability density function, \\(f(x)\\), by \\(x\\) and integrate it across the range of support for \\(x\\)). For our exponential distribution, this definition gives\n\\[ E(X) = \\int_{-\\infty}^{\\infty} x f(x) \\, dx = \\int_{0}^{\\infty} x \\times \\lambda e^{-\\lambda x} \\, dx \\]\nSince we wish to integrate over \\(x\\), and we see that we have a product of functions where \\(x\\) is in multiple places, so we need to use integration by parts. We will let \\(u = x\\lambda\\) and \\(dv = e^{-\\lambda x} \\, dx\\), but still need to calculate \\(du\\) and \\(v\\):\n\\[\n\\begin{array}{cc}\nu = x\\lambda & dv = e^{-\\lambda x} \\, dx \\\\\ndu = \\lambda \\, dx & v = \\frac{-e^{-\\lambda x}}{\\lambda}\n\\end{array}\n\\]\nWe can then plug these values into our integration by parts formula:\n\\[ uv |_{a}^{b} - \\int_{a}^{b} v \\, du = \\left. (x\\lambda) \\times \\frac{-e^{-\\lambda x}}{\\lambda} \\right|_{0}^{\\infty} - \\int_0^{\\infty} \\frac{-e^{-\\lambda x}}{\\lambda} \\times \\lambda \\, dx \\]\nThis can be simplified by cancelling some of the \\(\\lambda\\) values we see in each part:\n\\[ \\left. (x) \\times -e^{-\\lambda x} \\right|_{0}^{\\infty} - \\int_0^{\\infty} -e^{-\\lambda x} \\, dx  \\]\nIt may be helpful to write \\(-e^{-\\lambda x}\\) as \\(\\frac{-1}{e^{\\lambda x}}\\) for our \\(uv |_{a}^{b}\\) piece. For the \\(\\int_{a}^{b} v \\, du\\) piece, we can note that this is our \\(dv\\) identified above:\n\\[ \\left. \\frac{-x}{e^{\\lambda x}} \\right|_{0}^{\\infty} - \\left[ \\left. \\frac{-e^{-\\lambda x}}{\\lambda} \\right|_{0}^{\\infty}  \\right]  \\]\nNow we just need to plug everything in to finish up our calculation of \\(E(X)\\):\n\\[ \\left( \\frac{-\\infty}{e^{\\lambda \\times \\infty}} - \\frac{-0}{e^{\\lambda \\times 0}} \\right) - \\left[ \\frac{-e^{-\\lambda \\times \\infty}}{\\lambda} - \\frac{-e^{-\\lambda \\times 0}}{\\lambda} \\right] \\]\nSome of these values may look pretty funky, and we have to remember some properties about the rates of convergence (e.g., \\(e^{\\infty}\\) approaches infinity at a faster rate than just \\(\\infty\\)):\n\n\\(\\frac{-\\infty}{e^{\\lambda \\times \\infty}} = 0\\) because it is in an indeterminate form of \\(\\frac{\\infty}{\\infty}\\), so by applying L’Hôpital’s rule (the limit of the fraction is the same as the limit of the fraction of the derivatives of the numerator and denominator when in an indeterminate form, and when evaluating an integral that has a limit at infinity we are essentially evaluating the limit as \\(x \\to \\infty\\)) we have \\(\\lim_{x \\to \\infty} \\frac{-x}{e^{\\lambda \\times x}} = \\lim_{x \\to \\infty} \\frac{-1}{\\lambda e^{\\lambda \\times x}} = 0\\)\n\\(\\frac{-0}{e^{\\lambda \\times 0}} = \\frac{0}{1} = 0\\)\n\\(\\frac{-e^{-\\lambda \\times \\infty}}{\\lambda} = \\frac{-1}{\\lambda e^{\\lambda \\times \\infty}} = 0\\)\n\\(\\frac{-e^{-\\lambda \\times 0}}{\\lambda} = \\frac{1}{\\lambda}\\)\n\nUltimately, this results in:\n\\[ E(X) = (0 - 0) - \\left[0 - \\frac{1}{\\lambda}\\right] = 0 - \\left[ -\\frac{1}{\\lambda} \\right] = \\frac{1}{\\lambda} \\]\nWe can check this by looking up the exponential distribution on Wikipedia or using Wolfram Alpha to check what we should end up with.\nIf we wanted to calculate the variance of the exponential distribution, such as in Homework 2, it might be easier to calculate \\(E(X^2)\\) using a similar approach to the integration by parts above, then calculating \\(Var(X) = E(X^2) - E(X)^2\\). We can then plug in the given value in the homework to get our \\(E(X)\\) and \\(Var(X)\\) for a given \\(\\lambda\\)."
  },
  {
    "objectID": "labs/lab2/index.html#initialize-a-vector-example-1-use-actual-numbers",
    "href": "labs/lab2/index.html#initialize-a-vector-example-1-use-actual-numbers",
    "title": "Week 2 Lab",
    "section": "Initialize a Vector Example 1: Use Actual Numbers",
    "text": "Initialize a Vector Example 1: Use Actual Numbers\nIn our lecture set on Simulations, we initialized a vector that had a length of 10 and used NA for our results. Instead, we could create a vector with numbers (or any other value):\n\n\nCode\n# number of simulations\nreps &lt;- 10\n\n# initialize a vector of thetas (-999 = missing)\nthetas &lt;- rep(-999, reps)\nthetas\n\n\n [1] -999 -999 -999 -999 -999 -999 -999 -999 -999 -999\n\n\nFrom the output, we see that we have 10 -999 values, as we would expect.\nIn some data sets, numbers are used to code missing data. This is sometimes for informative purposes (e.g., -999 is missing for an unknown reason, -998 is a skipped visit, etc.). However, if we do not know this coding scheme, we may be confused at a value that is drastically different from the expected data or even wonder why so many are equal to the same thing. In practice, I like to avoid this strategy since it won’t kick back any warnings/errors if we try to do something like calculate the mean."
  },
  {
    "objectID": "labs/lab2/index.html#initialize-a-vector-example-2-use-na",
    "href": "labs/lab2/index.html#initialize-a-vector-example-2-use-na",
    "title": "Week 2 Lab",
    "section": "Initialize a Vector Example 2: Use NA",
    "text": "Initialize a Vector Example 2: Use NA\nIn some cases you may be unsure what type of data you are expecting to encounter, so you may be worried about using a numeric value. One option I frequently use to avoid this issue is NA, which represents general missingness:\n\n\nCode\n# number of simulations\nreps &lt;- 10\n\n# initialize a vector of thetas (NA = missing)\nthetas2 &lt;- rep(NA, reps)\nthetas2\n\n\n [1] NA NA NA NA NA NA NA NA NA NA\n\n\nFrom the output, we see that we have 10 NA values, as we would expect.\nOne potentially frustrating aspect of NA is that many functions will return back an NA result if any item of the vector is missing, even if you may want to calculate the mean of the non-missing values. Fortunately, we can often times solve this with certain arguments for a function (e.g., na.rm=TRUE indicates that we want to remove any NA values for the function)."
  },
  {
    "objectID": "labs/lab2/index.html#initialize-a-vector-example-3-make-it-null",
    "href": "labs/lab2/index.html#initialize-a-vector-example-3-make-it-null",
    "title": "Week 2 Lab",
    "section": "Initialize a Vector Example 3: Make it NULL",
    "text": "Initialize a Vector Example 3: Make it NULL\nA third way that may be useful in some contexts is to create a NULL object that can become a vector by concatenating results (which we will see an example of below for our loops!):\n\n\nCode\n# initialize an object to store thetas in\nthetas3 &lt;- NULL\nthetas3\n\n\nNULL\n\n\nSince we just wanted to create an object to store our results in, we see that it only returns a NULL value. This is useful because without initializing an empty object, our loops would run into an error where the object we were trying to store values in did not exist."
  },
  {
    "objectID": "labs/lab2/index.html#for-loop-example-1-initialize-a-vector-then-save-results-in-vector-by-index",
    "href": "labs/lab2/index.html#for-loop-example-1-initialize-a-vector-then-save-results-in-vector-by-index",
    "title": "Week 2 Lab",
    "section": "for Loop Example 1: Initialize a Vector, then Save Results in Vector by Index",
    "text": "for Loop Example 1: Initialize a Vector, then Save Results in Vector by Index\nFor this example we will first initialize a vector of size 10 (here using NA) and then save the results from 10 random samples with \\(\\mu=5\\) and \\(\\sigma=3\\):\n\n\nCode\n# set our seed for reproducibility\nset.seed(515)\n\n# number of simulations\nreps &lt;- 10\n\n# initialize our vector\nmean1 &lt;- rep(NA, reps)\n\n\nWe know at this stage that mean1 is a vector of length 10 with only NA values. So now we can loop through and generate 10 random samples:\n\n\nCode\n# loop through where \n## \"i\" is our \"value\"\n## \"that\" is a vector from 1 up to the length of mean1 (i.e., 10)\n## \"this\" is our code within the loop\n\nfor( i in 1:length(mean1)){\n  mean1[i] &lt;- mean( rnorm(n=30, mean=5, sd=3) ) #calculate the mean for the random sample of sample size 30\n}\n\nmean1\n\n\n [1] 5.862780 5.038238 5.652081 4.730882 5.537787 5.381006 6.167604 5.287453\n [9] 4.247241 5.586936\n\n\nWe see that mean1 is no longer a vector of 10 NA values, but instead has the estimated values from 10 simulated random experiments.\nIn this example we specifically indexed the mean1 vector by using the [i] to tell R which of the 10 positions to save the result in. If we had forgotten it, the initialized mean1 vector would have just been replaced with the sample mean from the last simulated data set!\nWe can also see that R has created an object for i that should indicate whatever the last value from the loop was:\n\n\nCode\ni\n\n\n[1] 10\n\n\nWe see that i=10, so we know R made it through the entire loop. As you get into increasingly complex simulations you may encounter functions or models where errors occur (e.g., due to convergence), and this can serve as a great tool to work backwards to try and pinpoint what is causing the code to stop and either modify the code or work to escape these issues.\nAnother piece of coding that may help in the future is that I wrote 1:length(mean1) instead of 1:10. If I ever changed the number of simulations I wouldn’t also have to update the that in our for loop. (We could have also used 1:reps to achieve the same result.)\n\nDetour for Example 1: Let’s Change the Order of Index\nTo keep things somewhat simple, let’s see what happens if we change the vector from 1:length(mean1) to c(2,1,3:10) (i.e., a vector where we’ve switched the order of the first 2 values):\n\n\nCode\n# set our seed for reproducibility\nset.seed(515)\n\n# initialize our vector\nmean1_detour &lt;- rep(NA, 10)\n\n\nfor( j in c(2,1,3:10)){\n  mean1_detour[j] &lt;- mean( rnorm(n=30, mean=5, sd=3) )\n}\n\nmean1_detour\n\n\n [1] 5.038238 5.862780 5.652081 4.730882 5.537787 5.381006 6.167604 5.287453\n [9] 4.247241 5.586936\n\n\nWe can see that, since we set the same seed, we have the same values for the 3rd to 10th spots in the vector, with the 1st and 2nd spots just changing places. Also note that I changed the value from i to j and it still worked since the “value” was also updated in the loop to be [j]."
  },
  {
    "objectID": "labs/lab2/index.html#for-loop-example-2-initialize-a-null-object-then-save-results-by-concatenation",
    "href": "labs/lab2/index.html#for-loop-example-2-initialize-a-null-object-then-save-results-by-concatenation",
    "title": "Week 2 Lab",
    "section": "for Loop Example 2: Initialize a NULL Object, then Save Results by Concatenation",
    "text": "for Loop Example 2: Initialize a NULL Object, then Save Results by Concatenation\nIn the previous example, we may examine our code and realize that the index of [i] is not really serving much of a purpose beyond indexing a specific location. Instead we may wish to instead initialize a NULL object and then concatenate our results:\n\n\nCode\n# set our seed for reproducibility\nset.seed(515)\n\n# number of simulations\nreps &lt;- 10\n\n# initialize our vector\nmean2 &lt;- NULL\n\n# specify the number of times to loop through and add (concatenate) the results to mean2\nfor( k in 1:reps){\n  mean2 &lt;- c( mean2, mean( rnorm(n=30, mean=5, sd=3) ) )\n}\n\nmean2\n\n\n [1] 5.862780 5.038238 5.652081 4.730882 5.537787 5.381006 6.167604 5.287453\n [9] 4.247241 5.586936\n\n\nIn this example we see that k doesn’t appear within the for loop at all! Instead we leverage the properties of NULL objects and just concatenate the results that match those in Example 1 above.\nWe can also note that we made the code a little shorter by concatenating and calculating the mean in one line of code. It is totally fine to separate the steps as well:\n\n\nCode\n# set our seed for reproducibility\nset.seed(515)\n\n# number of simulations\nreps &lt;- 10\n\n# initialize our vector\nmean2 &lt;- NULL\n\n# specify the number of times to loop through and add (concatenate) the results to mean2\nfor( k in 1:reps){\n  sim_dat &lt;- rnorm(n=30, mean=5, sd=3) # first we simulate our data\n  mean_sim &lt;- mean(sim_dat) # then we calculate the mean\n  mean2 &lt;- c( mean2, mean_sim) # finally, we concatenate the mean to our mean2 vector\n}\n\nmean2\n\n\n [1] 5.862780 5.038238 5.652081 4.730882 5.537787 5.381006 6.167604 5.287453\n [9] 4.247241 5.586936\n\n\nNotice the results between the two loops match, so the approaches result in the same answer."
  },
  {
    "objectID": "labs/lab2/index.html#for-loop-example-3-create-a-vector-with-names-then-save-results-by-name",
    "href": "labs/lab2/index.html#for-loop-example-3-create-a-vector-with-names-then-save-results-by-name",
    "title": "Week 2 Lab",
    "section": "for Loop Example 3: Create a Vector with Names, then Save Results by Name",
    "text": "for Loop Example 3: Create a Vector with Names, then Save Results by Name\nFun fact: you can give names to the elements in a vector! (Yes, I am quite the thrill at parties.)\nNaming is something we may expect more with data frames (e.g., the names of each column), but we can also give names to columns/rows in matrices and other R objects. This can be helpful if we have a vector of values to loop through that do not simply go from 1 up to some larger number. For example, on Homework 1, Exercise 3b, we are asked to increase the sample size from 100 to 100,000 by increments of 100.\nFor our example here, we will choose a more manageable scenario of sample sizes from 100 to 500 by increments of 100, where we want to calculate the sample mean for a random sample from the normal distribution with \\(\\mu=5\\) and \\(\\sigma=3\\) at each of the sample sizes.\nOur first step is to create the vector from 100 to 500 by increments of 100:\n\n\nCode\n# Generate vector of sample sizes\nsampsize_vec &lt;- seq(from=100, to=500, by=100)\nsampsize_vec\n\n\n[1] 100 200 300 400 500\n\n\nThe next step is to create a vector to save the results, with names for the different sample sizes (note: here they are numeric, but you could have names that are text strings as well):\n\n\nCode\n# Generate vector save results\nmean3 &lt;- rep(NA, length(sampsize_vec) )\nmean3\n\n\n[1] NA NA NA NA NA\n\n\nCode\n# Rename vector elements to be the sample size\nnames(mean3) &lt;- sampsize_vec\nmean3\n\n\n100 200 300 400 500 \n NA  NA  NA  NA  NA \n\n\nWe see above that mean3 now has a “name” for each component of the vector. The one super tricky thing about names that are also numbers is that we have to be very intentional in what we request from R. For example:\n\n\nCode\n# Extract what is in named '200' \nmean3['200']\n\n\n200 \n NA \n\n\nCode\n# What if we forget the quotes/apostrophes\nmean3[200]\n\n\n&lt;NA&gt; \n  NA \n\n\nWe will have to make sure in our loop to enter the results with the name as a character string, even though it is a number. This is because R, by default, is trying to extract the 200th element of our vector of length 5 (i.e., it doesn’t exist).\nLet’s now program the loop to save the resulting sample means:\n\n\nCode\n# set our seed for reproducibility\nset.seed(515)\n\n# calculate the means at different sample sizes\nfor( n in sampsize_vec){\n  mean3[ paste0(n) ] &lt;- mean( rnorm(n=n, mean=5, sd=3) )\n}\n\nmean3\n\n\n     100      200      300      400      500 \n5.458848 5.294377 5.187769 4.667010 4.790853 \n\n\nNotice that n is coerced to a character by using paste0(n), but remains a number for rnorm(n=n.... The nice thing here is that our results are already labeled, so we can identify which mean corresponds to which sample size."
  },
  {
    "objectID": "labs/lab2/index.html#for-loop-example-4-vector-with-names-but-loop-through-an-index-to-extract-specific-value",
    "href": "labs/lab2/index.html#for-loop-example-4-vector-with-names-but-loop-through-an-index-to-extract-specific-value",
    "title": "Week 2 Lab",
    "section": "for Loop Example 4: Vector with Names, but Loop through an Index to Extract Specific Value",
    "text": "for Loop Example 4: Vector with Names, but Loop through an Index to Extract Specific Value\nLet’s recreate Example 3 above, but instead of looping through the sampsize_vec, let’s loop through an index from 1:length(sampsize_vec) and extract the correct sample size. We’ll also recreate the named vector to store our results in:\n\n\nCode\nmean4 &lt;- rep(NA, length(sampsize_vec) )\nnames(mean4) &lt;- sampsize_vec\n\n# set our seed for reproducibility\nset.seed(515)\n\n# calculate the means at different sample sizes\nfor( sampsize in 1:length(sampsize_vec)){\n    n_to_use &lt;- sampsize_vec[ sampsize ] #extract the \"sampsize\"th value from the vector\n    mean4[ paste0(n_to_use) ] &lt;- mean( rnorm(n=n_to_use, mean=5, sd=3) )\n}\n\nmean4\n\n\n     100      200      300      400      500 \n5.458848 5.294377 5.187769 4.667010 4.790853 \n\n\nWe can see that mean3 and mean4 are identical, illustrating we can achieve the same result in different ways. If the names were not of interest, we could have also stored the results either via concatenation with a NULL initial object or by indexing the vector (i.e., mean4[ sampsize ]).\nIn this example the strategy above may seem a bit silly. However, in more complex simulations or problems you may wish to work with different structures of data or multiple scenarios that are easier to define outside the loop."
  },
  {
    "objectID": "labs/lab4/index.html",
    "href": "labs/lab4/index.html",
    "title": "Week 4 Lab",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Labs collection, the mini-lecture content delivered at the start of class before breaking out into small groups to work on the homework assignment."
  },
  {
    "objectID": "labs/lab4/index.html#scenario-1---known-sigma-find-power",
    "href": "labs/lab4/index.html#scenario-1---known-sigma-find-power",
    "title": "Week 4 Lab",
    "section": "Scenario 1 - Known \\(\\sigma\\), Find Power",
    "text": "Scenario 1 - Known \\(\\sigma\\), Find Power\n\\(\\sigma=10\\) (known);\\(N=15,20,25\\); detectable difference between null and alternative means from -15 to 15; \\(\\alpha=0.01,0.05,0.10\\) (two-sided). Find the power.\nFrom our lecture slides we know that\n\\[ Z_{1-\\beta} = \\frac{|\\mu_0 - \\mu_1|}{se(\\bar{X})} - Z_{1-\\frac{\\alpha}{2}} \\]\nHowever, in this form we have to do a few more steps to get the power and not just \\(Z_{1-\\beta}\\) (i.e., the value of the Z-statistic from our standard normal distribution that is at the given level of power):\n\\[ 1-\\beta = \\Phi\\left[ \\frac{|\\mu_0 - \\mu_1|}{\\sigma / \\sqrt{n}} - Z_{1-\\frac{\\alpha}{2}} \\right] \\]\nIn R it is easiest to program our own function from the equations provided in this lecture to calculate the power for our two-sided Z:\n\n\nCode\n# Write function to calculate the power given the detectable difference, SD, N, and alpha\nfindPowerZ &lt;- function(diff = 5, sd = 1, n = 10, alpha = 0.05) { \n    z.alpha &lt;- qnorm(1 - (alpha/2))\n    power &lt;- pnorm( abs(diff) / (sd/sqrt(n) ) - z.alpha)\n    return(power)\n}\n\n\nFor example, we can calculate what our power would be if we observed a detectable difference of either 5 or -5 when \\(\\sigma=10\\), \\(N=20\\), \\(\\alpha=0.05\\):\n\n\nCode\n# Calculate power for a detectable difference of 5\nfindPowerZ( diff=5, sd=10, n=20, alpha=0.05)\n\n\n[1] 0.6087659\n\n\nCode\n# Calculate power for a detectable difference of -5\nfindPowerZ( diff=-5, sd=10, n=20, alpha=0.05)\n\n\n[1] 0.6087659\n\n\nNote that they result in the same power because the normal distribution is symmetric and we assume that \\(H_0: \\mu_0=0\\).\nWe can leverage the expand.grid() function to more easily identify all combinations of \\(n\\), \\(\\alpha\\), and our detectable difference (279 in total) to then loop through and calculate the power:\n\n\nCode\n# Set standard deviation at desired value and create data frame with expand.grid for all combinations of parameters of interest\nstdev &lt;- 10\npowers &lt;- expand.grid(n=c(15,20,25), alpha=c(0.01,0.05,0.1), diff=(-15:15))\n\n# Create new column to save results of power\npowers$power &lt;- NA\n\n# Loop through each row (i.e., combination) and calculate the power achieved\nfor(i in 1:nrow(powers)){ \n    p &lt;- findPowerZ(n = powers$n[i], alpha = powers$alpha[i], diff = powers$diff[i], sd = stdev)\n    powers$power[i] &lt;- p\n}\n\n# View the first 6 rows\nhead(powers)\n\n\n   n alpha diff     power\n1 15  0.01  -15 0.9993889\n2 20  0.01  -15 0.9999820\n3 25  0.01  -15 0.9999996\n4 15  0.05  -15 0.9999408\n5 20  0.05  -15 0.9999990\n6 25  0.05  -15 1.0000000\n\n\nWe may remember that some functions in R are able to handle vectorized arguments (i.e., instead of looping through a vector of values of interest it can automatically calculate across all values in the vector). Here we see that skipping the for loop with our findPowerZ function results in the same power estimates:\n\n\nCode\n# Since our findPowerZ function uses R functions that can take vectors for their arguments, we can bypass the loop and just use our respective columns to calculate the power without a for loop:\n\npowers$power_vectorize &lt;- findPowerZ(n = powers$n, alpha = powers$alpha, diff = powers$diff, sd = stdev)\n\n# check that all powers are equal (should all be TRUE)\ntable(powers$power == powers$power_vectorize)\n\n\n\nTRUE \n 279 \n\n\nNow that we’ve calculated the expected power for each of the 279 combinations, we can work on plotting them in one figure. For this example we will plot our desired measure, power, on the y-axis, and the detectable difference on the x-axis.\nThe figure will be somewhat busy, but we can help differentiate the various combinations with different line colors, line styles, and types of points. One extra step we need to complete is to tell R we want to treat n and alpha as factors, because as their numeric values ggplot() will return errors.\n\n\nCode\n# load the ggplot2 package\nlibrary(ggplot2)\n\n# To take advantage of the linetype, color, and shape arguments with our current data frame we need to coerce the values into factors\npowers$n &lt;- as.factor(powers$n)\npowers$alpha &lt;- as.factor(powers$alpha)\n\n# Plot the resulting power curves\nqplot(diff, power, data = powers, linetype = n, color = alpha, geom = \"line\",shape = n) +\n    geom_point() + # add points to plot\n  theme_bw() + # change plotting aesthetic/style to have black/white background with grid\n  xlab(\"Difference\") + ylab(\"Power\") + # change axis labels\n    ggtitle(\"Achievable Power vs. Difference of Means by Sample Size (N) and Alpha\")\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nFrom this figure we can make various insights, a few of which are highlighted below:\n\nWhen we evaluate the power at \\(H_0\\), what we actually have summarized is the type I error rate. We can roughly observe this since the colored lines achieve the approximate \\(\\alpha\\) value when the difference is 0. Alternatively, we can look at our powers data frame to review the resulting power when the difference is 0. (Strictly speaking, since we were calculating power, we actually have calculated \\(\\frac{\\alpha}{2}\\)…but we could solve the equation for \\(Z_{1-\\alpha/2}\\) to then solve for \\(\\alpha\\) precisely.)\nWe can see that as \\(\\alpha\\) increases we have higher power, holding \\(n\\) and the difference constant. This makes sense given the trade off of \\(\\alpha\\) and \\(\\beta\\) (our type I and type II errors).\nSimilar, as \\(n\\) increases we have higher power.\nWe can see the combinations where we wouldn’t have desirable power (e.g., &lt;80%)."
  },
  {
    "objectID": "labs/lab4/index.html#equivalent-of-qplot",
    "href": "labs/lab4/index.html#equivalent-of-qplot",
    "title": "Week 4 Lab",
    "section": "Equivalent of qplot",
    "text": "Equivalent of qplot\nWe can also just use the direct ggplot functions instead of its quick plotting shortcut function:\n\n\nCode\n# To take advantage of the linetype, color, and shape arguments with our current data frame we need to coerce the values into factors\npowers$n &lt;- as.factor(powers$n)\npowers$alpha &lt;- as.factor(powers$alpha)\n\n# Plot the resulting power curves\nggplot(data=powers, aes(x=diff, y=power, color=alpha, shape=n, linetype=n)) +\n  geom_line() +\n    geom_point() + \n  theme_bw() + \n  xlab(\"Difference\") + ylab(\"Power\") +\n    ggtitle(\"Achievable Power vs. Difference of Means by Sample Size (N) and Alpha\")"
  },
  {
    "objectID": "labs/lab4/index.html#scenario-2---unknown-sigma-find-power",
    "href": "labs/lab4/index.html#scenario-2---unknown-sigma-find-power",
    "title": "Week 4 Lab",
    "section": "Scenario 2 - Unknown \\(\\sigma\\), Find Power",
    "text": "Scenario 2 - Unknown \\(\\sigma\\), Find Power\n\\(s=10\\) (\\(\\sigma\\) unknown);\\(N=15,20,25\\); detectable difference between null and alternative means from -15 to 15; \\(\\alpha=0.01,0.05,0.10\\) (two-sided). Find the power.\nIn this case we know that there are existing functions we can leverage to calculate our power, such as power.t.test():\n\n\nCode\npwrt_b &lt;- power.t.test(n = 15, sd = 10, sig.level = 0.01, delta = -15, type = \"one.sample\", alternative = \"two.sided\")\npwrt_b # print the results\n\n\n\n     One-sample t test power calculation \n\n              n = 15\n          delta = 15\n             sd = 10\n      sig.level = 0.01\n          power = 0.9937996\n    alternative = two.sided\n\n\nCode\npwrt_b$power # specifically print the estimated power\n\n\n[1] 0.9937996\n\n\nWe can essentially take our code from Scenario 1 above and replace the function with power.t.test to create our figure:\n\n\nCode\n# Set standard deviation at desired value and create data frame with expand.grid for all combinations of parameters of interest\nstdev &lt;- 10\npowers2 &lt;- expand.grid(n=c(15,20,25), alpha=c(0.01,0.05,0.1), diff=(-15:15))\n\n# Create new column to save results of power\npowers2$power &lt;- NA\n\n# Loop through each row (i.e., combination) and calculate the power achieved\nfor(i in 1:nrow(powers2)){ \n    p &lt;- power.t.test(n = powers2$n[i], sig.level = powers2$alpha[i], delta = powers2$diff[i], sd = stdev, power = NULL, \n                      type = 'one.sample', alternative = 'two.sided')\n    powers2$power[i] &lt;- p$power\n}\n\n\n\n\nCode\n# load the ggplot2 package (in case you hadn't already)\nlibrary(ggplot2)\n\n# To take advantage of the linetype, color, and shape arguments with our current data frame we need to coerce the values into factors\npowers2$n &lt;- as.factor(powers2$n)\npowers2$alpha &lt;- as.factor(powers2$alpha)\n\n# Plot the resulting power curves\nqplot(diff, power, data = powers2, linetype = n, color = alpha, geom = \"line\",shape = n) +\n    geom_point() + theme_bw() + xlab(\"Difference\") + ylab(\"Power\") +\n    ggtitle(\"Achievable Power vs. Difference of Means by Sample Size (N) and Alpha\")\n\n\n\n\n\nWe see similar results to Scenario 1, however we can see that for each combination, we have generally lower power than in Scenario 1. This is a result of the difference between assuming known versus unknown standard deviation."
  },
  {
    "objectID": "labs/lab4/index.html#can-power.t.test-take-vectors",
    "href": "labs/lab4/index.html#can-power.t.test-take-vectors",
    "title": "Week 4 Lab",
    "section": "Can power.t.test Take Vectors",
    "text": "Can power.t.test Take Vectors\nThe power.t.test function is able to handle our vectorized data, but we can see that it returns multiple values from the function the we could reference:\n\n\nCode\nnames(pwrt_b)\n\n\n[1] \"n\"           \"delta\"       \"sd\"          \"sig.level\"   \"power\"      \n[6] \"alternative\" \"note\"        \"method\"     \n\n\nSo we can extract whatever information we find most relevant:\n\n\nCode\n# Extract only the power\npwrt_b$power\n\n\n[1] 0.9937996\n\n\nCode\n# Extract only the delta\npwrt_b$delta\n\n\n[1] 15\n\n\nCode\n# Extract both power and delta\npwrt_b[c('power','delta')]\n\n\n$power\n[1] 0.9937996\n\n$delta\n[1] 15\n\n\nCode\n# Extract both power and delta, but force it to be a vector and not a list\nunlist(pwrt_b[c('power','delta')])\n\n\n     power      delta \n 0.9937996 15.0000000 \n\n\nIf we just put in our vectors we get all of the quantities returned:\n\n\nCode\nstdev &lt;- 10\npowers2_vec &lt;- expand.grid(n=c(15,20,25), alpha=c(0.01,0.05,0.1), diff=(-15:15))\n\npwrt_vectorized &lt;- power.t.test(n = powers2_vec$n, \n                                sig.level = powers2_vec$alpha, \n                                delta = powers2_vec$diff, \n                                sd = stdev, \n                                power = NULL, \n                                type = 'one.sample', \n                                alternative = 'two.sided')\npwrt_vectorized\n\n\n\n     One-sample t test power calculation \n\n              n = 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25, 15, 20, 25\n          delta = 15, 15, 15, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15\n             sd = 10\n      sig.level = 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05, 0.10, 0.10, 0.10\n          power = 0.99379964, 0.99977175, 0.99999374, 0.99968449, 0.99999414, 0.99999991, 0.99994334, 0.99999923, 0.99999999, 0.98492058, 0.99904610, 0.99995370, 0.99890204, 0.99996285, 0.99999894, 0.99976298, 0.99999400, 0.99999987, 0.96672386, 0.99656180, 0.99971913, 0.99659928, 0.99980079, 0.99998997, 0.99912482, 0.99996071, 0.99999843, 0.93326751, 0.98929560, 0.99860140, 0.99061448, 0.99909540, 0.99992373, 0.99714477, 0.99978391, 0.99998523, 0.87812317, 0.97115118, 0.99427099, 0.97688373, 0.99651570, 0.99953252, 0.99175854, 0.99900034, 0.99988907, 0.79670393, 0.93249537, 0.98063807, 0.94908647, 0.98859129, 0.99768460, 0.97891618, 0.99610286, 0.99933233, 0.68910390, 0.86228289, 0.94578357, 0.89945099, 0.96815164, 0.99070503, 0.95208492, 0.98716538, 0.99677194, 0.56192862, 0.75363390, 0.87346165, 0.82130998, 0.92389872, 0.96963105, 0.90297615, 0.96417280, 0.98741901, 0.42761072, 0.61050606, 0.75174714, 0.71289906, 0.84350550, 0.91877622, 0.82425706, 0.91484955, 0.96028374, 0.30095252, 0.45011623, 0.58571735, 0.58040966, 0.72100197, 0.82072129, 0.71377695, 0.82663952, 0.89776394, 0.19441072, 0.29734431, 0.40227469, 0.43784659, 0.56448290, 0.66970136, 0.57805549, 0.69514934, 0.78338612, 0.11453654, 0.17375617, 0.23822547, 0.30284108, 0.39686955, 0.48396689, 0.43215762, 0.53181412, 0.61725900, 0.06121551, 0.08891184, 0.11957051, 0.19037733, 0.24648577, 0.30161925, 0.29495606, 0.36277965, 0.42572891, 0.02954918, 0.03952378, 0.05021130, 0.10800411, 0.13348840, 0.15876192, 0.18211929, 0.21707518, 0.25048467, 0.01283438, 0.01516757, 0.01747061, 0.05498090, 0.06241123, 0.06948589, 0.10098691, 0.11249211, 0.12326268, 0.00500000, 0.00500000, 0.00500000, 0.02500000, 0.02500000, 0.02500000, 0.05000000, 0.05000000, 0.05000000, 0.01283438, 0.01516757, 0.01747061, 0.05498090, 0.06241123, 0.06948589, 0.10098691, 0.11249211, 0.12326268, 0.02954918, 0.03952378, 0.05021130, 0.10800411, 0.13348840, 0.15876192, 0.18211929, 0.21707518, 0.25048467, 0.06121551, 0.08891184, 0.11957051, 0.19037733, 0.24648577, 0.30161925, 0.29495606, 0.36277965, 0.42572891, 0.11453654, 0.17375617, 0.23822547, 0.30284108, 0.39686955, 0.48396689, 0.43215762, 0.53181412, 0.61725900, 0.19441072, 0.29734431, 0.40227469, 0.43784659, 0.56448290, 0.66970136, 0.57805549, 0.69514934, 0.78338612, 0.30095252, 0.45011623, 0.58571735, 0.58040966, 0.72100197, 0.82072129, 0.71377695, 0.82663952, 0.89776394, 0.42761072, 0.61050606, 0.75174714, 0.71289906, 0.84350550, 0.91877622, 0.82425706, 0.91484955, 0.96028374, 0.56192862, 0.75363390, 0.87346165, 0.82130998, 0.92389872, 0.96963105, 0.90297615, 0.96417280, 0.98741901, 0.68910390, 0.86228289, 0.94578357, 0.89945099, 0.96815164, 0.99070503, 0.95208492, 0.98716538, 0.99677194, 0.79670393, 0.93249537, 0.98063807, 0.94908647, 0.98859129, 0.99768460, 0.97891618, 0.99610286, 0.99933233, 0.87812317, 0.97115118, 0.99427099, 0.97688373, 0.99651570, 0.99953252, 0.99175854, 0.99900034, 0.99988907, 0.93326751, 0.98929560, 0.99860140, 0.99061448, 0.99909540, 0.99992373, 0.99714477, 0.99978391, 0.99998523, 0.96672386, 0.99656180, 0.99971913, 0.99659928, 0.99980079, 0.99998997, 0.99912482, 0.99996071, 0.99999843, 0.98492058, 0.99904610, 0.99995370, 0.99890204, 0.99996285, 0.99999894, 0.99976298, 0.99999400, 0.99999987, 0.99379964, 0.99977175, 0.99999374, 0.99968449, 0.99999414, 0.99999991, 0.99994334, 0.99999923, 0.99999999\n    alternative = two.sided\n\n\nFortunately this is easily solved by either [1] just extracting the power object in the results or [2] adding $power to our function so it only return those values:\n\n\nCode\n# Extract results from output dump\npowers2_vec$power &lt;- pwrt_vectorized$power\n\n# Rerun power.t.test to only save power\npowers2_vec$power_v2 &lt;- power.t.test(n = powers2_vec$n, \n                                sig.level = powers2_vec$alpha, \n                                delta = powers2_vec$diff, \n                                sd = stdev, \n                                power = NULL, \n                                type = 'one.sample', \n                                alternative = 'two.sided')$power\n\n# check that everything matches the for loop above\ntable(powers2$power == powers2_vec$power)\n\n\n\nTRUE \n 279 \n\n\nCode\ntable(powers2$power == powers2_vec$power_v2)\n\n\n\nTRUE \n 279"
  },
  {
    "objectID": "labs/lab4/index.html#scenario-3---unknown-sigma-find-n",
    "href": "labs/lab4/index.html#scenario-3---unknown-sigma-find-n",
    "title": "Week 4 Lab",
    "section": "Scenario 3 - Unknown \\(\\sigma\\), Find N",
    "text": "Scenario 3 - Unknown \\(\\sigma\\), Find N\n\\(s=10\\) (\\(\\sigma\\) unknown); detectable difference between null and alternative means from 5 to 10; \\(\\alpha=0.01,0.05,0.10\\) (two-sided); \\(1-\\beta=\\text{Power}=0.80,0.90,0.95\\). Find N.\nWe again can modify our code from above to solve for the sample size needed. But first let’s examine how we can extract the needed sample size and round up to the next whole number with the ceiling() function:\n\n\nCode\npwrt_c &lt;- power.t.test(power=0.8, sd = 10, sig.level = 0.01, delta = 5, type = \"one.sample\", alternative = \"two.sided\")\npwrt_c # view all output\n\n\n\n     One-sample t test power calculation \n\n              n = 50.0647\n          delta = 5\n             sd = 10\n      sig.level = 0.01\n          power = 0.8\n    alternative = two.sided\n\n\nCode\nceiling(pwrt_c$n) # only extract the needed N and round up to the next whole number\n\n\n[1] 51\n\n\nWe see that our estimate here is rounded up from 50.0646962 to be 51.\nNow let’s modify our code from Scenario 2 to solve for \\(N\\) over differences from 5 to 10 in increments of 0.1:\n\n\nCode\n# Set standard deviation at desired value and create data frame with expand.grid for all combinations of parameters of interest\nstdev &lt;- 10\nsampsize_df &lt;- expand.grid(power=c(0.8,0.9,0.95), alpha=c(0.01,0.05,0.1), diff=seq(5,10,by=0.1) )\n\n# Create new column to save results of N\nsampsize_df$n &lt;- NA\n\n# Loop through each row (i.e., combination) and calculate the power achieved\nfor(i in 1:nrow(sampsize_df)){ \n    p &lt;- power.t.test(n = NULL, sig.level = sampsize_df$alpha[i], delta = sampsize_df$diff[i], sd = stdev, \n                      power = sampsize_df$power[i], type = 'one.sample', alternative = 'two.sided')\n    sampsize_df$n[i] &lt;- ceiling( p$n )\n}\n\n# load the ggplot2 package (in case you hadn't already)\nlibrary(ggplot2)\n\n# To take advantage of the linetype, color, and shape arguments with our current data frame we need to coerce the values into factors\nsampsize_df$power &lt;- as.factor(sampsize_df$power)\nsampsize_df$alpha &lt;- as.factor(sampsize_df$alpha)\n\n# Plot the resulting power curves\nqplot(diff, n, data = sampsize_df, linetype = power, color = alpha, geom = \"line\",shape = power) +\n    geom_point() + theme_bw() + xlab(\"Difference\") + ylab(\"Required Sample Size\") +\n    ggtitle(\"Required Sample Size (N) vs. Difference of Means by Power and Alpha\")\n\n\n\n\n\nFrom this figure we can take away the conclusion that smaller detectable differences lead to a larger sample size to achieve a desired power, type I error rate, and SD."
  },
  {
    "objectID": "labs/lab4/index.html#scenario-4---unknown-sigma-find-detectable-difference",
    "href": "labs/lab4/index.html#scenario-4---unknown-sigma-find-detectable-difference",
    "title": "Week 4 Lab",
    "section": "Scenario 4 - Unknown \\(\\sigma\\), Find Detectable Difference",
    "text": "Scenario 4 - Unknown \\(\\sigma\\), Find Detectable Difference\n\\(s=10\\) (\\(\\sigma\\) unknown); \\(N=15,20,25\\), \\(\\alpha=0.01,0.05,0.10\\) (two-sided); \\(1-\\beta=\\text{Power}=0.80,0.90,0.95\\). Find the detectable difference between null and alternative means.\nAgain, we can modify our earlier set-up from Scenarios 2 and 3 to address this question:\n\n\nCode\npwrt_d &lt;- power.t.test(power=0.8, sd = 10, sig.level = 0.01, n=15, type = \"one.sample\", alternative = \"two.sided\")\npwrt_d # view the output\n\n\n\n     One-sample t test power calculation \n\n              n = 15\n          delta = 10.03483\n             sd = 10\n      sig.level = 0.01\n          power = 0.8\n    alternative = two.sided\n\n\nCode\npwrt_d$delta # view the detectable difference only\n\n\n[1] 10.03483\n\n\nLet’s modify our code to create a plot of our detectable difference across the scenarios:\n\n\nCode\n# Set standard deviation at desired value and create data frame with expand.grid for all combinations of parameters of interest\nstdev &lt;- 10\ndiff_df &lt;- expand.grid(n=c(15,20,25), alpha=c(0.01,0.05,0.1), power=c(0.8,0.9,0.95))\n\n# Create new column to save results of diff\ndiff_df$diff &lt;- NA\n\n# Loop through each row (i.e., combination) and calculate the power achieved\nfor(i in 1:nrow(diff_df)){ \n    p &lt;- power.t.test(n = diff_df$n[i], sig.level = diff_df$alpha[i], delta = NULL, sd = stdev, power = diff_df$power[i], \n                      type = 'one.sample', alternative = 'two.sided')\n    diff_df$diff[i] &lt;- p$delta\n}\n\n# load the ggplot2 package (in case you hadn't already)\nlibrary(ggplot2)\n\n# To take advantage of the linetype, color, and shape arguments with our current data frame we need to coerce the values into factors\ndiff_df$power &lt;- as.factor(diff_df$power)\ndiff_df$alpha &lt;- as.factor(diff_df$alpha)\n\n# Plot the resulting power curves\nqplot(n, diff, data = diff_df, linetype = power, color = alpha, geom = \"line\",shape = power) +\n    geom_point() + theme_bw() + xlab(\"Sample Size\") + ylab(\"Difference in Means\") +\n    ggtitle(\"Detectable Mean Difference vs. Sample Size (N) by Power and Alpha\")\n\n\n\n\n\nIn this figure we can note that for any combination of \\(\\alpha\\) and power, the smaller our sample size is, the larger our difference in means will be that we can detect. In other words, if we wanted to detect a smaller difference in means, we would need to increase our sample size if we already set the values for \\(s\\), \\(\\alpha\\), and \\(1-\\beta\\) (power)."
  },
  {
    "objectID": "labs/lab6/index.html",
    "href": "labs/lab6/index.html",
    "title": "Week 6 Lab",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Labs collection, the mini-lecture content delivered at the start of class before breaking out into small groups to work on the homework assignment."
  },
  {
    "objectID": "labs/lab6/index.html#the-colorful-dataset",
    "href": "labs/lab6/index.html#the-colorful-dataset",
    "title": "Week 6 Lab",
    "section": "The Colorful Dataset",
    "text": "The Colorful Dataset\nLet’s assume we have conducted a study with 6 observations in two groups, each represented by a color. Our “outcome” is the average color within the group (we can calculate this using the average_colors() function from the miscHelpers package that can be downloaded from GitHub):\n\n\nCode\n# Run this function to install the kableExtra package to create some tables\n#devtools::install_github(\"haozhu233/kableExtra\")\nlibrary(kableExtra)\n\n# Run this function to install the miscHelpers package to use the average_colors() function\n#remotes::install_github(\"BenaroyaResearch/miscHelpers\")\nlibrary(miscHelpers)\n\n# Create vectors to store colors in\ngrp1 &lt;- c('#0072B2','#0072B2','#0072B2','#97C9E4','#97C9E4','#97C9E4') # vector with 2 blues\ngrp2 &lt;- c('#F0E442','#F0E442','#F0E442','#EA1F2F','#EA1F2F','#EA1F2F') # vector with reds and yellows\n\n# Create matrix for 1:6 (group 1) and A:F (group 2)\ngrp_mat &lt;- matrix( c(1:6,'Avg. 1','A','B','C','D','E','F','Avg. 2'), nrow=7, byrow=F, dimnames=list(c(paste0('Observation ',1:6),'Average'), c('Group 1','Group 2')))\n\ngrp_mat %&gt;%\n  kbl(align='cc') %&gt;%\n  kable_paper(full_width = F) %&gt;%\n  column_spec(2, color = \"black\",\n            background = c(grp1, average_colors(grp1))) %&gt;%\n  column_spec(3, color = \"black\",\n              background = c(grp2, average_colors(grp2))) %&gt;%\n  row_spec(6, extra_css = \"border-bottom: 2px solid\")\n\n\n\n\n\n\nGroup 1\nGroup 2\n\n\n\n\nObservation 1\n1\nA\n\n\nObservation 2\n2\nB\n\n\nObservation 3\n3\nC\n\n\nObservation 4\n4\nD\n\n\nObservation 5\n5\nE\n\n\nObservation 6\n6\nF\n\n\nAverage\nAvg. 1\nAvg. 2\n\n\n\n\n\n\n\nWe can see in the above that the average color in Group 1 is a blue that is the “average” of the 3 light and 3 dark blues. For Group 2 the average of 3 yellows and 3 reds is orange.\nLet’s first see an example of bootstrap resampling. Remember, here we sample within each group and with replacement:\n\n\nCode\nset.seed(515)\ngrp1b &lt;- sort(sample(1:6, size=6, replace=T))\ngrp2b &lt;- sort(sample(1:6, size=6, replace=T))\n\ngrp_matb &lt;- matrix( c((1:6)[grp1b],'Avg. Boot 1',c('A','B','C','D','E','F')[grp2b],'Avg. Boot 2'), nrow=7, byrow=F)\ncolnames(grp_matb) &lt;- c('Boot Group 1','Boot Group 2')\n\ngrp_combo &lt;- cbind(grp_mat, '', grp_matb)\n\ngrp_combo %&gt;%\n  kbl(align='cc') %&gt;%\n  kable_paper(full_width = F) %&gt;%\n  column_spec(2, color = \"black\",\n            background = c(grp1, average_colors(grp1))) %&gt;%\n  column_spec(3, color = \"black\",\n              background = c(grp2, average_colors(grp2))) %&gt;%\n  column_spec(5, color = \"black\",\n            background = c(grp1[grp1b], average_colors(grp1[grp1b]))) %&gt;%\n  column_spec(6, color = \"black\",\n              background = c(grp2[grp2b], average_colors(grp2[grp2b]))) %&gt;%\n  row_spec(6, extra_css = \"border-bottom: 2px solid\")\n\n\n\n\n\n\nGroup 1\nGroup 2\n\nBoot Group 1\nBoot Group 2\n\n\n\n\nObservation 1\n1\nA\n\n1\nA\n\n\nObservation 2\n2\nB\n\n2\nA\n\n\nObservation 3\n3\nC\n\n3\nB\n\n\nObservation 4\n4\nD\n\n4\nB\n\n\nObservation 5\n5\nE\n\n4\nC\n\n\nObservation 6\n6\nF\n\n6\nC\n\n\nAverage\nAvg. 1\nAvg. 2\n\nAvg. Boot 1\nAvg. Boot 2\n\n\n\n\n\n\n\n\nIn this bootstrap, we see that Boot Group 1 has resampled the 4th observation twice, so the 5th “light blue” observation is not included in the sample. However, since both observation 4 and 5 are “light blue” the overall average color is unchanged!\nIn this bootstrap, we see that Boot Group 2 has resampled each “A”, “B”, and “C” twice…leaving no red observations! In this case the bootstrap distribution is only yellow observations (one potential extreme) and our average is simply yellow.\n\nHow does this differ from a permutation test? For the permutation test we combine all 12 observations before resampling without replacement as to who belongs to which group:\n\n\nCode\nset.seed(1012)\ngrp_perm &lt;- sort(sample(1:12, size=6, replace=F)) # sample 6 observations to go into group 1, the rest will go into group 2\n\ngrp1p &lt;- c(1:6,'A','B','C','D','E','F')[grp_perm] # take values according to index sampled for grp_perm\ngrp2p &lt;- c(1:6,'A','B','C','D','E','F')[-grp_perm] # remove values according to index sampled from grp_perm\n\ngrp1p_col &lt;- c(grp1,grp2)[grp_perm]\ngrp2p_col &lt;- c(grp1,grp2)[-grp_perm]\n\ngrp_matp &lt;- matrix( c(grp1p,'Avg. Perm 1',grp2p,'Avg. Perm 2'), nrow=7, byrow=F)\ncolnames(grp_matp) &lt;- c('Perm Group 1','Perm Group 2')\n\ngrp_combo2 &lt;- cbind(grp_combo, '', grp_matp)\n\ngrp_combo2 %&gt;%\n  kbl(align='cc') %&gt;%\n  kable_paper(full_width = F) %&gt;%\n  column_spec(2, color = \"black\",\n            background = c(grp1, average_colors(grp1))) %&gt;%\n  column_spec(3, color = \"black\",\n              background = c(grp2, average_colors(grp2))) %&gt;%\n  column_spec(5, color = \"black\",\n            background = c(grp1[grp1b], average_colors(grp1[grp1b]))) %&gt;%\n  column_spec(6, color = \"black\",\n              background = c(grp2[grp2b], average_colors(grp2[grp2b]))) %&gt;%\n  column_spec(8, color = \"black\",\n            background = c(grp1p_col, average_colors(grp1p_col))) %&gt;%\n  column_spec(9, color = \"black\",\n              background = c(grp2p_col, average_colors(grp2p_col))) %&gt;%\n  row_spec(6, extra_css = \"border-bottom: 2px solid\")\n\n\n\n\n\n\nGroup 1\nGroup 2\n\nBoot Group 1\nBoot Group 2\n\nPerm Group 1\nPerm Group 2\n\n\n\n\nObservation 1\n1\nA\n\n1\nA\n\n1\n2\n\n\nObservation 2\n2\nB\n\n2\nA\n\n3\n4\n\n\nObservation 3\n3\nC\n\n3\nB\n\nA\n5\n\n\nObservation 4\n4\nD\n\n4\nB\n\nC\n6\n\n\nObservation 5\n5\nE\n\n4\nC\n\nE\nB\n\n\nObservation 6\n6\nF\n\n6\nC\n\nF\nD\n\n\nAverage\nAvg. 1\nAvg. 2\n\nAvg. Boot 1\nAvg. Boot 2\n\nAvg. Perm 1\nAvg. Perm 2\n\n\n\n\n\n\n\n\nIn our permutation sample we see that group membership has been broken, so that members of the original Group 1 and Group 2 are now part of the Perm Group 1 and Perm Group 2. This process helps us to break any potential association that may have existed previously (e.g., only blues in one group versus red and yellows in another).\nWe see our average colors are now an interesting brownish (Avg. Perm 1) and grayish-blue (Avg. Perm 2).\nIf our original observation (e.g., the difference in sample means between groups) was actually from the null distribution, then we would expect that estimate to fall near the center of our permutation distribution. In our color example, the original colors are decided in our two camps (blue vs. red/yellow), so here we see a muddier picture of the average color.\n\nBased on either approach, we would want to conduct a large number of bootstrap or permutation resamples, and then examine the overall distribution:\n\n\nCode\nbp_mat &lt;- matrix('', ncol=5,nrow=100)\ncolnames(bp_mat) &lt;- c('Boot Group 1','Boot Group 2','','Perm Group 1','Perm Group 2')\nrownames(bp_mat) &lt;- paste0('Simulation ',1:100)\n\nfor(j in 1:100){\n  set.seed(2020+j)\n  \n  # permutation resample\n  grp_perm &lt;- sample(1:12, size=6, replace=F) # sample 6 observations to go into group 1, the rest will go into group 2\n  bp_mat[j,'Perm Group 1'] &lt;- average_colors(c(grp1,grp2)[grp_perm])\n  bp_mat[j,'Perm Group 2'] &lt;- average_colors(c(grp1,grp2)[-grp_perm])\n  \n  # bootstrap resample\n  bp_mat[j,'Boot Group 1'] &lt;- average_colors( grp1[ sample(1:6,size=6,replace=T) ])\n  bp_mat[j,'Boot Group 2'] &lt;- average_colors( grp2[ sample(1:6,size=6,replace=T) ])\n}\n\n# Object to create kable from\nbp_mat_kableshell &lt;- matrix('', ncol=5,nrow=100)\ncolnames(bp_mat_kableshell) &lt;- c('Boot Group 1','Boot Group 2','','Perm Group 1','Perm Group 2')\nrownames(bp_mat_kableshell) &lt;- paste0('Simulation ',1:100)\n\nbp_mat_kableshell %&gt;%\n  kbl() %&gt;%\n  kable_paper(full_width = F) %&gt;%\n  column_spec(2, color = \"black\",\n            background = bp_mat[,'Boot Group 1']) %&gt;%\n  column_spec(3, color = \"black\",\n              background = bp_mat[,'Boot Group 2']) %&gt;%\n  column_spec(5, color = \"black\",\n            background = bp_mat[,'Perm Group 1']) %&gt;%\n  column_spec(6, color = \"black\",\n              background = bp_mat[,'Perm Group 2'])\n\n\n\n\n\n\nBoot Group 1\nBoot Group 2\n\nPerm Group 1\nPerm Group 2\n\n\n\n\nSimulation 1\n\n\n\n\n\n\n\nSimulation 2\n\n\n\n\n\n\n\nSimulation 3\n\n\n\n\n\n\n\nSimulation 4\n\n\n\n\n\n\n\nSimulation 5\n\n\n\n\n\n\n\nSimulation 6\n\n\n\n\n\n\n\nSimulation 7\n\n\n\n\n\n\n\nSimulation 8\n\n\n\n\n\n\n\nSimulation 9\n\n\n\n\n\n\n\nSimulation 10\n\n\n\n\n\n\n\nSimulation 11\n\n\n\n\n\n\n\nSimulation 12\n\n\n\n\n\n\n\nSimulation 13\n\n\n\n\n\n\n\nSimulation 14\n\n\n\n\n\n\n\nSimulation 15\n\n\n\n\n\n\n\nSimulation 16\n\n\n\n\n\n\n\nSimulation 17\n\n\n\n\n\n\n\nSimulation 18\n\n\n\n\n\n\n\nSimulation 19\n\n\n\n\n\n\n\nSimulation 20\n\n\n\n\n\n\n\nSimulation 21\n\n\n\n\n\n\n\nSimulation 22\n\n\n\n\n\n\n\nSimulation 23\n\n\n\n\n\n\n\nSimulation 24\n\n\n\n\n\n\n\nSimulation 25\n\n\n\n\n\n\n\nSimulation 26\n\n\n\n\n\n\n\nSimulation 27\n\n\n\n\n\n\n\nSimulation 28\n\n\n\n\n\n\n\nSimulation 29\n\n\n\n\n\n\n\nSimulation 30\n\n\n\n\n\n\n\nSimulation 31\n\n\n\n\n\n\n\nSimulation 32\n\n\n\n\n\n\n\nSimulation 33\n\n\n\n\n\n\n\nSimulation 34\n\n\n\n\n\n\n\nSimulation 35\n\n\n\n\n\n\n\nSimulation 36\n\n\n\n\n\n\n\nSimulation 37\n\n\n\n\n\n\n\nSimulation 38\n\n\n\n\n\n\n\nSimulation 39\n\n\n\n\n\n\n\nSimulation 40\n\n\n\n\n\n\n\nSimulation 41\n\n\n\n\n\n\n\nSimulation 42\n\n\n\n\n\n\n\nSimulation 43\n\n\n\n\n\n\n\nSimulation 44\n\n\n\n\n\n\n\nSimulation 45\n\n\n\n\n\n\n\nSimulation 46\n\n\n\n\n\n\n\nSimulation 47\n\n\n\n\n\n\n\nSimulation 48\n\n\n\n\n\n\n\nSimulation 49\n\n\n\n\n\n\n\nSimulation 50\n\n\n\n\n\n\n\nSimulation 51\n\n\n\n\n\n\n\nSimulation 52\n\n\n\n\n\n\n\nSimulation 53\n\n\n\n\n\n\n\nSimulation 54\n\n\n\n\n\n\n\nSimulation 55\n\n\n\n\n\n\n\nSimulation 56\n\n\n\n\n\n\n\nSimulation 57\n\n\n\n\n\n\n\nSimulation 58\n\n\n\n\n\n\n\nSimulation 59\n\n\n\n\n\n\n\nSimulation 60\n\n\n\n\n\n\n\nSimulation 61\n\n\n\n\n\n\n\nSimulation 62\n\n\n\n\n\n\n\nSimulation 63\n\n\n\n\n\n\n\nSimulation 64\n\n\n\n\n\n\n\nSimulation 65\n\n\n\n\n\n\n\nSimulation 66\n\n\n\n\n\n\n\nSimulation 67\n\n\n\n\n\n\n\nSimulation 68\n\n\n\n\n\n\n\nSimulation 69\n\n\n\n\n\n\n\nSimulation 70\n\n\n\n\n\n\n\nSimulation 71\n\n\n\n\n\n\n\nSimulation 72\n\n\n\n\n\n\n\nSimulation 73\n\n\n\n\n\n\n\nSimulation 74\n\n\n\n\n\n\n\nSimulation 75\n\n\n\n\n\n\n\nSimulation 76\n\n\n\n\n\n\n\nSimulation 77\n\n\n\n\n\n\n\nSimulation 78\n\n\n\n\n\n\n\nSimulation 79\n\n\n\n\n\n\n\nSimulation 80\n\n\n\n\n\n\n\nSimulation 81\n\n\n\n\n\n\n\nSimulation 82\n\n\n\n\n\n\n\nSimulation 83\n\n\n\n\n\n\n\nSimulation 84\n\n\n\n\n\n\n\nSimulation 85\n\n\n\n\n\n\n\nSimulation 86\n\n\n\n\n\n\n\nSimulation 87\n\n\n\n\n\n\n\nSimulation 88\n\n\n\n\n\n\n\nSimulation 89\n\n\n\n\n\n\n\nSimulation 90\n\n\n\n\n\n\n\nSimulation 91\n\n\n\n\n\n\n\nSimulation 92\n\n\n\n\n\n\n\nSimulation 93\n\n\n\n\n\n\n\nSimulation 94\n\n\n\n\n\n\n\nSimulation 95\n\n\n\n\n\n\n\nSimulation 96\n\n\n\n\n\n\n\nSimulation 97\n\n\n\n\n\n\n\nSimulation 98\n\n\n\n\n\n\n\nSimulation 99\n\n\n\n\n\n\n\nSimulation 100\n\n\n\n\n\n\n\n\n\n\n\n\nIf this were an actual study with a numeric outcome we could describe the variability of our average color within or between groups (bootstrap sampling) or we could calculate if our observed data is more extreme than the null/permutation distribution (permutation test).\nIndeed we can see that for all 100 bootstrap samples, there are various shades of blue for Group 1 and red/orange/yellow for Group 2. Whereas for all 100 permutation samples there is a range of colors from purpleish to blueish to orangeish to greenish…a random combination of our colors!"
  },
  {
    "objectID": "labs/lab6/index.html#null-scenario",
    "href": "labs/lab6/index.html#null-scenario",
    "title": "Week 6 Lab",
    "section": "Null Scenario",
    "text": "Null Scenario\nLet’s start with the null scenario, where both groups have the same variance. We will simulate from: [ Y_{P} (=10, =3), ; Y_{T} (=5, =) ] These parameters were chosen so that both sets of data will have a variance of 90 (i.e., the variance for the gamma distribution when parameterized with the shape (\\(k\\)) and scale (\\(\\theta\\)) is \\(k\\theta^2\\)).\n\n\nCode\nset.seed(612)\nplacebo &lt;- rgamma(n=50, shape=10, scale=3)\nvaccine &lt;- rgamma(n=50, shape=5, scale=sqrt(18))\n\nvar(placebo) # calculate the sample variances\n\n\n[1] 101.8625\n\n\nCode\nvar(vaccine) # calculate the sample variances\n\n\n[1] 93.42304\n\n\nCode\nobs_ratio &lt;- var(placebo)/var(vaccine) # calculate the ratio of the variances\nobs_ratio\n\n\n[1] 1.090336"
  },
  {
    "objectID": "labs/lab6/index.html#null-scenario-bootstrap",
    "href": "labs/lab6/index.html#null-scenario-bootstrap",
    "title": "Week 6 Lab",
    "section": "Null Scenario: Bootstrap",
    "text": "Null Scenario: Bootstrap\nLet’s conduct a bootstrap with 10,000 resamples of our ratio of the sample variances to describe the variability of this statistic:\n\n\nCode\nB &lt;- 10^4 #set number of bootstraps\nvar_ratio &lt;- numeric(B) #initialize vector to store results in\n\nnP &lt;- length(placebo) #sample size of placebo group\nnT &lt;- length(vaccine) #sample size of vaccine group\n\nset.seed(612) #set seed for reproducibility\n\nfor (i in 1:B){\n    placebo.boot &lt;- sample(placebo, nP, replace=T)\n    vaccine.boot &lt;- sample(vaccine, nT, replace=T)\n    var_ratio[i] &lt;- var(placebo.boot) / var(vaccine.boot)\n}\n\n\nLet’s now visualize the shape of our bootstrap distribution:\n\n\nCode\npar(mfrow=c(1,2)) #create plotting area for 2 figures in one row\n\nhist(var_ratio, main='Bootstrap Dist. of\\nVar(Placebo)/Var(Vaccine)', xlab='Placebo/Vaccine Variance Ratio')\nqqnorm(var_ratio); qqline(var_ratio)\n\n\n\n\n\nThe shapes of these plots suggest the ratio of variances is not normally distributed. Our histogram is right skewed and the normal Q-Q plot deviates from the diagonal line.\nLet’s then calculate the mean, SE, and bias of the bootstrap distribution:\n\n\nCode\nmean(var_ratio) # bootstrap mean ratio\n\n\n[1] 1.185681\n\n\nCode\nmean(var_ratio)-obs_ratio # bias for ratio\n\n\n[1] 0.09534477\n\n\nCode\nsd(var_ratio) # bootstrap SE\n\n\n[1] 0.4441747\n\n\nCode\n(mean(var_ratio)-obs_ratio) / sd(var_ratio) # estimate of accuracy\n\n\n[1] 0.214656\n\n\nThe most concerning aspect of this is the bias/SE estimate &gt; 0.10, suggesting our bootstrap percentile intervals may not be very accurate. However, let’s calculate the 95% bootstrap percentile interval as our “best” approach given the two options from our lecture (i.e., normal percentile or bootstrap percentile):\n\n\nCode\nquantile( var_ratio, c(0.025,0.975))\n\n\n     2.5%     97.5% \n0.5456348 2.2689698 \n\n\nFor the 95% bootstrap percentile CI, we are 95% confident that the true ratio of variances falls between 0.546 and 2.269. Additionally, because it is estimated from our data directly, 95% of the bootstrap estimates of ratios of variances fall in this interval.\nThe accuracy of our bootstrap percentile can be estimated by the ratio of the bias/SE, which we noted was 0.215. Since this exceeds +0.10 we may be concerned about the accuracy of this estimate.\nIt could also be noted that our 95% bootstrap percentile CI includes 1, so we would fail to reject the null hypothesis."
  },
  {
    "objectID": "labs/lab6/index.html#null-scenario-permutation-test",
    "href": "labs/lab6/index.html#null-scenario-permutation-test",
    "title": "Week 6 Lab",
    "section": "Null Scenario: Permutation Test",
    "text": "Null Scenario: Permutation Test\nPerhaps we are more interested in calculating a p-value to determine if the sample ratio differs from its underlying null distribution:\n\n\nCode\nB &lt;- 10^4 - 1 #set number of times to complete permutation sampling\nresult &lt;- numeric(B)\n\nnP &lt;- length(placebo)\n\nobs_ratio &lt;- var(placebo)/var(vaccine) # calculate the ratio of the variances\n\nset.seed(612) #set seed for reproducibility\npool_dat &lt;- c(placebo, vaccine)\n\nfor(j in 1:B){\n    index &lt;- sample(x=1:length(pool_dat), size=nP, replace=F)\n    placebo_permute &lt;- pool_dat[index]\n    vaccine_permute &lt;- pool_dat[-index]\n    result[j] &lt;- var(placebo_permute) / var(vaccine_permute)\n}\n\n# Histogram\nhist( result, xlab='Var(Placebo) / Var(Vaccine)', \n      main='Permutation Distribution for Ratio of Variances')\nabline( v=obs_ratio, lty=2, col='blue', lwd=2)\n\n\n\n\n\nAgain, we see a distribution that is right skewed, and our observed ratio of variances falls fairly close to our expected null of 1. To calculate a two-sided p-value we would take the larger of the proportion of our distribution that falls above our observed ratio or, in our context, the proportion that falls below 1/obs_ratio (the inverse of our observed ratio), and multiply it by 2:\n\n\nCode\n#note, we take the larger p-value and multiply by 2 (as compared to replacing &lt;= with &gt;)\n((sum(result &gt;= obs_ratio) + 1)/(B+1))\n\n\n[1] 0.3992\n\n\nCode\n((sum(result &lt;= (1/obs_ratio)) + 1)/(B+1))\n\n\n[1] 0.4009\n\n\nCode\n# Calculate permutation p-value for two-sided test\n2 * ((sum(result &lt;= (1/obs_ratio)) + 1)/(B+1)) \n\n\n[1] 0.8018\n\n\nHere we see that our two-sided p-value is 0.8018.\nA few important notes here:\n\nIt is helpful to plot the permutation distribution to note what direction (\\(\\leq\\) vs. \\(\\geq\\)) we need to use in our calculation.\nWe multiple the larger p-value by 2 to (1) account for the two-sided test and (2) to be more conservative (vs. using the smaller proportion).\nIf we wanted to calculate a one-sided p-value we would need to define that null and alternative hypothesis. For example, if our \\(H_0\\) is that the ratio of variances is larger than 1 (i.e., the placebo group has larger variance), we would specifically interpret our result as \\(p=0.3992\\). If the null hypothesis is that the ratio of variance is smaller than 1, based on our observed ratio we would have \\(1-0.3992=0.6008\\)."
  },
  {
    "objectID": "labs/lab6/index.html#alternative-scenario",
    "href": "labs/lab6/index.html#alternative-scenario",
    "title": "Week 6 Lab",
    "section": "Alternative Scenario",
    "text": "Alternative Scenario\nLet’s check an alternative scenario, where we will simulate from: [ Y_{P} (=10, =3), ; Y_{T} (=5, =3) ] These parameters were chosen so that the placebo has a true variance of 90 and the vaccine has a true variance of 45 (i.e., the variance for the gamma distribution when parameterized with the shape (\\(k\\)) and scale (\\(\\theta\\)) is \\(k\\theta^2\\)).\n\n\nCode\nset.seed(312)\nplacebo &lt;- rgamma(n=50, shape=10, scale=3)\nvaccine &lt;- rgamma(n=50, shape=5, scale=3)\n\nvar(placebo) # calculate the sample variances\n\n\n[1] 69.07464\n\n\nCode\nvar(vaccine) # calculate the sample variances\n\n\n[1] 31.92498\n\n\nCode\nobs_ratio &lt;- var(placebo)/var(vaccine) # calculate the ratio of the variances\nobs_ratio\n\n\n[1] 2.163655"
  },
  {
    "objectID": "labs/lab6/index.html#alternative-scenario-bootstrap",
    "href": "labs/lab6/index.html#alternative-scenario-bootstrap",
    "title": "Week 6 Lab",
    "section": "Alternative Scenario: Bootstrap",
    "text": "Alternative Scenario: Bootstrap\nLet’s conduct a bootstrap with 10,000 resamples of our ratio of the sample variances to describe the variability of this statistic:\n\n\nCode\nB &lt;- 10^4 #set number of bootstraps\nvar_ratio &lt;- numeric(B) #initialize vector to store results in\n\nnP &lt;- length(placebo) #sample size of placebo group\nnT &lt;- length(vaccine) #sample size of vaccine group\n\nset.seed(312) #set seed for reproducibility\n\nfor (i in 1:B){\n    placebo.boot &lt;- sample(placebo, nP, replace=T)\n    vaccine.boot &lt;- sample(vaccine, nT, replace=T)\n    var_ratio[i] &lt;- var(placebo.boot) / var(vaccine.boot)\n}\n\n\nLet’s now visualize the shape of our bootstrap distribution:\n\n\nCode\npar(mfrow=c(1,2)) #create plotting area for 2 figures in one row\n\nhist(var_ratio, main='Bootstrap Dist. of\\nVar(Placebo)/Var(Vaccine)', xlab='Placebo/Vaccine Variance Ratio')\nqqnorm(var_ratio); qqline(var_ratio)\n\n\n\n\n\nThe shapes of these plots suggest the ratio of variances is not normally distributed. Our histogram is right skewed and the normal Q-Q plot deviates from the diagonal line.\nLet’s then calculate the mean, SE, and bias of the bootstrap distribution:\n\n\nCode\nmean(var_ratio) # bootstrap mean ratio\n\n\n[1] 2.398181\n\n\nCode\nmean(var_ratio)-obs_ratio # bias for ratio\n\n\n[1] 0.2345258\n\n\nCode\nsd(var_ratio) # bootstrap SE\n\n\n[1] 0.9640094\n\n\nCode\n(mean(var_ratio)-obs_ratio) / sd(var_ratio) # estimate of accuracy\n\n\n[1] 0.2432816\n\n\nThe most concerning aspect of this is the bias/SE estimate &gt; 0.10, suggesting our bootstrap percentile intervals may not be very accurate. However, let’s calculate the 95% bootstrap percentile interval as our “best” approach given the two options from our lecture (i.e., normal percentile or bootstrap percentile):\n\n\nCode\nquantile( var_ratio, c(0.025,0.975))\n\n\n    2.5%    97.5% \n1.150953 4.786700 \n\n\nFor the 95% bootstrap percentile CI, we are 95% confident that the true ratio of variances falls between 1.151 and 4.787. Additionally, because it is estimated from our data directly, 95% of the bootstrap estimates of ratios of variances fall in this interval.\nThe accuracy of our bootstrap percentile can be estimated by the ratio of the bias/SE, which we noted was 0.243. Since this exceeds +0.10 we may be concerned about the accuracy of this estimate.\nIt could also be noted that our 95% bootstrap percentile CI excludes 1, so we may conclude that we would reject the null hypothesis, and conclude our ratio of sample variances are not equal. Further, given the ratio as placebo/vaccine, we could conclude that the placebo has greater variability."
  },
  {
    "objectID": "labs/lab6/index.html#alternative-scenario-permutation-test",
    "href": "labs/lab6/index.html#alternative-scenario-permutation-test",
    "title": "Week 6 Lab",
    "section": "Alternative Scenario: Permutation Test",
    "text": "Alternative Scenario: Permutation Test\nPerhaps we are more interested in calculating a p-value to determine if the sample ratio differs from its underlying null distribution:\n\n\nCode\nB &lt;- 10^4 - 1 #set number of times to complete permutation sampling\nresult &lt;- numeric(B)\n\nnP &lt;- length(placebo)\n\nobs_ratio &lt;- var(placebo)/var(vaccine) # calculate the ratio of the variances\n\nset.seed(312) #set seed for reproducibility\npool_dat &lt;- c(placebo, vaccine)\n\nfor(j in 1:B){\n    index &lt;- sample(x=1:length(pool_dat), size=nP, replace=F)\n    placebo_permute &lt;- pool_dat[index]\n    vaccine_permute &lt;- pool_dat[-index]\n    result[j] &lt;- var(placebo_permute) / var(vaccine_permute)\n}\n\n# Histogram\nhist( result, xlab='Var(Placebo) / Var(Vaccine)', \n      main='Permutation Distribution for Ratio of Variances')\nabline( v=obs_ratio, lty=2, col='blue', lwd=2)\n\n\n\n\n\nAgain, we see a distribution that is right skewed, and our observed ratio of variances falls fairly close to our expected null of 1. To calculate a two-sided p-value we would take the larger of the proportion of our distribution that falls above our observed ratio or, in our context, the proportion that falls below 1/obs_ratio (the inverse of our observed ratio), and multiply it by 2:\n\n\nCode\n#note, we take the larger p-value and multiply by 2 (as compared to replacing &lt;= with &gt;)\n((sum(result &gt;= obs_ratio) + 1)/(B+1))\n\n\n[1] 6e-04\n\n\nCode\n((sum(result &lt;= (1/obs_ratio)) + 1)/(B+1))\n\n\n[1] 8e-04\n\n\nCode\n# Calculate permutation p-value for two-sided test\n2 * ((sum(result &lt;= (1/obs_ratio)) + 1)/(B+1)) \n\n\n[1] 0.0016\n\n\nHere we see that our two-sided p-value is 0.0016, so we would reject the null hypothesis that the variances are equal for placebo and vaccine groups.\nA few important notes here:\n\nIt is helpful to plot the permutation distribution to note what direction (\\(\\leq\\) vs. \\(\\geq\\)) we need to use in our calculation.\nIf we wanted to calculate a one-sided p-value we would need to define that null and alternative hypothesis. For example, if our \\(H_0\\) is that the placebo has a larger variance than the vaccine group, we would conclude that the placebo group does appear to have a larger variance based on our one-sided \\(p=0.0006\\).\nIf the null hypothesis is that the vaccine has a larger variance than the placebo (i.e., the ratio is &lt;1), based on our observed ratio we would have \\(p=1-0.0006=0.9994\\), or we would fail to reject that null hypothesis. In other words, we cannot conclude that the vaccine group has a larger variance than the placebo."
  },
  {
    "objectID": "labs/lab6/index.html#wait-a-minute-what-is-the-distribution-of-the-ratio-of-variances",
    "href": "labs/lab6/index.html#wait-a-minute-what-is-the-distribution-of-the-ratio-of-variances",
    "title": "Week 6 Lab",
    "section": "Wait a Minute, What is the Distribution of the Ratio of Variances??",
    "text": "Wait a Minute, What is the Distribution of the Ratio of Variances??\nAhh, we almost snuck away without addressing the theoretical distribution! Generally speaking the ratio of variances will follow an \\(F_{n_1-1,n_2-1}\\) distribution under the null hypothesis that the variances are equal:\n\n\nCode\nset.seed(612)\nplacebo &lt;- rgamma(n=50, shape=10, scale=3)\nvaccine &lt;- rgamma(n=50, shape=5, scale=sqrt(18))\n\nB &lt;- 10^4 - 1 #set number of times to complete permutation sampling\nresult &lt;- numeric(B)\n\nnP &lt;- length(placebo)\n\nobs_ratio &lt;- var(placebo)/var(vaccine) # calculate the ratio of the variances\n\nset.seed(612) #set seed for reproducibility\npool_dat &lt;- c(placebo, vaccine)\n\nfor(j in 1:B){\n    index &lt;- sample(x=1:length(pool_dat), size=nP, replace=F)\n    placebo_permute &lt;- pool_dat[index]\n    vaccine_permute &lt;- pool_dat[-index]\n    result[j] &lt;- var(placebo_permute) / var(vaccine_permute)\n}\n\n# Histogram\nhist( result, xlab='Var(Placebo) / Var(Vaccine)', \n      main='Permutation Distribution for Ratio of Variances', freq=F, ylim=c(0,1.5))\ncurve(df(x,df1=49,df2=49),add=T, lwd=2, col='orangered2')"
  },
  {
    "objectID": "labs/lab9/index.html",
    "href": "labs/lab9/index.html",
    "title": "Week 9 Lab",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Labs collection, the mini-lecture content delivered at the start of class before breaking out into small groups to work on the homework assignment."
  },
  {
    "objectID": "labs/lab9/index.html#simple-linear-regression-example",
    "href": "labs/lab9/index.html#simple-linear-regression-example",
    "title": "Week 9 Lab",
    "section": "Simple Linear Regression Example",
    "text": "Simple Linear Regression Example\nWe will start with an example for SLR where we violate the assumption of homoscedasticity:\n\n\nCode\n# Simulate some data with increasing variance for regression models\nset.seed(1102)\nx1 &lt;- abs(rnorm(n=100, mean=50, sd=20))\n# simulate our error to have increasing variance based on increasing values of x1\nerror &lt;- rnorm(n=100, mean=0, sd=x1) \ny &lt;- 25 + 2*x1 + error\nmod1 &lt;- glm(y ~ x1)\n\n\n\n\nCode\npar(mfrow=c(2,3), mar=c(4.1,4.1,3.1,2.1))\n\n## Scatterplot of Y-X\nplot(x=x1, y=y, xlab='X', ylab='Y', \n     main='Scatterplot', cex=1); abline( mod1 )\n\n## Scatterplot of residuals by X\nplot(x=x1, y=rstudent(mod1), xlab='X', ylab='Jackknife Residual', \n     main='Residual Plot by X', cex=1); abline(h=0, lty=2, col='gray65')\n\n## Scatterplot of residuals by predicted values\nplot(x=predict(mod1), y=rstudent(mod1), xlab=expression(hat(Y)), ylab='Jackknife Residual', \n     main='Residual Plot by Y-hat', cex=1); abline(h=0, lty=2, col='gray65')\n\n## Histogram of jackknife residuals with normal curve\nhist(rstudent(mod1), xlab='Jackknife Residual', \n     main='Histogram of Residuals', freq=F, breaks=seq(-5,5,0.25)); \n  curve( dnorm(x,mean=0,sd=1), lwd=2, col='blue', add=T)\n\n## PP-plot\nplot( ppoints(length(rstudent(mod1))), sort(pnorm(rstudent(mod1))), \n      xlab='Observed Cumulative Probability', \n      ylab='Expected Cumulative Probability', \n      main='Normal Probability Plot', cex=1); \n  abline(a=0,b=1, col='gray65', lwd=1)\n\n## QQ-plot\nqqnorm( rstudent(mod1) ); qqline( rstudent(mod1) )\n\n\n\n\n\nIf you want to create similar figures in SAS, you can use the code (hidden) below (or the default PROC REG output provides many of these figures as well):\n\n\nCode\n/* Y-X scatterplot with LINEAR regression line */\nPROC GPLOT DATA=amniotic;\n    PLOT lncells*temp;\n    SYMBOL INTERPOL=rl VALUE=dot COLOR=black;\nRUN;\n/* -OR- */\nPROC SGPLOT DATA=amniotic;\n    REG Y=lncells X=temp;\nRUN;\n\n/* Jackknife Residual Plot versus Predictor, versus Predicted */\nPROC GPLOT DATA=resids3;\n    PLOT jackknife*(temp pred);\n    SYMBOL VALUE=dot INTERPOL=rl COLOR=black;\nRUN;\n/* -OR- */\nPROC SGPLOT DATA=resids3;\n    REG Y=jackknife X=temp;\nRUN;\n\nPROC SGPLOT DATA=resids3;\n    REG Y=jackknife X=pred;\nRUN;\n\n/* Histogram of Jackknife Residuals */\nPROC GCHART DATA=resids3;\n    VBAR jackknife;\nRUN;\n/* -OR- */\nPROC SGPLOT DATA=resids3;\n    histogram jackknife;\n    density jackknife;\nRUN;\n\n/* Normal Probability Plot of Jackknife Residuals */\nPROC UNIVARIATE NORMAL PLOT DATA=resids3;\n    VAR jackknife;\nRUN;"
  },
  {
    "objectID": "labs/lab9/index.html#multiple-linear-regression-example",
    "href": "labs/lab9/index.html#multiple-linear-regression-example",
    "title": "Week 9 Lab",
    "section": "Multiple Linear Regression Example",
    "text": "Multiple Linear Regression Example\nLet’s simulate some data for a multiple linear regression, but exclude a predictor (i.e., we omitted it from the model because we either don’t have it or we didn’t think to include it initially):\n\n\nCode\n# Simulate some data with increasing variance for regression models\nset.seed(1102)\nx1 &lt;- rnorm(n=100, mean=50, sd=20)\nx2 &lt;- rbinom(n=100, size=1, prob=0.3)\nerror &lt;- rnorm(n=100, mean=0, sd=4) \ny &lt;- 25 + 2*x1 + 30*x2 + error\nmod2 &lt;- glm(y ~ x1)\nmod3 &lt;- glm(y ~ x1 + x2)\n\n\nLet’s start with the SLR model where we only fit y ~ x1:\n\n\nCode\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))\n\n## X1 Partial Plot SLR with X1 only\nx1_slr_step1 &lt;- glm(y ~ 1)\nx1_slr_step2 &lt;- glm(x1 ~ 1)\nplot(x=residuals(x1_slr_step2), y=residuals(x1_slr_step1),\n     main=expression('(SLR) Partial Plot for X'[1]), ylab='Partial Dependent Residual', \n     xlab='Partial Regressor Residual')\nabline(lm(residuals(x1_slr_step1) ~ residuals(x1_slr_step2)))\n\n## Scatterplot of residuals by predicted values\nplot(x=predict(mod2), y=rstudent(mod2), xlab=expression(hat(Y)), ylab='Jackknife Residual', \n     main='Residual Plot by Y-hat', cex=1); abline(h=0, lty=2, col='gray65')\n\n## Histogram of jackknife residuals with normal curve\nhist(rstudent(mod2), xlab='Jackknife Residual', \n     main='Histogram of Residuals', freq=F, breaks=seq(-5,5,0.25)); \n  curve( dnorm(x,mean=0,sd=1), lwd=2, col='blue', add=T)\n\n## PP-plot\nplot( ppoints(length(rstudent(mod2))), sort(pnorm(rstudent(mod2))), \n      xlab='Observed Cumulative Probability', \n      ylab='Expected Cumulative Probability', \n      main='Normal Probability Plot', cex=1); \n  abline(a=0,b=1, col='gray65', lwd=1)\n\n\n\n\n\nNow let’s see what happens if we look at the diagnostic plots for y ~ x1 + x2 (while keeping the partial plot for x1 from the SLR for comparison):\n\n\nCode\npar(mfrow=c(2,3), mar=c(4.1,4.1,3.1,2.1))\n\n## X1 Partial Plot SLR with X1 only\nx1_slr_step1 &lt;- glm(y ~ 1)\nx1_slr_step2 &lt;- glm(x1 ~ 1)\nplot(x=residuals(x1_slr_step2), y=residuals(x1_slr_step1),\n     main=expression('(SLR) Partial Plot for X'[1]), ylab='Partial Dependent Residual', \n     xlab='Partial Regressor Residual')\nabline(lm(residuals(x1_slr_step1) ~ residuals(x1_slr_step2)))\n\n\n## X1 Partial Plot MLR with X1+X2\nx1_mlr_step1 &lt;- glm(y ~ x2)\nx1_mlr_step2 &lt;- glm(x1 ~ x2)\nplot(x=residuals(x1_mlr_step2), y=residuals(x1_mlr_step1),\n     main=expression('Partial Plot for X'[1]), ylab='Partial Dependent Residual', \n     xlab='Partial Regressor Residual')\nabline(lm(residuals(x1_mlr_step1) ~ residuals(x1_mlr_step2)))\n\n\n## X2 Partial Plot MLR\nx2_mlr_step1 &lt;- glm(y ~ x1)\nx2_mlr_step2 &lt;- glm(x2 ~ x1)\nplot(x=residuals(x2_mlr_step2), y=residuals(x2_mlr_step1),\n     main=expression('Partial Plot for X'[2]), ylab='Partial Dependent Residual', \n     xlab='Partial Regressor Residual')\nabline(lm(residuals(x2_mlr_step1) ~ residuals(x2_mlr_step2)))\n\n\n## Scatterplot of residuals by predicted values\nplot(x=predict(mod3), y=rstudent(mod3), xlab=expression(hat(Y)), ylab='Jackknife Residual', \n     main='Residual Plot by Y-hat', cex=1); abline(h=0, lty=2, col='gray65')\n\n## Histogram of jackknife residuals with normal curve\nhist(rstudent(mod3), xlab='Jackknife Residual', \n     main='Histogram of Residuals', freq=F, breaks=seq(-5,5,0.25)); \n  curve( dnorm(x,mean=0,sd=1), lwd=2, col='blue', add=T)\n\n## PP-plot\nplot( ppoints(length(rstudent(mod3))), sort(pnorm(rstudent(mod3))), \n      xlab='Observed Cumulative Probability', \n      ylab='Expected Cumulative Probability', \n      main='Normal Probability Plot', cex=1); \n  abline(a=0,b=1, col='gray65', lwd=1)\n\n\n\n\n\nIn SAS we can request the partial plots by simply adding / PARTIAL to our MODEL statement:\n\n\nCode\nPROC REG DATA=dat;\n  MODEL y = x1 x2 / PARTIAL;\nRUN;"
  },
  {
    "objectID": "labs/prac10s/index.html",
    "href": "labs/prac10s/index.html",
    "title": "Week 10 Practice Problems: Solutions",
    "section": "",
    "text": "This page includes the solutions to the optional practice problems for the given week. If you want to see a version without solutions please click here. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises focus on ANOVA and categorical variables."
  },
  {
    "objectID": "labs/prac10s/index.html#exercise-1-one-way-anova",
    "href": "labs/prac10s/index.html#exercise-1-one-way-anova",
    "title": "Week 10 Practice Problems: Solutions",
    "section": "Exercise 1: One-Way ANOVA",
    "text": "Exercise 1: One-Way ANOVA\nFor this exercise, we will compare age as our outcome against surgery size with a one-way ANOVA.\n\n1a: Testing Homogeneity of the Variances Assumption\nUse both Levene’s test and Bartlett’s test to evaluate if the variances are homogeneous (i.e., equal) across our three surgery size groups. Write the null and alternative hypothesis being tested.\nSolution:\nWe are testing\n\\[ H_{0}\\colon \\sigma^2_{small} = \\sigma^2_{medium} = \\sigma^2_{large} \\text{ versus } H_{1}\\colon \\text{at least one variance is different}  \\]\nBased on Levene’s and Bartlett’s test we have\n\n\nCode\nlibrary(car)\n\nleveneTest( preOp_age ~ as.factor(intraOp_surgerySize), data=dat )\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup   2  7.9072 0.0004763 ***\n      232                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nbartlett.test( preOp_age ~ as.factor(intraOp_surgerySize), data=dat )\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  preOp_age by as.factor(intraOp_surgerySize)\nBartlett's K-squared = 13.887, df = 2, p-value = 0.0009649\n\n\nFor both tests, \\(p&lt;0.05\\). Therefore, we reject our null hypothesis that the variances are equal within each surgery size group for our outcome of age.\n\n\n1b: One-Way ANOVA with Equal Variances\nAssume that the variances are equal across groups and test the hypothesis that the mean age between groups is equal across the three surgery size groups. State the null and alternative hypothesis being tested.\nSolution:\nThe hypothesis we are testing is\n\\[ H_0\\colon \\mu_{small} = \\mu_{medium} = \\mu_{large} \\text{ versus } H_1\\colon \\text{ at least one mean is different} \\]\nWe can fit our one-way ANOVA assuming equal variances with oneway.test:\n\n\nCode\noneway.test(preOp_age ~ as.factor(intraOp_surgerySize), data=dat, var.equal=T)\n\n\n\n    One-way analysis of means\n\ndata:  preOp_age and as.factor(intraOp_surgerySize)\nF = 2.137, num df = 2, denom df = 232, p-value = 0.1203\n\n\nSince \\(p=0.1203&gt;0.05\\), we fail to reject our null hypothesis and cannot conclude that at least one mean age is different.\n\n\n1c: One-Way ANOVA with Unequal Variances\nAssume that the variances are unequal across groups and test the hypothesis that the mean age between groups is equal across the three surgery size groups. State the null and alternative hypothesis being tested.\nSolution:\nWe have the same hypothesis being tested in 1c even though we are not assuming equal variances:\n\\[ H_0\\colon \\mu_{small} = \\mu_{medium} = \\mu_{large} \\text{ versus } H_1\\colon \\text{ at least one mean is different} \\]\nWe can fit our one-way ANOVA allowing for unequal variances with oneway.test:\n\n\nCode\noneway.test(preOp_age ~ as.factor(intraOp_surgerySize), data=dat, var.equal=F)\n\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  preOp_age and as.factor(intraOp_surgerySize)\nF = 2.0796, num df = 2.000, denom df = 55.683, p-value = 0.1346\n\n\nSince \\(p=0.1346&gt;0.05\\), we fail to reject our null hypothesis and cannot conclude that at least one mean age is different.\n\n\n1d: Nonparametric Kruskal-Wallis Test\nAssume that we think our normality assumption for one-way ANOVA is violated. Implement the nonparametric Kruskal-Wallis test and interpret our result. State the null and alternative hypothesis being tested.\nSolution:\nEven though some call the Kruskal-Wallis test a nonparametric ANOVA, it does not test the same hypothesis as the one-way ANOVA. Instead it tests:\n\n\\(H_0\\): the mean ranks of the groups are the same\n\\(H_1\\): at least one group has a different mean rank\n\nWe can use kruskal.test to implement this test:\n\n\nCode\nkruskal.test(preOp_age ~ as.factor(intraOp_surgerySize), data=dat)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  preOp_age by as.factor(intraOp_surgerySize)\nKruskal-Wallis chi-squared = 2.0355, df = 2, p-value = 0.3614\n\n\nSince \\(p=0.3614 &gt; 0.05\\), we fail to reject our null hypothesis and cannot conclude that at least one group’s mean rank for age is different.\n\n\n1e: Post-Hoc Testing\nRegardless of our earlier results in 1b, compare the means of each pair of groups with the Tukey HSD method and summarize the results. Was post-hoc testing necessary in this case?\nSolution:\nThere are various functions we could use to implement Tukey’s HSD. Here we will use the default TukeyHSD function after fitting an aov ANOVA model:\n\n\nCode\naov1 &lt;- aov(preOp_age ~ as.factor(intraOp_surgerySize), data=dat)\nTukeyHSD(aov1)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = preOp_age ~ as.factor(intraOp_surgerySize), data = dat)\n\n$`as.factor(intraOp_surgerySize)`\n        diff       lwr       upr     p adj\n2-1 4.161166 -1.483245  9.805578 0.1929435\n3-1 7.005952 -2.281060 16.292965 0.1787406\n3-2 2.844786 -5.585118 11.274690 0.7058412\n\n\nOur Tukey’s HSD post-hoc test result does not indicate any significant pairwise comparisons based on the adjusted p-values (all \\(p&gt;0.05\\)). This is not necessarily surprising since our overall test in 1b did not reject the null hypothesis that at least one group’s mean was different."
  },
  {
    "objectID": "labs/prac10s/index.html#exercise-2-categorical-variables",
    "href": "labs/prac10s/index.html#exercise-2-categorical-variables",
    "title": "Week 10 Practice Problems: Solutions",
    "section": "Exercise 2: Categorical Variables",
    "text": "Exercise 2: Categorical Variables\nFor this exercise, we will use age our continuous outcome and consider both ASA status and surgery size as predictors.\n\n2a: Reference Cell Model\nThe reference cell model is our more common approach to regression modeling in many biostatistics applications. Write down the true regression equation and any assumptions for a model where ASA status 1 and surgery size small are the reference categories.\nSolution:\nOur true regression equation is\n\\[ Y = \\beta_0 + \\beta_1 I_{G=2} + \\beta_2 I_{G=3} + \\beta_3 I_{A=2} + \\beta_4 I_{A=3} + \\epsilon \\]\nwhere \\(I_{G=g}\\) and \\(I_{A=a}\\) represent indicators for surgery group (medium for \\(G=2\\) and large for \\(G=3\\)) and ASA status (ASA II for \\(A_2\\) and ASA III for \\(A_3\\)), and \\(\\epsilon \\sim N(0,\\sigma^{2}_{Y|X})\\).\n\n\n2b: Partial \\(F\\)-test for ASA Status\nEvaluate if ASA status contributes significantly to the model from 2a. Write the null and alternative hypotheses, test the null hypothesis, and state your conclusion.\nSolution:\nBased on our specified model in 2a, our hypothesis is\n\\[ H_0\\colon \\beta_3 = \\beta_4 = 0 \\text{ versus } H_1\\colon \\text{ at least one coefficient is not 0} \\]\nWe can fit a full and reduced model to use in evaluating this hypothesis with a partial \\(F\\)-test:\n\n\nCode\n# Fit full model and summarize output for our own information\nmod_full &lt;- glm(preOp_age ~ as.factor(intraOp_surgerySize) + as.factor(preOp_asa), data=dat)\nsummary(mod_full)\n\n\n\nCall:\nglm(formula = preOp_age ~ as.factor(intraOp_surgerySize) + as.factor(preOp_asa), \n    data = dat)\n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       40.788      2.563  15.917  &lt; 2e-16 ***\nas.factor(intraOp_surgerySize)2    2.187      2.174   1.006    0.315    \nas.factor(intraOp_surgerySize)3    3.025      3.608   0.839    0.403    \nas.factor(preOp_asa)2             17.939      2.498   7.180 9.57e-12 ***\nas.factor(preOp_asa)3             18.067      2.859   6.319 1.35e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 192.4575)\n\n    Null deviance: 55935  on 234  degrees of freedom\nResidual deviance: 44265  on 230  degrees of freedom\nAIC: 1909.9\n\nNumber of Fisher Scoring iterations: 2\n\n\n\n\nCode\n# Fit reduced model and summarize output for our own information\nmod_red_b &lt;- glm(preOp_age ~ as.factor(intraOp_surgerySize), data=dat)\nsummary(mod_red_b)\n\n\n\nCall:\nglm(formula = preOp_age ~ as.factor(intraOp_surgerySize), data = dat)\n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       53.946      2.056  26.238   &lt;2e-16 ***\nas.factor(intraOp_surgerySize)2    4.161      2.393   1.739   0.0834 .  \nas.factor(intraOp_surgerySize)3    7.006      3.937   1.779   0.0765 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 236.7369)\n\n    Null deviance: 55935  on 234  degrees of freedom\nResidual deviance: 54923  on 232  degrees of freedom\nAIC: 1956.6\n\nNumber of Fisher Scoring iterations: 2\n\n\n\n\nCode\n# Partial F-test\nanova(mod_full, mod_red_b, test='F')\n\n\nAnalysis of Deviance Table\n\nModel 1: preOp_age ~ as.factor(intraOp_surgerySize) + as.factor(preOp_asa)\nModel 2: preOp_age ~ as.factor(intraOp_surgerySize)\n  Resid. Df Resid. Dev Df Deviance      F    Pr(&gt;F)    \n1       230      44265                                 \n2       232      54923 -2   -10658 27.689 1.681e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince \\(p&lt;0.05\\), we reject the null hypothesis and conclude that at least one beta coefficient for ASA status is not equal to 0. This indicates that the addition of our three category ASA variable is meaningful in predicting age above and beyond just surgery size alone.\n\n\n2c: Partial \\(F\\)-test for Surgery Size\nEvaluate if surgery size contributes significantly to the model from 2a. Write the null and alternative hypotheses, test the null hypothesis, and state your conclusion.\nSolution:\nBased on our specified model in 2a, our hypothesis is\n\\[ H_0\\colon \\beta_1 = \\beta_2 = 0 \\text{ versus } H_1\\colon \\text{ at least one coefficient is not 0} \\]\nWe can fit a new reduced model to use in evaluating this hypothesis with a partial \\(F\\)-test:\n\n\nCode\n# Fit reduced model and summarize output for our own information\nmod_red_c &lt;- glm(preOp_age ~ as.factor(preOp_asa), data=dat)\nsummary(mod_red_c)\n\n\n\nCall:\nglm(formula = preOp_age ~ as.factor(preOp_asa), data = dat)\n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             42.195      2.163  19.509  &lt; 2e-16 ***\nas.factor(preOp_asa)2   18.297      2.472   7.403 2.44e-12 ***\nas.factor(preOp_asa)3   18.572      2.806   6.618 2.49e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 191.7873)\n\n    Null deviance: 55935  on 234  degrees of freedom\nResidual deviance: 44495  on 232  degrees of freedom\nAIC: 1907.1\n\nNumber of Fisher Scoring iterations: 2\n\n\n\n\nCode\n# Partial F-test\nanova(mod_full, mod_red_c, test='F')\n\n\nAnalysis of Deviance Table\n\nModel 1: preOp_age ~ as.factor(intraOp_surgerySize) + as.factor(preOp_asa)\nModel 2: preOp_age ~ as.factor(preOp_asa)\n  Resid. Df Resid. Dev Df Deviance      F Pr(&gt;F)\n1       230      44265                          \n2       232      44495 -2  -229.45 0.5961 0.5518\n\n\nSince \\(p=0.5518 &gt; 0.05\\), we fail to reject the null hypothesis that at least one beta coefficient for surgery size is not equal to 0. This indicates that the addition of our three category surgery size variable is not meaningful in predicting age above and beyond just ASA status alone.\n\n\n2d: Overall \\(F\\)-test\nEvaluate if ASA status and surgery size contribute significantly to the prediction of \\(Y\\). Write the null and alternative hypotheses, test the null hypothesis, and state your conclusion.\nSolution:\nBased on our specified model in 2a, our hypothesis is\n\\[ H_0\\colon \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = 0 \\text{ versus } H_1\\colon \\text{ at least one coefficient is not 0} \\]\nWe can fit a new reduced model with only any intercept to use in evaluating this hypothesis with an \\(F\\)-test:\n\n\nCode\nmod_int &lt;- glm(preOp_age ~ 1, data=dat)\nanova(mod_full, mod_int, test='F')\n\n\nAnalysis of Deviance Table\n\nModel 1: preOp_age ~ as.factor(intraOp_surgerySize) + as.factor(preOp_asa)\nModel 2: preOp_age ~ 1\n  Resid. Df Resid. Dev Df Deviance      F    Pr(&gt;F)    \n1       230      44265                                 \n2       234      55935 -4   -11670 15.159 5.147e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince \\(p&lt;0.05\\), we reject our null hypothesis and conclude that at least one beta coefficient is significantly different from 0. In other words, while the full model might not be the “best” model, it is better than just using the mean of age alone.\n\n\n2e: ASA Status III Significance\nEvaluate the significance of an ASA status of III and provide an interpretation for the beta coefficient.\nSolution:\nFrom our full regression output in 2b we see that as.factor(preOp_asa)3 has an estimated beta coefficient of 18.067 with \\(p&lt;0.05\\). Our beta coefficient indicates that those with ASA III status are older by 18.067 years on average than those with ASA I (our reference category). And since \\(p&lt;0.05\\), this is a significantly higher age.\n\n\n2f: ASA Status III Removal\nRegardless of your conclusion in 2e, if \\(p&gt;0.05\\) and we failed to reject the null hypothesis that our beta coefficient was equal to 0, should we consider removing just ASA III from our regression model and refitting (i.e., still have ASA II in the model)?\nSolution:\nNo, it would not make sense to exclude a single indicator variable/category from the model. Currently, the reference category is ASA I and the two indicator variables represent ASA II and ASA III. If we drop the indicator for ASA III in the model, this implies that everyone with \\(I_{A=3}=1\\) joins the reference category, which becomes a combination of ASA I and III. This likely does not make clinical or biological sense, so even if ASA III was not statistically significant, we would still want to include it as its own category (or exclude the entire ASA variable from the model)."
  },
  {
    "objectID": "labs/prac11s/index.html",
    "href": "labs/prac11s/index.html",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "",
    "text": "This page includes the solutions to the optional practice problems for the given week. If you want to see a version without solutions please click here. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises focus on the wonderfully wide world of applications for MLR: confounding, mediation, interactions, general linear hypothesis testing, and polynomial regression."
  },
  {
    "objectID": "labs/prac11s/index.html#a-unadjusted-model-and-interpretation",
    "href": "labs/prac11s/index.html#a-unadjusted-model-and-interpretation",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "1a: Unadjusted Model and Interpretation",
    "text": "1a: Unadjusted Model and Interpretation\nWhat is the unadjusted (crude) estimate for the association between RSI and day of the week? Write a brief, but complete, summary of the relationship between RSI and day of the week. Hint: you will need to create a new variable for day of the week.\nSolution:\n\n\nCode\ndat$AK_mt &lt;- dat$dow %in% c(1,2)\nmod1u &lt;- lm(complication_rsi ~ AK_mt, data=dat)\nsummary(mod1u)\n\n\n\nCall:\nlm(formula = complication_rsi ~ AK_mt, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3843 -0.4943  0.0757  0.4333 13.8033 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.335689   0.008956  -37.48   &lt;2e-16 ***\nAK_mtTRUE   -0.167634   0.013534  -12.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.201 on 31999 degrees of freedom\nMultiple R-squared:  0.004772,  Adjusted R-squared:  0.004741 \nF-statistic: 153.4 on 1 and 31999 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nconfint(mod1u)\n\n\n                 2.5 %     97.5 %\n(Intercept) -0.3532430 -0.3181357\nAK_mtTRUE   -0.1941607 -0.1411073\n\n\nThere is a significant relationship between RSI for in-hospital complications and day of week (M/T vs. W/Th/F) (p&lt;0.001). On average, RSI is 0.17 lower (95% CI: 0.14 to 0.19 lower) on M/T compared to W/Th/F surgeries."
  },
  {
    "objectID": "labs/prac11s/index.html#b-adjusted-model-and-interpretation",
    "href": "labs/prac11s/index.html#b-adjusted-model-and-interpretation",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "1b: Adjusted Model and Interpretation",
    "text": "1b: Adjusted Model and Interpretation\nAdjusting for the effect of moon phase, what is the adjusted estimate for the association between RSI and day of the week? Write a brief, but complete, summary of the relationship between RSI and day of the week adjusting for moon phase.\nSolution:\n\n\nCode\nmod1a &lt;- lm(complication_rsi ~ AK_mt + as.factor(moonphase), data=dat)\nsummary(mod1a)\n\n\n\nCall:\nlm(formula = complication_rsi ~ AK_mt + as.factor(moonphase), \n    data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3828 -0.4928  0.0772  0.4278 13.8247 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -0.32039    0.01490 -21.507   &lt;2e-16 ***\nAK_mtTRUE             -0.16740    0.01354 -12.369   &lt;2e-16 ***\nas.factor(moonphase)2 -0.03689    0.01911  -1.930   0.0536 .  \nas.factor(moonphase)3 -0.00707    0.01914  -0.369   0.7118    \nas.factor(moonphase)4 -0.01682    0.01909  -0.881   0.3782    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.201 on 31996 degrees of freedom\nMultiple R-squared:  0.004904,  Adjusted R-squared:  0.00478 \nF-statistic: 39.42 on 4 and 31996 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nconfint(mod1a)\n\n\n                            2.5 %        97.5 %\n(Intercept)           -0.34959437 -0.2911952782\nAK_mtTRUE             -0.19393341 -0.1408770162\nas.factor(moonphase)2 -0.07434743  0.0005763537\nas.factor(moonphase)3 -0.04458653  0.0304459049\nas.factor(moonphase)4 -0.05423393  0.0205935279\n\n\nThere is a significant relationship between RSI for in-hospital complications and day of week (M/T vs. W/Th/F) (p&lt;0.001). After accounting for moon phase, on average, RSI is 0.17 lower (95% CI: 0.14 to 0.19 lower) on M/T compared to W/Th/F surgeries."
  },
  {
    "objectID": "labs/prac11s/index.html#c-moon-phase-confounding",
    "href": "labs/prac11s/index.html#c-moon-phase-confounding",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "1c: Moon Phase Confounding",
    "text": "1c: Moon Phase Confounding\nIs moon phase a confounder of the association between RSI and day of the week based on the operational criterion? Should you report the results from (A) or (B)? Justify your answer.\nSolution:\nMoon phase is not a confounder by the operational definition, since it does not greatly change our point estimate of M/T vs. W/Th/F:\n\\[ \\frac{|0.167634 - 0.16740|}{0.167634} \\times 100 = 0.1396% \\]\nWhile there is no rigid threshold for a change that is “big” enough, this is rather small and &lt;10% or &lt;20% are commonly used to rule out potential confounding with the operational definition."
  },
  {
    "objectID": "labs/prac11s/index.html#a-mediation-dag",
    "href": "labs/prac11s/index.html#a-mediation-dag",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "2a: Mediation DAG",
    "text": "2a: Mediation DAG\nFit the three fundamental models of mediation analysis and fill in the following DAG:\n\n\n\n\n\n\n\n\n\nSolution:\n\n\nCode\n# Fit our three models\nmod2_crude &lt;- lm(complication_rsi ~ AK_mt, data=dat)\nmod2_adjusted &lt;- lm(complication_rsi ~ AK_mt + bmi, data=dat)\nmod2_covariate &lt;- lm(bmi ~ AK_mt, data=dat)\n\n# Extract coefficients\ncoef(mod2_crude)\n\n\n(Intercept)   AK_mtTRUE \n -0.3356893  -0.1676340 \n\n\nCode\ncoef(mod2_adjusted)\n\n\n(Intercept)   AK_mtTRUE         bmi \n 0.57313211 -0.15765353 -0.03148069 \n\n\nCode\ncoef(mod2_covariate)\n\n\n(Intercept)   AK_mtTRUE \n  29.290095    0.367003"
  },
  {
    "objectID": "labs/prac11s/index.html#b-percent-mediated",
    "href": "labs/prac11s/index.html#b-percent-mediated",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "2b: Percent Mediated",
    "text": "2b: Percent Mediated\nWhat is the proportion/percent mediated by age?\nSolution:\nThe proportion mediated is \\(\\frac{\\text{indirect effect}}{\\text{total effect}} = \\frac{\\hat{\\beta}_{crude} - \\hat{\\beta}_{adj}}{\\hat{\\beta}_{crude}} = \\frac{-0.1676 - (-0.1577)}{-0.1676} = 0.0591\\). This corresponds to the percent mediated of \\(0.0591 \\times 100 = 5.91\\%\\)."
  },
  {
    "objectID": "labs/prac11s/index.html#c-95-ci-for-percent-mediated",
    "href": "labs/prac11s/index.html#c-95-ci-for-percent-mediated",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "2c: 95% CI for Percent Mediated",
    "text": "2c: 95% CI for Percent Mediated\nWhat is the 95% CI and corresponding p-value for the proportion/percent mediated by age using the normal approximation to estimate the standard error (i.e., Sobel’s test)?\nSolution:\nWe first need to calculate the standard error for our indirect effect:\n\\[ SE(\\hat{\\beta}_{crude} - \\hat{\\beta}_{adj}) = \\sqrt{{\\hat{\\gamma}}_X^2\\left(SE\\left({\\hat{\\beta}}_M\\right)\\right)^2+{\\hat{\\beta}}_M^2\\left(SE\\left({\\hat{\\gamma}}_X\\right)\\right)^2}  \\]\nWe can find this information in our coefficient tables:\n\n\nCode\nsummary(mod2_adjusted)$coefficient\n\n\n               Estimate   Std. Error   t value      Pr(&gt;|t|)\n(Intercept)  0.57313211 0.0290929009  19.70007  7.953691e-86\nAK_mtTRUE   -0.15765353 0.0138026533 -11.42197  3.773477e-30\nbmi         -0.03148069 0.0009427987 -33.39068 7.305638e-240\n\n\nCode\nsummary(mod2_covariate)$coefficient\n\n\n             Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) 29.290095 0.05731452 511.041478 0.000000e+00\nAK_mtTRUE    0.367003 0.08637699   4.248851 2.155467e-05\n\n\n\\[ SE(\\hat{\\beta}_{crude} - \\hat{\\beta}_{adj}) = \\sqrt{(0.3670)^2(0.0009)^2 + (-0.0315)^2 (0.0864)^2} = 0.0027 \\]\nOur 95% CI for the indirect effect is\n\\[ (-0.1676 - (-0.1577)) \\pm 1.96 \\times 0.0027 = (-0.015, -0.005)  \\]\nThis corresponds to a 95% CI for our proportion mediated of:\n\\[ \\left(\\frac{-0.005}{-0.1676}, \\frac{-0.015}{-0.1676} \\right) = (0.0298, 0.0895)) \\]\nWe are 95% confident that the proportion mediated by BMI is betwee 2.98% and 8.95%.\nOur p-value is calculated from referencing the standard normal distribution (per Sobel’s test):\n\\[ Z = \\frac{\\text{indirect effect}}{SE(\\hat{\\beta}_{crude} - \\hat{\\beta}_{adj})} = \\frac{-0.0099}{0.0027} = -3.67 \\]\nThis corresponds to a p-value of 2 * pnorm(-3.67)=2.4^{-4}, which is less than 0.05.\nNote, that although this proportion mediated is not very large, our larger sample size may be powered enough to detect even weak mediation relationships."
  },
  {
    "objectID": "labs/prac11s/index.html#a-fitted-regression-equation-with-interaction",
    "href": "labs/prac11s/index.html#a-fitted-regression-equation-with-interaction",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "3a: Fitted Regression Equation with Interaction",
    "text": "3a: Fitted Regression Equation with Interaction\nWrite down the fitted regression equation for the regression of RSI on BMI, diabetes, and the interaction between the two. Provide an interpretation for each of the coefficients in the model (including the intercept).\nSolution:\n\n\nCode\nmod3 &lt;- lm( complication_rsi ~ bmi + baseline_diabetes + bmi*baseline_diabetes, data=dat)\n\ncbind(summary(mod3)$coef, confint(mod3))\n\n\n                         Estimate  Std. Error    t value      Pr(&gt;|t|)\n(Intercept)            0.53713273 0.031985132  16.793200  5.472708e-63\nbmi                   -0.03156706 0.001080522 -29.214640 6.452865e-185\nbaseline_diabetes     -0.64875563 0.082053863  -7.906461  2.742595e-15\nbmi:baseline_diabetes  0.01223915 0.002424084   5.048979  4.468924e-07\n                             2.5 %      97.5 %\n(Intercept)            0.474440377  0.59982508\nbmi                   -0.033684938 -0.02944919\nbaseline_diabetes     -0.809585030 -0.48792624\nbmi:baseline_diabetes  0.007487831  0.01699046\n\n\nOur fitted regression model is\n\\[ \\hat{Y} = 0.537 + -0.032 \\times \\text{BMI} + -0.649 \\times \\text{diabetes} + 0.012 \\times \\text{BMI} \\times \\text{diabetes} \\]\n\\(\\hat{\\beta}_0 = 0.537\\): The expected RSI for patients with a 0 kg/m2 BMI and who do not have diabetes is 0.537 points.\n\\(\\hat{\\beta}_{bmi}=-0.032\\): For patients who do not have diabetes, RSI decreases, on average, by 0.032 points for 1 kg/m2 increase in BMI.\n\\(\\hat{\\beta}_{diabetes}=-0.649\\): For patients with a 0 kg/m2 BMI, RSI, on average, is 0.649 points lower for those with diabetes.\n\\(\\hat{\\beta}_{bmi*diabetes} = 0.012\\): This is the difference between the effect of BMI for those with diabetes compared to those without diabetes. For patients with diabetes, a one kg/m2 increase in BMI results in an RSI that is 0.012 units higher, on average."
  },
  {
    "objectID": "labs/prac11s/index.html#b-interaction-test",
    "href": "labs/prac11s/index.html#b-interaction-test",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "3b: Interaction Test",
    "text": "3b: Interaction Test\nTest whether the relationship between RSI and BMI depends on whether the patient had diabetes.\nSolution:\nFrom our model results in 3a, the relationship between RSI and BMI is significantly different for those with diabetes compared to those without diabetes (p=4.47e-07&lt;0.001)."
  },
  {
    "objectID": "labs/prac11s/index.html#c-fitted-regression-model-for-patients-without-diabetes",
    "href": "labs/prac11s/index.html#c-fitted-regression-model-for-patients-without-diabetes",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "3c: Fitted Regression Model for Patients without Diabetes",
    "text": "3c: Fitted Regression Model for Patients without Diabetes\nWhat is the regression equation for patients who don’t have diabetes?\nSolution:\n\\[ \\hat{Y} = 0.537 + -0.032 \\times \\text{BMI} + -0.649 \\times 0 + 0.012 \\times \\text{BMI} \\times 0 = 0.537 + -0.032 \\times \\text{BMI} \\]"
  },
  {
    "objectID": "labs/prac11s/index.html#d-fitted-regression-model-for-patients-with-diabetes",
    "href": "labs/prac11s/index.html#d-fitted-regression-model-for-patients-with-diabetes",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "3d: Fitted Regression Model for Patients with Diabetes",
    "text": "3d: Fitted Regression Model for Patients with Diabetes\nWhat is the regression equation for patients with diabetes?\nSolution:\n\\[ \\hat{Y} = 0.537 + -0.032 \\times \\text{BMI} + -0.649 \\times 1 + 0.012 \\times \\text{BMI} \\times 1 = -0.112 + -0.020 \\times \\text{BMI} \\]"
  },
  {
    "objectID": "labs/prac11s/index.html#e-interaction-visualization",
    "href": "labs/prac11s/index.html#e-interaction-visualization",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "3e: Interaction Visualization",
    "text": "3e: Interaction Visualization\nCreate a scatterplot of RSI versus BMI, using different symbols and separate regression lines for patients with and without diabetes.\nSolution:\n\n\nCode\nplot(x=dat$bmi, y=dat$complication_rsi, xlab='BMI', ylab='RSI', pch=dat$baseline_diabetes)\nabline(a=0.537, b=-0.032, lty=1, col='orangered2')\nabline(a=-0.112, b=-0.020, lty=2, col='orangered2')\nlegend('topright', bty='n', pch=c(0,1), lty=c(1,2),\n       legend=c('No Diabetes','Diabetes'))"
  },
  {
    "objectID": "labs/prac11s/index.html#f-hypothesis-test-for-bmi-without-diabetes",
    "href": "labs/prac11s/index.html#f-hypothesis-test-for-bmi-without-diabetes",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "3f: Hypothesis Test for BMI without Diabetes",
    "text": "3f: Hypothesis Test for BMI without Diabetes\nTest if the slope for BMI for those who don’t have diabetes is significantly different from 0.\nSolution:\nFor this question we can directly use the \\(t\\)-test results from our table of coefficients in 3A. In that output we see that p&lt;2e-16 for bmi, so we reject the null hypothesis that the slope is significantly different from 0 for those who didn’t have diabetes."
  },
  {
    "objectID": "labs/prac11s/index.html#g-hypothesis-test-for-bmi-with-diabetes",
    "href": "labs/prac11s/index.html#g-hypothesis-test-for-bmi-with-diabetes",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "3g: Hypothesis Test for BMI with Diabetes",
    "text": "3g: Hypothesis Test for BMI with Diabetes\nTest if the slope for BMI for those who do have diabetes is significantly different from 0.\nSolution:\nFor this question we can approach it by either calculating the standard error of \\(\\hat{\\beta}_{bmi} + \\hat{\\beta}_{bmi*diabetes}\\) or using reverse coding.\nCalculating the standard error using our original model requires the variance-covariance matrix:\n\n\nCode\nvcov(mod3)\n\n\n                        (Intercept)           bmi baseline_diabetes\n(Intercept)            1.023049e-03 -3.363775e-05     -1.023049e-03\nbmi                   -3.363775e-05  1.167528e-06      3.363775e-05\nbaseline_diabetes     -1.023049e-03  3.363775e-05      6.732836e-03\nbmi:baseline_diabetes  3.363775e-05 -1.167528e-06     -1.923745e-04\n                      bmi:baseline_diabetes\n(Intercept)                    3.363775e-05\nbmi                           -1.167528e-06\nbaseline_diabetes             -1.923745e-04\nbmi:baseline_diabetes          5.876182e-06\n\n\nWith this information we can calculate our standard error:\n\\(\\begin{aligned} SE(\\hat{\\beta}_{bmi} + \\hat{\\beta}_{bmi*diabetes}) =& \\sqrt{Var(\\hat{\\beta}_{bmi}) + Var(\\hat{\\beta}_{bmi*diabetes}) + 2 \\times Cov(\\hat{\\beta}_{bmi},\\hat{\\beta}_{bmi*diabetes})} \\\\ =& \\sqrt{1.167528e-06 + 5.876182e-06 + 2 \\times (-1.167528e-06)} \\\\ =& \\sqrt{4.708654e-06} \\\\ =& 0.002169943 \\end{aligned}\\)\nOur estimated 95% CI using a critical value of 1.96 (i.e., qt(0.975, 28707) based on \\(n=28711\\) with complete data for BMI and diabetes status and subtracting four degrees of freedom) is\n\\[ -0.020 \\pm 1.96 \\times 0.002169943 = (-0.0243, -0.0157) \\]\nWe have \\(t=\\frac{-0.020}{0.002169943}=-9.217\\) and a p-value of 2 * pt(-9.217,28707,lower.tail=T)=0.\nWe can verify this result if we reverse code baseline_diabetes:\n\n\nCode\n# create reverse coding where not_first2y=1 if they did NOT live in first 2 years\ndat$AK_no_diabetes &lt;- abs(dat$baseline_diabetes - 1)\n\n# fit model with reverse coding\nmod3_reverse &lt;- lm(complication_rsi ~ bmi + AK_no_diabetes + bmi*AK_no_diabetes, data=dat)\nround( cbind(summary(mod3_reverse)$coefficients, confint(mod3_reverse)), 4)\n\n\n                   Estimate Std. Error t value Pr(&gt;|t|)   2.5 %  97.5 %\n(Intercept)         -0.1116     0.0756 -1.4772   0.1396 -0.2597  0.0365\nbmi                 -0.0193     0.0022 -8.9071   0.0000 -0.0236 -0.0151\nAK_no_diabetes       0.6488     0.0821  7.9065   0.0000  0.4879  0.8096\nbmi:AK_no_diabetes  -0.0122     0.0024 -5.0490   0.0000 -0.0170 -0.0075\n\n\nWe can see that we arrive at the same results (with slight differences due to rounding) when interpreting bmi in our reverse coded model results."
  },
  {
    "objectID": "labs/prac11s/index.html#h-interactions-and-brief-but-complete-summaries",
    "href": "labs/prac11s/index.html#h-interactions-and-brief-but-complete-summaries",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "3h: Interactions and Brief, but Complete, Summaries",
    "text": "3h: Interactions and Brief, but Complete, Summaries\nProvide a brief, but complete, summary of the relationship between RSI and BMI, accounting for any observed interaction with diabetes (i.e., if there is a significant interaction, interpret those who do and don’t have diabetes separately).\nSolution:\nThe relationship between BMI and RSI differs significantly according to whether or not the patient has diabetes (p&lt;0.001).\nThere is a significant association between BMI and RSI for patients without diabetes (p &lt; 0.001). For these patients, RSI decreases an average of 0.032 units for every one kg/m2 increase in BMI (95% CI: 0.0294 to 0.0337 units lower).\nThere is a significant association between BMI and RSI among patients with diabetes (p &lt; 0.001). For these patients, RSI decreases an average of 0.020 units for every one kg/m2 increase in BMI (95% CI: 0.0157 to 0.0243 units lower).\nThe difference in these relationships for patients with and without diabetes can be seen in the scatterplot with regression fits for each group, where those without diabetes have a slightly stronger negative linear relationship with RSI as BMI increases."
  },
  {
    "objectID": "labs/prac11s/index.html#a-slr",
    "href": "labs/prac11s/index.html#a-slr",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "4a: SLR",
    "text": "4a: SLR\nFit a simple linear regression model for the outcome of RSI for in-hospital complications with age as the predictor.\nSolution:\n\n\nCode\ndat4 &lt;- dat[which(dat$complication==1 & dat$ahrq_ccs=='Hip replacement; total and partial'),]\n\ndat4 &lt;- dat4[!is.na(dat4$age),] # remove one record without age\n\nlm4 &lt;- lm(complication_rsi ~ age, data=dat4)"
  },
  {
    "objectID": "labs/prac11s/index.html#b-polynomial-regression",
    "href": "labs/prac11s/index.html#b-polynomial-regression",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "4b: Polynomial Regression",
    "text": "4b: Polynomial Regression\nFit a polynomial regression model that adds a squared term for age.\nSolution:\n\n\nCode\nlm4_2 &lt;- lm(complication_rsi ~ age + I(age^2), data=dat4)"
  },
  {
    "objectID": "labs/prac11s/index.html#c-polynomial-plots",
    "href": "labs/prac11s/index.html#c-polynomial-plots",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "4c: Polynomial Plots",
    "text": "4c: Polynomial Plots\nCreate a scatterplot of the data and add the predicted regression lines for each model.\nSolution:\n\n\nCode\nplot(x=dat4$age, y=dat4$complication_rsi, xlab='Age', ylab='RSI (In-Hospital Complications)')\nabline(lm4, lwd=2) # plot SLR predicted regression line\n\n# predict Y-hat from regression model with age and age^2, add to scatterplot\nxval &lt;- seq(min(dat4$age,na.rm=T), max(dat4$age,na.rm=T), length.out=100)\ny2 &lt;- predict(lm4_2, newdata = data.frame(age=xval))\nlines(x=xval, y=y2, col='blue', lwd=2, lty=2)\n\n# add legend\nlegend('topleft', bty='n', col=c('black','blue'), lwd=c(2,2), lty=c(1,2), legend=c('Linear','Quadratic'))"
  },
  {
    "objectID": "labs/prac11s/index.html#d-polynomial-vs-slr",
    "href": "labs/prac11s/index.html#d-polynomial-vs-slr",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "4d: Polynomial vs SLR",
    "text": "4d: Polynomial vs SLR\nBased on the model output and figure, is there evidence that a quadratic model may be more appropriate than the simple linear regression model?\nSolution:\nVisually, both models may be appropriate. There are fewer young patients, so it can be challenging to definitely identify if the linear or quadratic model is more appropriate. While we know that age and age2 are highly correlated, we can examine our summary output to note that age2 is significant in the quadratic model (p&lt;0.05):\n\n\nCode\nsummary(lm4)\n\n\n\nCall:\nlm(formula = complication_rsi ~ age, data = dat4)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8344 -1.0164 -0.4061  0.5047  8.9619 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.307892   0.535371  -2.443 0.015352 *  \nage          0.028510   0.007953   3.585 0.000415 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.772 on 221 degrees of freedom\nMultiple R-squared:  0.05495,   Adjusted R-squared:  0.05067 \nF-statistic: 12.85 on 1 and 221 DF,  p-value: 0.0004152\n\n\nCode\nsummary(lm4_2)\n\n\n\nCall:\nlm(formula = complication_rsi ~ age + I(age^2), data = dat4)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9407 -0.9988 -0.3736  0.4102  9.1707 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  1.8377058  1.4762057   1.245   0.2145  \nage         -0.0827367  0.0493547  -1.676   0.0951 .\nI(age^2)     0.0009173  0.0004018   2.283   0.0234 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.755 on 220 degrees of freedom\nMultiple R-squared:  0.07683,   Adjusted R-squared:  0.06843 \nF-statistic: 9.154 on 2 and 220 DF,  p-value: 0.0001517\n\n\nWe can also calculate the MSE to see if it decreases in the quadratic model relative to the first-order model without any polynomial terms:\n\n\nCode\nmean(lm4$residuals^2) # MSE for first-order model\n\n\n[1] 3.110576\n\n\nCode\nmean(lm4_2$residuals^2) # MSE for second-order model\n\n\n[1] 3.038569\n\n\nSince the change in MSE is not very large (3.11 to 3.04), we might argue the more parsimonious SLR model may be preferred given the limited improvement relative to the increased complexity.\nFinally, we may want to evalute the diagnostic plots to see if there is evidence to support the more complex model:\n\n\nCode\n# SLR diagnostics (no quadratic term)\npar(mfrow=c(2,3), mar=c(4.1,4.1,3.1,2.1))\n\n# Scatterplot of Y-X\nplot(x=dat4$age, y=dat4$complication_rsi, xlab='Age', ylab='RSI', \n     main='Scatterplot', cex=1); abline( lm4, col='blue', lwd=2 )\n\n# Scatterplot of residuals by X\nplot(x=dat4$age, y=rstudent(lm4), xlab='Age', ylab='Jackknife Residual', \n     main='Residual Plot by Age', cex=1); abline(h=0, lty=2, col='gray65')\n\n# Scatterplot of residuals by predicted values\nplot(x=predict(lm4), y=rstudent(lm4), xlab=expression(hat(Y)), ylab='Jackknife Residual', \n     main='Residual Plot by Y-hat', cex=1); abline(h=0, lty=2, col='gray65')\n\n# Histogram of jackknife residuals with normal curve\nhist(rstudent(lm4), xlab='Jackknife Residual', \n     main='Histogram of Residuals', freq=F, breaks=seq(-5,6,0.25)); \n  curve( dnorm(x,mean=0,sd=1), lwd=2, col='blue', add=T)\n\n# PP-plot\nplot( ppoints(length(rstudent(lm4))), sort(pnorm(rstudent(lm4))), \n      xlab='Observed Cumulative Probability', \n      ylab='Expected Cumulative Probability', \n      main='Normal Probability Plot', cex=1); \n  abline(a=0,b=1, col='gray65', lwd=1)\n\n# QQ-plot\nqqnorm( rstudent(lm4) ); qqline( rstudent(lm4) )\n\n\n\n\n\n\n\nCode\n# SLR diagnostics (no quadratic term)\npar(mfrow=c(2,3), mar=c(4.1,4.1,3.1,2.1))\n\n# Scatterplot of Y-X\nplot(x=dat4$age, y=dat4$complication_rsi, xlab='Age', ylab='RSI', \n     main='Scatterplot', cex=1)\n# predict Y-hat from regression model with age and age^2, add to scatterplot\nxval &lt;- seq(min(dat4$age,na.rm=T), max(dat4$age,na.rm=T), length.out=100)\ny2 &lt;- predict(lm4_2, newdata = data.frame(age=xval))\nlines(x=xval, y=y2, col='blue', lwd=2)\n\n# Scatterplot of residuals by X\nplot(x=dat4$age, y=rstudent(lm4_2), xlab='Age', ylab='Jackknife Residual', \n     main='Residual Plot by Age', cex=1); abline(h=0, lty=2, col='gray65')\n\n# Scatterplot of residuals by predicted values\nplot(x=predict(lm4_2), y=rstudent(lm4_2), xlab=expression(hat(Y)), ylab='Jackknife Residual', \n     main='Residual Plot by Y-hat', cex=1); abline(h=0, lty=2, col='gray65')\n\n# Histogram of jackknife residuals with normal curve\nhist(rstudent(lm4_2), xlab='Jackknife Residual', \n     main='Histogram of Residuals', freq=F, breaks=seq(-5,6,0.25)); \n  curve( dnorm(x,mean=0,sd=1), lwd=2, col='blue', add=T)\n\n# PP-plot\nplot( ppoints(length(rstudent(lm4_2))), sort(pnorm(rstudent(lm4_2))), \n      xlab='Observed Cumulative Probability', \n      ylab='Expected Cumulative Probability', \n      main='Normal Probability Plot', cex=1); \n  abline(a=0,b=1, col='gray65', lwd=1)\n\n# QQ-plot\nqqnorm( rstudent(lm4_2) ); qqline( rstudent(lm4_2) )\n\n\n\n\n\nBased on the diagnostic plots, it appears both models are impacted by violations of the normality (histogram, PP, and QQ plots all have departures from normality) and potentially homoscedasticity assumption (residual plot by age might have a bit of a fan shape, but Y-hat not as much). Regardless, if just comparing to select between the first- and second-order model, there is not much evidence supporting a drastic improvement with the more complicated model with the quadratic polynomial term."
  },
  {
    "objectID": "labs/prac11s/index.html#a-fitting-the-model",
    "href": "labs/prac11s/index.html#a-fitting-the-model",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "5a: Fitting The Model",
    "text": "5a: Fitting The Model\nFit a linear regression model for the outcome of RSI for in-hospital complication with day of the week as a predictor for all days.\nSolution:\nHere we will fit our model and view the summary output for use in later problems.\n\n\nCode\nmod5 &lt;- glm(complication_rsi ~ as.factor(dow), data=dat)\nsummary(mod5)\n\n\n\nCall:\nglm(formula = complication_rsi ~ as.factor(dow), data = dat)\n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -0.51529    0.01435 -35.907  &lt; 2e-16 ***\nas.factor(dow)2  0.02393    0.02029   1.179    0.238    \nas.factor(dow)3  0.18989    0.02088   9.092  &lt; 2e-16 ***\nas.factor(dow)4  0.15168    0.02149   7.057 1.73e-12 ***\nas.factor(dow)5  0.19485    0.02105   9.258  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.442609)\n\n    Null deviance: 46388  on 32000  degrees of freedom\nResidual deviance: 46158  on 31996  degrees of freedom\nAIC: 102549\n\nNumber of Fisher Scoring iterations: 2\n\n\nOur fitted regression equation is\n\\[\\hat{Y} = -0.52 + 0.02 I_{T} + 0.19 I_{W} + 0.15 I_{Th} + 0.19 I_{F} \\]"
  },
  {
    "objectID": "labs/prac11s/index.html#b-glht-for-two-coefficients",
    "href": "labs/prac11s/index.html#b-glht-for-two-coefficients",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "5b: GLHT for Two Coefficients",
    "text": "5b: GLHT for Two Coefficients\nUsing a general linear hypothesis, test if the mean RSI on Tuesday and Wednesday are equal to each other.\nSolution:\nFirst, if we want to compare the mean RSI value for each day we know from our reference cell model we would have an estimate of the mean for each group as \\(\\mu_{T} = \\beta_0 + \\beta_T\\) and \\(\\mu_{W} = \\beta_0 + \\beta_{W}\\). However, since our intercept \\(\\beta_0\\) is in both, it will cancel out when testing our null hypothesis that \\(\\mu_T = \\mu_W\\).\nTherefore, this tests \\(H_0\\colon \\beta_{T} = \\beta_{W}\\), or equivalently \\(H_0\\colon \\beta_{T} - \\beta_{W} = 0\\). We can further imagine this as \\(1 \\times \\beta_{T} - 1 \\times \\beta_{W}\\), so our matrix is \\(\\mathbf{c} = \\begin{pmatrix} 0 & 1 & -1 & 0 & 0 \\end{pmatrix}\\):\n\n\nCode\nlibrary(gmodels)\n\n\nWarning: package 'gmodels' was built under R version 4.4.1\n\n\nCode\n# First, create matrix for given contrast\nc_matrix_5b &lt;- rbind( c(0,1,-1,0,0) )\n\n# Then, implement general linear hypothesis test\nglh.test(mod5, c_matrix_5b, d=rep(0,nrow(c_matrix_5b)))\n\n\n\n     Test of General Linear Hypothesis  \nCall:\nglh.test(reg = mod5, cm = c_matrix_5b, d = rep(0, nrow(c_matrix_5b)))\nF = 63.1627, df1 =     1, df2 = 31996, p-value = 1.998e-15 \n\n\nSince \\(p&lt;0.05\\), we would reject the null hypothesis and conclude that Tuesday and Wednesday have different mean RSI values.\nSince we only wanted to compare if Tuesday and Wednesday were equal, we could also have fit a model with the reference category for our categorical variable changed to Tuesday or Wednesday:\n\n\nCode\ndat$dow_v2 &lt;- factor(dat$dow, levels=c(2,3,4,5,1), labels=c('T','W','Th','F','M'))\nsummary( glm(complication_rsi ~ dow_v2, data=dat))\n\n\n\nCall:\nglm(formula = complication_rsi ~ dow_v2, data = dat)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.49136    0.01435 -34.247  &lt; 2e-16 ***\ndow_v2W      0.16596    0.02088   7.947 1.97e-15 ***\ndow_v2Th     0.12775    0.02149   5.945 2.80e-09 ***\ndow_v2F      0.17093    0.02104   8.122 4.74e-16 ***\ndow_v2M     -0.02393    0.02029  -1.179    0.238    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.442609)\n\n    Null deviance: 46388  on 32000  degrees of freedom\nResidual deviance: 46158  on 31996  degrees of freedom\nAIC: 102549\n\nNumber of Fisher Scoring iterations: 2\n\n\nIn our regression output with Tuesday as the reference category, we see we have a similar p-value and that \\(t^2 = 7.946^2 = 63.139\\), which is approximately equal to our \\(F\\)-statistic from the GLHT (differing slightly due to rounding)."
  },
  {
    "objectID": "labs/prac11s/index.html#c-glht-for-combination-of-coefficients",
    "href": "labs/prac11s/index.html#c-glht-for-combination-of-coefficients",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "5c: GLHT for Combination of Coefficients",
    "text": "5c: GLHT for Combination of Coefficients\nUsing a general linear hypothesis, test if the mean RSI on Monday is equal to two times the RSI on Thursday.\nSolution:\nHere we know that for our model fit in 5a, \\(\\mu_M = \\beta_0\\) and \\(\\mu_{Th} = \\beta_0 + \\beta_{Th}\\). Therefore this hypothesis is testing the null that \\(\\mu_M = 2 \\times \\mu_{Th}\\). Plugging in our regression coefficients we have \\(\\beta_0 = 2 \\times (\\beta_0 + \\beta_{Th})\\). This can be rearranged to show that \\(-\\beta_0 + 2\\beta_{Th} = 0\\):\n\\[ H_0 \\colon \\begin{pmatrix} -1 & 0 & 0 & 2 & 0 \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_T \\\\ \\beta_W \\\\ \\beta_{Th} \\\\ \\beta_{F} \\end{pmatrix} = 0 \\]\n\n\nCode\n# First, create matrix for given contrast\nc_matrix_5c &lt;- rbind( c(-1,0,0,2,0) )\n\n# Then, implement general linear hypothesis test\nglh.test(mod5, c_matrix_5c, d=rep(0,nrow(c_matrix_5c)))\n\n\n\n     Test of General Linear Hypothesis  \nCall:\nglh.test(reg = mod5, cm = c_matrix_5c, d = rep(0, nrow(c_matrix_5c)))\nF = 232.9077, df1 =     1, df2 = 31996, p-value = &lt; 2.2e-16 \n\n\nSince \\(p &lt; 0.05\\), we reject \\(H_0\\) and conclude that the mean RSI on Monday is not equal to two times the mean RSI on Thursday."
  },
  {
    "objectID": "labs/prac11s/index.html#d-glht-for-simultaneous-testing",
    "href": "labs/prac11s/index.html#d-glht-for-simultaneous-testing",
    "title": "Week 11 Practice Problems: Solutions",
    "section": "5d: GLHT for Simultaneous Testing",
    "text": "5d: GLHT for Simultaneous Testing\nUsing a general linear hypothesis, test both 5b and 5c simultaneously.\nSolution:\nFor 5b and 5c, we tested each hypothesis on its own. However, we could test both simultaneously:\n\\[ H_0 \\colon \\begin{pmatrix} 0 & 1 & -1 & 0  & 0 \\\\ -1 & 0 & 0 & 2 & 0 \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_T \\\\ \\beta_W \\\\ \\beta_{Th} \\\\ \\beta_{F} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\]\n\n\nCode\n# First, create matrix for given contrast\nc_matrix_5d &lt;- rbind( c(0,1,-1,0,0), c(-1,0,0,2,0) )\n\n# Then, implement general linear hypothesis test\nglh.test(mod5, c_matrix_5d, d=rep(0,nrow(c_matrix_5d)))\n\n\n\n     Test of General Linear Hypothesis  \nCall:\nglh.test(reg = mod5, cm = c_matrix_5d, d = rep(0, nrow(c_matrix_5d)))\nF = 148.0352, df1 =     2, df2 = 31996, p-value = &lt; 2.2e-16 \n\n\nUnsurprisingly, given our previous results with both being very significant on their own, this result is still also significant since \\(p&lt;0.001\\). In other words, our results suggests that at least one null hypothesis is rejected."
  },
  {
    "objectID": "labs/prac12s/index.html",
    "href": "labs/prac12s/index.html",
    "title": "Week 12 Practice Problems: Solutions",
    "section": "",
    "text": "This page includes the solutions to the optional practice problems for the given week. If you want to see a version without solutions please click here. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises focus on model/variable selection and examining the potential for influential points."
  },
  {
    "objectID": "labs/prac12s/index.html#a-replicating-the-lecture-results",
    "href": "labs/prac12s/index.html#a-replicating-the-lecture-results",
    "title": "Week 12 Practice Problems: Solutions",
    "section": "1a: Replicating the Lecture Results",
    "text": "1a: Replicating the Lecture Results\nUsing the observed data, recreate the results from our slides.\nSolution:\nRecall, we have the following thresholds for determining influence, leverage, or outliers:\n\nLeverage: \\(2(p+1)/n=2(2)/21=0.19\\)\nJackknife Residual: \\(\\pm 3\\); \\(\\pm 4\\)\nDDFITS: \\(\\pm 2\\sqrt{(p+1)/n}=2\\sqrt{2/21}=\\pm0.62\\)\nDFBETAS: \\(\\pm 2/\\sqrt{n} = 2 /\\sqrt{21} = 0.44\\)\nCook’s Distance: \\(d_i&gt;1.0\\)\n\n\n\nCode\nlibrary(olsrr) # load package for visualizations\n\n# Fit model and calculate summaries\nlm0 &lt;- lm(Y~X, data=oildat)\njk &lt;- rstandard(lm0)\ncooksd &lt;- cooks.distance(lm0)\nleverage &lt;- hatvalues(lm0)\ndffits_vals &lt;- dffits(lm0)\ndfbetas_int &lt;- dfbetas(lm0)[,1]\ndfbetas_X &lt;- dfbetas(lm0)[,2]\n\ninfluence_measures_df &lt;- cbind(\"JKResiduals\" = jk, \n                               \"CooksDistance\"=cooksd,\n                               \"Leverage\" = leverage,\n                               \"DFFITS\"=dffits_vals,\n                               \"DFBETAS_int\"=dfbetas_int,\n                               \"DFBETAS_X\"=dfbetas_X)\n\n# print influence measures data frame\nround(influence_measures_df,2)\n\n\n   JKResiduals CooksDistance Leverage DFFITS DFBETAS_int DFBETAS_X\n1        -2.78          2.69     0.41  -2.93        2.64     -2.76\n2         0.37          0.00     0.05   0.08        0.03     -0.02\n3         0.81          0.03     0.07   0.23       -0.12      0.14\n4        -1.01          0.05     0.08  -0.30       -0.22      0.20\n5        -1.73          0.08     0.05  -0.42       -0.13      0.08\n6         0.76          0.01     0.05   0.17        0.04     -0.02\n7        -0.04          0.00     0.05  -0.01        0.00      0.00\n8        -0.66          0.02     0.07  -0.18       -0.11      0.10\n9        -1.62          0.24     0.16  -0.73       -0.65      0.61\n10       -0.45          0.01     0.10  -0.15       -0.12      0.11\n11        0.81          0.02     0.05   0.19        0.08     -0.06\n12        0.37          0.00     0.05   0.09       -0.02      0.03\n13        0.29          0.01     0.14   0.12        0.10     -0.09\n14       -0.36          0.00     0.05  -0.08        0.01     -0.02\n15        0.47          0.01     0.05   0.10        0.00      0.01\n16        0.34          0.00     0.07   0.09        0.06     -0.06\n17       -0.12          0.00     0.07  -0.03        0.01     -0.02\n18        1.68          0.10     0.07   0.48       -0.22      0.27\n19        1.57          0.16     0.11   0.59       -0.40      0.45\n20        0.71          0.02     0.09   0.22        0.16     -0.14\n21       -0.04          0.00     0.15  -0.02        0.01     -0.01\n\n\nIn our summaries we see the following:\n\nJackknife Residuals: None are outside the range of \\(\\pm3\\)\nCook’s Distance: observation 1 has 2.69 &gt; 1 and may be an influential point\nLeverage: observation 1 has 0.41&gt;0.19 and may have high leverage\nDFFITS: observations 1 and 9 are both larger than our threshold (observation 1 has -2.96 &lt; -0.62; observation 9 has -0.73 &lt; -0.62)\nDFBETAS: observation 1 has both the intercept and predictor with large value (intercept has 2.64 &gt; 0.44, predictor has -2.76 &lt; -0.44); observations 9 and 19 also have values indicating potential influence\n\nWe can also visualize the evaluation of influence/leverage/outliers (and olsrr will add the thresholds to our plots):\n\n\nCode\n# create plots of data\n\n# plot the data with labels\nplot(Y~X, col=\"lightblue\",pch=19,cex=2,data=oildat)\nabline(lm0,col=\"red\",lwd=3)\ntext(Y~X, labels=rownames(oildat),cex=0.9,font=2,data=oildat)\n\n\n\n\n\nCode\n# plot Cook's D (one measure of influence)\nols_plot_cooksd_chart(lm0)\n\n\n\n\n\nCode\n# plot DFBETAS (one measure of influence)\nols_plot_dfbetas(lm0)\n\n\n\n\n\nCode\n# plot DFFITS (one measure of influence)\nols_plot_dffits(lm0)\n\n\n\n\n\nCode\n# Plot standardized residuals (one measure for outliers)\nols_plot_resid_stand(lm0)\n\n\n\n\n\nCode\n# Plot outlier and leverage diagnostics plot\nols_plot_resid_lev(lm0)\n\n\n\n\n\nThis final plot nicely shows that observation 1 may be both an outlier and a leverage point."
  },
  {
    "objectID": "labs/prac12s/index.html#b-modification-1",
    "href": "labs/prac12s/index.html#b-modification-1",
    "title": "Week 12 Practice Problems: Solutions",
    "section": "1b: Modification 1",
    "text": "1b: Modification 1\nReplace first row of data with (X,Y)=(150, 115). Confirm the first observation is not an outlier, has little influence, and has high leverage.\nSolution:\n\n\nCode\n# manipulate data\noildat2 &lt;- oildat\noildat2[1,] &lt;- c(150,115)\n\n# Fit model and calculate summaries\nlm1 &lt;- lm(Y~X, data=oildat2)\njk &lt;- rstandard(lm1)\ncooksd &lt;- cooks.distance(lm1)\nleverage &lt;- hatvalues(lm1)\ndffits_vals &lt;- dffits(lm1)\ndfbetas_int &lt;- dfbetas(lm1)[,1]\ndfbetas_X &lt;- dfbetas(lm1)[,2]\n\ninfluence_measures_df2 &lt;- cbind(\"JKResiduals\" = jk, \n                               \"CooksDistance\"=cooksd,\n                               \"Leverage\" = leverage,\n                               \"DFFITS\"=dffits_vals,\n                               \"DFBETAS_int\"=dfbetas_int,\n                               \"DFBETAS_X\"=dfbetas_X)\n\n# print influence measures data frame\nround(influence_measures_df2,2)\n\n\n   JKResiduals CooksDistance Leverage DFFITS DFBETAS_int DFBETAS_X\n1        -0.25          0.02     0.41  -0.20        0.18     -0.19\n2         0.44          0.01     0.05   0.10        0.04     -0.03\n3         0.39          0.01     0.07   0.11       -0.06      0.06\n4        -1.01          0.05     0.08  -0.30       -0.22      0.20\n5        -2.34          0.14     0.05  -0.62       -0.19      0.12\n6         0.86          0.02     0.05   0.19        0.05     -0.03\n7        -0.25          0.00     0.05  -0.06       -0.01      0.00\n8        -0.68          0.02     0.07  -0.18       -0.12      0.10\n9        -1.38          0.18     0.16  -0.61       -0.54      0.51\n10       -0.14          0.00     0.10  -0.05       -0.04      0.03\n11        1.05          0.03     0.05   0.25        0.11     -0.09\n12        0.06          0.00     0.05   0.01        0.00      0.00\n13        1.03          0.09     0.14   0.42        0.37     -0.34\n14       -0.85          0.02     0.05  -0.20        0.04     -0.06\n15        0.35          0.00     0.05   0.08        0.00      0.01\n16        0.68          0.02     0.07   0.19        0.13     -0.11\n17       -0.77          0.02     0.07  -0.21        0.10     -0.12\n18        1.58          0.09     0.07   0.45       -0.20      0.25\n19        1.09          0.08     0.11   0.39       -0.27      0.30\n20        1.24          0.07     0.09   0.39        0.29     -0.26\n21       -1.17          0.12     0.15  -0.49        0.37     -0.40\n\n\nFor observation 1, specifically, we now see that is has low estimates within our threshold for the jackknife residuals, Cook’s D, DFFITS, and DFBETAS. However, observation 1 still has a high estimate of leverage.\nOur plots reflect this change:\n\n\nCode\n# create plots of data\n\n# plot the data with labels\nplot(Y~X, col=\"lightblue\",pch=19,cex=2,data=oildat2)\nabline(lm1,col=\"red\",lwd=3)\ntext(Y~X, labels=rownames(oildat2),cex=0.9,font=2,data=oildat2)\n\n\n\n\n\nCode\n# plot Cook's D (one measure of influence)\nols_plot_cooksd_chart(lm1)\n\n\n\n\n\nCode\n# plot DFBETAS (one measure of influence)\nols_plot_dfbetas(lm1)\n\n\n\n\n\nCode\n# plot DFFITS (one measure of influence)\nols_plot_dffits(lm1)\n\n\n\n\n\nCode\n# Plot standardized residuals (one measure for outliers)\nols_plot_resid_stand(lm1)\n\n\n\n\n\nCode\n# Plot outlier and leverage diagnostics plot\nols_plot_resid_lev(lm1)\n\n\n\n\n\nUnlike the original data (in 1a), we see here that observation 1 is only considered a leverage point (but not an outlier). Interestingly, this change for the 1st observation results in the 5th observation being a potential outlier."
  },
  {
    "objectID": "labs/prac12s/index.html#c-modification-2",
    "href": "labs/prac12s/index.html#c-modification-2",
    "title": "Week 12 Practice Problems: Solutions",
    "section": "1c: Modification 2",
    "text": "1c: Modification 2\nReplace first row of data with (X,Y)=(114, 115). Confirm the first observation is an outlier, has little influence, and has low leverage.\nSolution:\n\n\nCode\n# manipulate data\noildat2 &lt;- oildat\noildat2[1,] &lt;- c(114,115)\n\n# Fit model and calculate summaries\nlm2 &lt;- lm(Y~X, data=oildat2)\njk &lt;- rstandard(lm2)\ncooksd &lt;- cooks.distance(lm2)\nleverage &lt;- hatvalues(lm2)\ndffits_vals &lt;- dffits(lm2)\ndfbetas_int &lt;- dfbetas(lm2)[,1]\ndfbetas_X &lt;- dfbetas(lm2)[,2]\n\ninfluence_measures_df3 &lt;- cbind(\"JKResiduals\" = jk, \n                               \"CooksDistance\"=cooksd,\n                               \"Leverage\" = leverage,\n                               \"DFFITS\"=dffits_vals,\n                               \"DFBETAS_int\"=dfbetas_int,\n                               \"DFBETAS_X\"=dfbetas_X)\n\n# print influence measures data frame\nround(influence_measures_df3,2)\n\n\n   JKResiduals CooksDistance Leverage DFFITS DFBETAS_int DFBETAS_X\n1         3.12          0.24     0.05   0.97        0.10     -0.02\n2         0.15          0.00     0.05   0.03        0.01     -0.01\n3         0.08          0.00     0.11   0.03       -0.02      0.02\n4        -0.86          0.03     0.09  -0.26       -0.19      0.18\n5        -1.80          0.08     0.05  -0.43       -0.07      0.04\n6         0.44          0.00     0.05   0.10        0.01      0.00\n7        -0.34          0.00     0.05  -0.08        0.01     -0.01\n8        -0.63          0.01     0.07  -0.17       -0.10      0.09\n9        -1.13          0.15     0.19  -0.55       -0.50      0.48\n10       -0.24          0.00     0.12  -0.08       -0.07      0.07\n11        0.58          0.01     0.05   0.13        0.05     -0.04\n12       -0.14          0.00     0.07  -0.04        0.02     -0.02\n13        0.59          0.04     0.17   0.27        0.24     -0.23\n14       -0.77          0.02     0.06  -0.20        0.08     -0.09\n15        0.07          0.00     0.05   0.02        0.00      0.00\n16        0.33          0.00     0.08   0.09        0.06     -0.06\n17       -0.74          0.03     0.10  -0.24        0.16     -0.17\n18        0.93          0.05     0.10   0.30       -0.20      0.22\n19        0.58          0.04     0.18   0.27       -0.22      0.23\n20        0.73          0.03     0.09   0.23        0.17     -0.16\n21       -1.09          0.19     0.24  -0.61        0.52     -0.55\n\n\nFor observation 1, specifically, we see it has a jackknife residual &gt;3 (indicating it may be an outlier) and that it generally has little influence and leverage (the exception being its DFFITS is 0.97 &gt; 0.62).\nOur plots reflect this change:\n\n\nCode\n# create plots of data\n\n# plot the data with labels\nplot(Y~X, col=\"lightblue\",pch=19,cex=2,data=oildat2)\nabline(lm2,col=\"red\",lwd=3)\ntext(Y~X, labels=rownames(oildat2),cex=0.9,font=2,data=oildat2)\n\n\n\n\n\nCode\n# plot Cook's D (one measure of influence)\nols_plot_cooksd_chart(lm2)\n\n\n\n\n\nCode\n# plot DFBETAS (one measure of influence)\nols_plot_dfbetas(lm2)\n\n\n\n\n\nCode\n# plot DFFITS (one measure of influence)\nols_plot_dffits(lm2)\n\n\n\n\n\nCode\n# Plot standardized residuals (one measure for outliers)\nols_plot_resid_stand(lm2)\n\n\n\n\n\nCode\n# Plot outlier and leverage diagnostics plot\nols_plot_resid_lev(lm2)\n\n\n\n\n\nUnlike the original data (in 1a), we see here that observation 1 is only an outlier but not a leverage point. However, this change in value has led to observations 9 and 21 being potential leverage points."
  },
  {
    "objectID": "labs/prac12s/index.html#d-modification-3",
    "href": "labs/prac12s/index.html#d-modification-3",
    "title": "Week 12 Practice Problems: Solutions",
    "section": "1d: Modification 3",
    "text": "1d: Modification 3\nRemove the first row of data. Confirm there are no outliers, influential points or leverage points.\nSolution:\nSince we’ve removed a data point, some of our thresholds will change (albeit slightly):\n\nLeverage: \\(2(p+1)/n=2(2)/20=0.20\\)\nJackknife Residual: \\(\\pm 3\\); \\(\\pm 4\\)\nDDFITS: \\(\\pm 2\\sqrt{(p+1)/n}=2\\sqrt{2/20}=\\pm0.63\\)\nDFBETAS: \\(\\pm 2/\\sqrt{n} = 2 /\\sqrt{20} = 0.45\\)\nCook’s Distance: \\(d_i&gt;1.0\\)\n\n\n\nCode\n# manipulate data\noildat2 &lt;- oildat[2:21,] # remove 1st observation\n\n# Fit model and calculate summaries\nlm3 &lt;- lm(Y~X, data=oildat2)\njk &lt;- rstandard(lm3)\ncooksd &lt;- cooks.distance(lm3)\nleverage &lt;- hatvalues(lm3)\ndffits_vals &lt;- dffits(lm3)\ndfbetas_int &lt;- dfbetas(lm3)[,1]\ndfbetas_X &lt;- dfbetas(lm3)[,2]\n\ninfluence_measures_df3 &lt;- cbind(\"JKResiduals\" = jk, \n                               \"CooksDistance\"=cooksd,\n                               \"Leverage\" = leverage,\n                               \"DFFITS\"=dffits_vals,\n                               \"DFBETAS_int\"=dfbetas_int,\n                               \"DFBETAS_X\"=dfbetas_X)\n\n# print influence measures data frame\nround(influence_measures_df3,2)\n\n\n   JKResiduals CooksDistance Leverage DFFITS DFBETAS_int DFBETAS_X\n2         0.43          0.00     0.05   0.10        0.03     -0.02\n3         0.34          0.01     0.11   0.12       -0.08      0.09\n4        -0.97          0.05     0.09  -0.30       -0.22      0.20\n5        -2.29          0.14     0.05  -0.61       -0.10      0.05\n6         0.83          0.02     0.05   0.19        0.02      0.00\n7        -0.26          0.00     0.05  -0.06        0.00     -0.01\n8        -0.66          0.02     0.07  -0.18       -0.11      0.10\n9        -1.33          0.21     0.19  -0.67       -0.60      0.57\n10       -0.10          0.00     0.12  -0.04       -0.03      0.03\n11        1.03          0.03     0.05   0.25        0.09     -0.06\n12        0.02          0.00     0.07   0.01        0.00      0.00\n13        1.07          0.12     0.18   0.49        0.44     -0.42\n14       -0.86          0.03     0.06  -0.22        0.09     -0.11\n15        0.32          0.00     0.05   0.07       -0.01      0.02\n16        0.68          0.02     0.08   0.20        0.13     -0.12\n17       -0.81          0.04     0.10  -0.27        0.17     -0.19\n18        1.52          0.13     0.10   0.52       -0.33      0.37\n19        1.04          0.12     0.18   0.49       -0.40      0.42\n20        1.24          0.08     0.09   0.41        0.30     -0.28\n21       -1.29          0.26     0.24  -0.74        0.63     -0.66\n\n\n\n\nCode\n# create plots of data\n\n# plot the data with labels\nplot(Y~X, col=\"lightblue\",pch=19,cex=2,data=oildat2)\nabline(lm3,col=\"red\",lwd=3)\ntext(Y~X, labels=rownames(oildat2),cex=0.9,font=2,data=oildat2)\n\n\n\n\n\nCode\n# plot Cook's D (one measure of influence)\nols_plot_cooksd_chart(lm3)\n\n\n\n\n\nCode\n# plot DFBETAS (one measure of influence)\nols_plot_dfbetas(lm3)\n\n\n\n\n\nCode\n# plot DFFITS (one measure of influence)\nols_plot_dffits(lm3)\n\n\n\n\n\nCode\n# Plot standardized residuals (one measure for outliers)\nols_plot_resid_stand(lm3)\n\n\n\n\n\nCode\n# Plot outlier and leverage diagnostics plot\nols_plot_resid_lev(lm3)\n\n\n\n\n\nBy removing the first observation, we see that our resulting diagnostics suggest that most points do not have extreme influence or leverage, and are likely not outliers. However, some values are beyond our thresholds and may not reflect the strong claim that there are definitively no outliers, influential points or leverage points. This also helps to further demonstrate that simply removing a “problem” observation does not necessarily fix everything and can still result in other points becoming potentially concerning."
  },
  {
    "objectID": "labs/prac12s/index.html#a-model-selection-approaches",
    "href": "labs/prac12s/index.html#a-model-selection-approaches",
    "title": "Week 12 Practice Problems: Solutions",
    "section": "2a: Model Selection Approaches",
    "text": "2a: Model Selection Approaches\nUsing all subsets regression, calculate the AIC, AICc, BIC, adjusted R2, and Mallows’ Cp. Identify the optimal model for each approach. Hint: you may need to use expand.grid, for loops, or other manual coding to generate all possible models and calculate each summary.\nSolution:\nWe can use expand.grid to generate indicators for all variables and then fit each model in a for loop and save all generated summaries. We will also want to convert our categorical variables from numeric to a factor or text string so R does not treat them as continuous predictors and create a new dataset with records with complete data for our outcome/predictors:\n\n\nCode\n# modify categorical variables with multiple categories\ndat$TVol &lt;- factor(dat$TVol, levels=c(1,2,3), labels=c('Low','Medium','Extensive'))\ndat$bGS &lt;- factor(dat$bGS, levels=c(1,2,3), labels=c('0-6','7','8-10'))\n\n# reduce data to complete cases\ndatc &lt;- dat[complete.cases(dat[,c('PreopPSA','Age','PVol','TVol','bGS')]),]\n\n# create data frame with all combinations of predictors\nall_subsets &lt;- expand.grid(Age=c(0,1), \n                           PVol=c(0,1),\n                           TVol=c(0,1),\n                           bGS=c(0,1))\n\n# create object to save results in\nres_mat &lt;- all_subsets\nres_mat[,c('aic','aicc','bic','r2','cp')] &lt;- NA\n\n# calculate full model with all predictors to save MSE for use in Cp calculation\nfull_mod &lt;- lm(PreopPSA ~ Age + PVol + TVol + bGS, data=datc)\nfull_mse &lt;- sum(residuals(full_mod)^2) / (nrow(datc) - 6 - 1) # calculate MSE for full model\n\n# loop through each combination and save results\nfor( i in 1:nrow(all_subsets)){\n  \n  rhs_lm &lt;- paste0( colnames(all_subsets)[which(all_subsets[i,]==1)], collapse = '+') # automatically extract included variables to place onto right hand side of lm() equation\n  \n  if( rhs_lm == '' ){\n    lm_i &lt;- lm( PreopPSA ~ 1, data=datc )\n  }else{\n    lm_i &lt;- lm( as.formula(paste0('PreopPSA ~ ', rhs_lm)), data=datc )\n  }\n  \n  res_mat[i,'aic'] &lt;- AIC(lm_i, k=2)\n  numpred &lt;- length(coef(lm_i))-1 # calculate the number of predictors to use for calculation of AICc\n  res_mat[i,'aicc'] &lt;- AIC(lm_i, k=2) + (2*numpred^2 + 2*numpred) / (nrow(datc) - numpred - 1)\n  res_mat[i,'bic'] &lt;- BIC(lm_i)\n  res_mat[i,'r2'] &lt;- summary(lm_i)$adj.r.squared\n  res_mat[i,'cp'] &lt;- ( sum(residuals(lm_i)^2) / full_mse) - (nrow(datc) - 2*(numpred+1))\n}\n\nround(res_mat,3)\n\n\n   Age PVol TVol bGS      aic     aicc      bic     r2     cp\n1    0    0    0   0 1937.794 1937.794 1945.202  0.000 80.955\n2    1    0    0   0 1939.461 1939.474 1950.572 -0.002 82.534\n3    0    1    0   0 1923.941 1923.955 1935.053  0.048 63.450\n4    1    1    0   0 1922.734 1922.774 1937.549  0.055 61.627\n5    0    0    1   0 1907.856 1907.897 1922.671  0.101 44.421\n6    1    0    1   0 1909.856 1909.937 1928.375  0.098 46.421\n7    0    1    1   0 1882.958 1883.039 1901.477  0.175 17.399\n8    1    1    1   0 1882.470 1882.606 1904.693  0.179 16.843\n9    0    0    0   1 1914.880 1914.920 1929.695  0.080 52.437\n10   1    0    0   1 1916.033 1916.114 1934.552  0.079 53.461\n11   0    1    0   1 1898.515 1898.596 1917.034  0.131 33.866\n12   1    1    0   1 1895.235 1895.371 1917.458  0.144 30.181\n13   0    0    1   1 1899.369 1899.504 1921.591  0.132 34.623\n14   1    0    1   1 1901.247 1901.451 1927.173  0.129 36.491\n15   0    1    1   1 1874.536 1874.740 1900.462  0.203  8.849\n16   1    1    1   1 1872.621 1872.907 1902.251  0.211  7.000\n\n\nThe results generally agree that a model with everything except age is likely optimal:\n\nAIC and AICc are both minimized for the full model, however the difference between this and the model without age is less than 2, suggesting the more parsimonious model may be more appropriate.\nBIC is minimized for model excluding only age. We might also consider the model with just PVol and TVol (model 7) since its BIC is only 1.015 larger, but the model is more parsimonious.\nOur adjusted \\(R^2\\) is technically maximized for the full model, but the improvement is not large relative to the model excluding age (i.e., 21.1% versus 20.3% of variability explained).\nMallows’ Cp is fairly small for the model excluding age relative to the full model, whereas most other models have a much larger values."
  },
  {
    "objectID": "labs/prac12s/index.html#b-variable-selection-approaches",
    "href": "labs/prac12s/index.html#b-variable-selection-approaches",
    "title": "Week 12 Practice Problems: Solutions",
    "section": "2b: Variable Selection Approaches",
    "text": "2b: Variable Selection Approaches\nIdentify the best model using forward selection, backward selection, and stepwise selection based on BIC.\nSolution:\n\n\nCode\nfull_mod &lt;- lm(PreopPSA ~ Age + PVol + TVol + bGS, data=datc)\nnull_mod &lt;- lm(PreopPSA ~ 1, data=datc)\nn &lt;- nrow(datc)\n\n# backward selection\nbackward &lt;- step(full_mod, \n                 direction='backward', \n                 k=log(n), \n                 trace=0)\nbackward\n\n\n\nCall:\nlm(formula = PreopPSA ~ PVol + TVol + bGS, data = datc)\n\nCoefficients:\n  (Intercept)           PVol     TVolMedium  TVolExtensive           bGS7  \n      1.97022        0.05621        1.40467        4.77595        2.03585  \n      bGS8-10  \n      3.12757  \n\n\nCode\n# forward selection\nforward &lt;- step(null_mod, \n                direction='forward',\n                scope = ~ Age + PVol + TVol + bGS,\n                k=log(n),\n                trace=0)\nforward\n\n\n\nCall:\nlm(formula = PreopPSA ~ TVol + PVol + bGS, data = datc)\n\nCoefficients:\n  (Intercept)     TVolMedium  TVolExtensive           PVol           bGS7  \n      1.97022        1.40467        4.77595        0.05621        2.03585  \n      bGS8-10  \n      3.12757  \n\n\nCode\n# stepwise selection starting with null model\nstepwise_null &lt;- step(null_mod, \n                direction='both',\n                scope = ~ Age + PVol + TVol + bGS,\n                k=log(n),\n                trace=0)\nstepwise_null\n\n\n\nCall:\nlm(formula = PreopPSA ~ TVol + PVol + bGS, data = datc)\n\nCoefficients:\n  (Intercept)     TVolMedium  TVolExtensive           PVol           bGS7  \n      1.97022        1.40467        4.77595        0.05621        2.03585  \n      bGS8-10  \n      3.12757  \n\n\nCode\n# stepwise selection starting with full model\nstepwise_full &lt;- step(full_mod, \n                direction='both',\n                k=log(n),\n                trace=0)\nstepwise_full\n\n\n\nCall:\nlm(formula = PreopPSA ~ PVol + TVol + bGS, data = datc)\n\nCoefficients:\n  (Intercept)           PVol     TVolMedium  TVolExtensive           bGS7  \n      1.97022        0.05621        1.40467        4.77595        2.03585  \n      bGS8-10  \n      3.12757  \n\n\nIn this example, all four approaches based on BIC for automatic selection agree that the model with all predictors except Age should be used."
  },
  {
    "objectID": "labs/prac12s/index.html#c-picking-a-model",
    "href": "labs/prac12s/index.html#c-picking-a-model",
    "title": "Week 12 Practice Problems: Solutions",
    "section": "2c: Picking a Model",
    "text": "2c: Picking a Model\nBased on your results from the prior questions, what model would you propose to use in modeling PSA?\nSolution:\nIn this example, most approaches seemed to indicate that the model with PVol, TVol, and bGS should be used. In other words, only Age would be excluded from the model. If we had more context, we might choose a different model, but the evidence from both variable and model selection approaches in this specific example seems to indicate this would be the most parsimonious model."
  },
  {
    "objectID": "labs/prac12s/index.html#a-power",
    "href": "labs/prac12s/index.html#a-power",
    "title": "Week 12 Practice Problems: Solutions",
    "section": "3a: Power",
    "text": "3a: Power\nSimulate the data set and fit the full model 1,000 times using set.seed(12521). Summarize the power/type I error for each variable.\nSolution:\nFor this simulation we will create a matrix to store if the resulting p-values are less than 0.05 for all predictors and the intercept, then summarize the proportion of simulations where a significant p-value was found:\n\n\nCode\nset.seed(12521)\nn &lt;- 200\nnsim &lt;- 1000\n\npower_mat &lt;- matrix(nrow=nsim, ncol=11,\n                    dimnames = list(1:nsim, paste0('beta',0:10))) # create matrix to save results in, with additional column for intercept\n\nfor(j in 1:nsim){\n  Xc &lt;- matrix( rnorm(n=200*5, mean=0, sd=1), ncol=5) # simulate continuous predictors\n  Xb &lt;- matrix( rbinom(n=200*5, size=1, prob=0.5), ncol=5) # simulate binary predictors\n  simdat &lt;- as.data.frame( cbind(Xc, Xb) ) # create data frame of predictors to add outcome to\n  simdat$Y &lt;- -2 + 2*simdat$V1 + 1*simdat$V2 + 0.5*simdat$V3 + 2*simdat$V6 + 1*simdat$V7 + 0.5*simdat$V8 + rnorm(n=n, mean=0, sd=5)\n\n  lmj &lt;- lm(Y ~ ., data=simdat)\n  power_mat[j,] &lt;- summary(lmj)$coefficients[,'Pr(&gt;|t|)'] &lt; 0.05 # save if p-values are less than 0.05\n}\n\n# calculate power\ncolMeans(power_mat)\n\n\n beta0  beta1  beta2  beta3  beta4  beta5  beta6  beta7  beta8  beta9 beta10 \n 0.630  0.999  0.794  0.301  0.047  0.042  0.789  0.306  0.122  0.043  0.067 \n\n\nFrom our output, we see that we have 99.9% power for \\(\\beta_1 = 2\\), 79.4% power when the continuous predictor effect size decreases to \\(\\beta_2 = 1\\), and only 30.1% when \\(\\beta_3=0.5\\). For our categorical predictors there is lower power with 78.9% for \\(\\beta_{6} = 2\\), 30.6% for \\(\\beta_{7}=1\\), and 12.2% for \\(\\beta_{8}=0.5\\). Our covariates with no effect (i.e., \\(\\beta_i = 0\\)) are generally around 5%, and the intercept has 63.0% power."
  },
  {
    "objectID": "labs/prac12s/index.html#b-the-best-model",
    "href": "labs/prac12s/index.html#b-the-best-model",
    "title": "Week 12 Practice Problems: Solutions",
    "section": "3b: The “Best” Model",
    "text": "3b: The “Best” Model\nSimulate the data set 1,000 times using set.seed(12521). For each data set identify the variables included in the optimal model for AIC, AICc, BIC, adjusted R2, backward selection, forward selection, stepwise selection from the null model, and stepwise selection from the full model. For model selection criteria, select the model that minimizes the criterion (i.e., we will ignore if other models might have fewer predictors but only a slightly larger criterion for sake of automating the simulation). For automatic selection models use BIC.\nSummarize both how often the “true” model is selected from each approach (i.e., with the 3 continuous and 3 categorical predictors that are non-null), as well as how often each variable was selected more generally. Does one approach seem better overall? On average?\nSolution:\nWith 10 predictors, there are \\(2 ^ {10} = 1024\\) possible models. In order to manually fit this many models for each simulation, it will likely take more time than our previous simulations from class. We can help streamline the process by writing a function to apply for each simulated data set and return the desired information for the optimally identified model:\n\n\nCode\n# Write function based on exercise 2 to apply to each simulated data set\noptimal_model &lt;- function(datopt, simnum){\n## Function to calculate AIC, AICc, BIC, adjusted R^2, and automatic selection models based on BIC  \n# datopt: dataframe with outcome Y and all other columns for predictors\n# simnum: simulation number for tracking purposes\n    \n  # create data frame with all combinations of predictors\n  all_subsets &lt;- expand.grid( rep(list(c(0,1)), ncol(datopt)-1) )\n  colnames(all_subsets) &lt;- paste0('V',1:(ncol(datopt)-1))\n  \n  # create object to save results in\n  res_mat &lt;- all_subsets\n  res_mat[,c('aic','aicc','bic','r2')] &lt;- NA\n\n  # loop through each combination and save results\n  for( i in 1:nrow(all_subsets)){\n    \n    rhs_lm &lt;- paste0( colnames(all_subsets)[which(all_subsets[i,]==1)], collapse = '+') # automatically extract included variables to place onto right hand side of lm() equation\n    \n    if( rhs_lm == '' ){\n      lm_i &lt;- lm( Y ~ 1, data=datopt )\n    }else{\n      lm_i &lt;- lm( as.formula(paste0('Y ~ ', rhs_lm)), data=datopt )\n    }\n    \n    res_mat[i,'aic'] &lt;- AIC(lm_i, k=2)\n    numpred &lt;- length(coef(lm_i))-1 # calculate the number of predictors to use for calculation of AICc\n    res_mat[i,'aicc'] &lt;- AIC(lm_i, k=2) + (2*numpred^2 + 2*numpred) / (nrow(datopt) - numpred - 1)\n    res_mat[i,'bic'] &lt;- BIC(lm_i)\n    res_mat[i,'r2'] &lt;- summary(lm_i)$adj.r.squared\n  }\n  \n  full_mod &lt;- lm(Y ~ ., data=datopt)\n  null_mod &lt;- lm(Y ~ 1, data=datopt)\n  n &lt;- nrow(datopt)\n  \n  # backward selection\n  backward &lt;- step(full_mod, \n                   direction='backward', \n                   k=log(n), \n                   trace=0)\n  \n  # forward selection\n  forward &lt;- step(null_mod, \n                  direction='forward',\n                  scope = as.formula(paste0('~ ', paste0('V',1:(ncol(datopt)-1), collapse='+'))),\n                  k=log(n),\n                  trace=0)\n  \n  # stepwise selection starting with null model\n  stepwise_null &lt;- step(null_mod, \n                        direction='both',\n                        scope = as.formula(paste0('~ ', paste0('V',1:(ncol(datopt)-1), collapse='+'))),\n                        k=log(n),\n                        trace=0)\n\n  # stepwise selection starting with full model\n  stepwise_full &lt;- step(full_mod, \n                        direction='both',\n                        k=log(n),\n                        trace=0)\n\n  # collect results to return\n  \n  aic_opt &lt;- res_mat[which( res_mat$aic == min(res_mat$aic)),paste0('V',1:10)]\n  aicc_opt &lt;- res_mat[which( res_mat$aicc == min(res_mat$aicc)),paste0('V',1:10)]\n  bic_opt &lt;- res_mat[which( res_mat$bic == min(res_mat$bic)),paste0('V',1:10)]\n  r2_opt &lt;- res_mat[which( res_mat$r2 == max(res_mat$r2)),paste0('V',1:10)]\n  \n  back_opt &lt;- paste0('V',1:10) %in% names(backward$coefficients)\n  fw_opt &lt;- paste0('V',1:10) %in% names(forward$coefficients)\n  stepnull_opt &lt;- paste0('V',1:10) %in% names(stepwise_null$coefficients)\n  stepfull_opt &lt;- paste0('V',1:10) %in% names(stepwise_full$coefficients)\n  \n  res_ret &lt;- cbind(data.frame( simnum = simnum, method = c('aic','aicc','bic','r2','forward','backward','stepwise_null','stepwise_full')),\n                   rbind( aic_opt, aicc_opt, bic_opt, r2_opt, back_opt, fw_opt, stepnull_opt, stepfull_opt) )\n}\n\n\nNow, with a function to implement the identification of the optimal model, we can implement our simulation:\n\n\nCode\nset.seed(12521)\n\nnsim &lt;- 1000\nresj &lt;- NULL\n\nfor(j in 1:nsim){\n  Xc &lt;- matrix( rnorm(n=200*5, mean=0, sd=1), ncol=5) # simulate continuous predictors\n  Xb &lt;- matrix( rbinom(n=200*5, size=1, prob=0.5), ncol=5) # simulate binary predictors\n  simdat &lt;- as.data.frame( cbind(Xc, Xb) ) # create data frame of predictors to add outcome to\n  simdat$Y &lt;- -2 + 2*simdat$V1 + 1*simdat$V2 + 0.5*simdat$V3 + 2*simdat$V6 + 1*simdat$V7 + 0.5*simdat$V8 + rnorm(n=n, mean=0, sd=5)\n\n  resj &lt;- rbind(resj, optimal_model(datopt = simdat, simnum = j))\n}\n\n\nWith our 1000 simulation results, we can summarize the answer to our question by variable and for the overall model. For the proportion of times each variable was included in the model we see:\n\n\nCode\nlibrary(doBy)\nperc_mean &lt;- function(x) round(100 * mean(x), 1) # create function to present % of simulations including variable\nsummaryBy( V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 ~ method, data=resj, FUN=perc_mean)\n\n\n         method V1.perc_mean V2.perc_mean V3.perc_mean V4.perc_mean\n1           aic        100.0         91.5         51.7         16.7\n2          aicc        100.0         90.9         50.5         14.9\n3      backward         99.9         69.0         20.0          1.9\n4           bic         99.9         69.5         20.2          1.9\n5       forward         99.9         69.6         20.2          1.9\n6            r2        100.0         95.7         68.4         34.0\n7 stepwise_full         99.9         69.6         20.2          1.9\n8 stepwise_null         99.9         68.9         19.9          1.9\n  V5.perc_mean V6.perc_mean V7.perc_mean V8.perc_mean V9.perc_mean\n1         16.2         90.8         53.3         27.9         15.8\n2         15.6         90.2         52.4         25.7         14.3\n3          1.5         67.8         20.3          6.3          2.0\n4          1.5         68.1         20.5          6.5          2.0\n5          1.6         68.4         20.9          6.6          2.1\n6         31.6         95.8         68.8         43.6         31.2\n7          1.6         68.4         20.9          6.6          2.1\n8          1.5         67.8         20.3          6.3          2.0\n  V10.perc_mean\n1          20.5\n2          18.5\n3           4.2\n4           4.2\n5           4.2\n6          35.2\n7           4.3\n8           4.2\n\n\nWe see a trade-off, where AIC, AICc, and the adjusted \\(R^2\\) have higher rates of including variables. This is true both for those which have an effect of 2, 1, or 0.5, as well as the null variables (V4, V5, V9, V10).\nIn contrast, BIC and the automatic procedures based on BIC are all fairly similar. While they almost always detect V1, the have lower rates of detecting V2, V3, V6, V7, and V8.\nThese results suggest that continuous variables are more often identified than categorical predictors, which makes sense given that continuous predictors have more information contained in their values (since they aren’t restricted to be 0/1). Further, the adjusted \\(R^2\\) is overly optimistic and picks null variables at a much higher rate, with the AIC and AICc being similar optimistic.\nWe can also summarize the proportion of models each approach identified that perfectly aligned with our true data generating scenario:\n\n\nCode\nresj$true_model_picked &lt;- sum(resj[,paste0('V',1:10)] == c(1,1,1,0,0,1,1,1,0,0))==10\nsummaryBy( true_model_picked ~ method, data=resj)\n\n\n         method true_model_picked.mean\n1           aic                      0\n2          aicc                      0\n3      backward                      0\n4           bic                      0\n5       forward                      0\n6            r2                      0\n7 stepwise_full                      0\n8 stepwise_null                      0\n\n\nWe see that the true model is never identified across any simulations. This may be due to a few different contributing factors:\n\nEach simulation is based on an underlying truth, but by chance each simulation can differ from the truth.\nFrom our power calculation, we know that those with 1 and 0.5 effect sizes have lower power. If we designed a simulation where all non-null coefficients had at least 80% power, we likely would see more scenarios that correctly identified the true model.\n\nHowever, this is useful to recall the old adage by George Box, a British statistician, that “all models are wrong, but some are useful.” While no single approach guarantees the “true” data generating model, each likely is useful for the given sample and we can use our variable-specific results to identify the method(s) that might reflect our comfort level with including null variables (e.g., AIC) or excluding non-null variables with lower effect sizes (e.g., BIC)."
  },
  {
    "objectID": "labs/prac13s/index.html",
    "href": "labs/prac13s/index.html",
    "title": "Week 13 Practice Problems: Solutions",
    "section": "",
    "text": "This page includes the solutions to the optional practice problems for the given week. If you want to see a version without solutions please click here. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises are focusing on some of our “advanced” topics including segmented (piecewise) regression, quantile regression, and splines to model nonlinear trends."
  },
  {
    "objectID": "labs/prac14s/index.html",
    "href": "labs/prac14s/index.html",
    "title": "Week 14 Practice Problems: Solutions",
    "section": "",
    "text": "This page includes the solutions to the optional practice problems for the given week. If you want to see a version without solutions please click here. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises are focusing on implementing and interpreting a linear regression model using Bayesian approaches. We leverage the same data set from the MLR practice problems to help compare results with our frequentist approaches."
  },
  {
    "objectID": "labs/prac2s/index.html",
    "href": "labs/prac2s/index.html",
    "title": "Week 2 Practice Problems: Solutions",
    "section": "",
    "text": "This page includes the solutions to the optional practice problems for the given week. If you want to see a version without solutions please click here. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course."
  },
  {
    "objectID": "labs/prac2s/index.html#possible-solution-1-for-loop-with-null-objects-looping-through-an-index-and-concatenation",
    "href": "labs/prac2s/index.html#possible-solution-1-for-loop-with-null-objects-looping-through-an-index-and-concatenation",
    "title": "Week 2 Practice Problems: Solutions",
    "section": "Possible Solution 1: for loop with NULL objects, looping through an index, and concatenation",
    "text": "Possible Solution 1: for loop with NULL objects, looping through an index, and concatenation\n\n\nCode\nset.seed(515) #for reproducibility\n\nns &lt;- seq(from=100, to=100000, by=100) #increasing sample size for vector\n\n# Initialize two objects to store mean and variance results in\nmean_res &lt;- NULL\nvar_res &lt;- NULL\n\nfor( i in 1:length(ns) ){\n  # simulate the data\n  dat_sim &lt;- rpois( n=ns[i], lambda=3.5 )\n  \n  # calculate the bias for the mean and variance\n  mean_bias &lt;- mean(dat_sim) - 3.5\n  var_bias &lt;- var(dat_sim) - 3.5\n  \n  # concatenate the results\n  mean_res &lt;- c(mean_res, mean_bias)\n  var_res &lt;- c(var_res, var_bias)\n}\n\n# Create plots\nplot( x=ns, y=mean_res, xlab='Sample Size', ylab='Bias', main='Bias for Poisson Mean', type='l')\n\n\n\n\n\nCode\nplot( x=ns, y=var_res, xlab='Sample Size', ylab='Bias', main='Bias for Poisson Variance', type='l')"
  },
  {
    "objectID": "labs/prac2s/index.html#possible-solution-2-for-loop-with-results-stored-in-matrix-looping-through-sample-size-vector",
    "href": "labs/prac2s/index.html#possible-solution-2-for-loop-with-results-stored-in-matrix-looping-through-sample-size-vector",
    "title": "Week 2 Practice Problems: Solutions",
    "section": "Possible Solution 2: for loop with results stored in matrix, looping through sample size vector",
    "text": "Possible Solution 2: for loop with results stored in matrix, looping through sample size vector\nWhile the lab focused on vectors, if you wanted to try and leverage the fact we are calculating two different biases you may want to store the results in one object (where we can also name the rows with the sample sizes, and the columns as “mean” or “variance”):\n\n\nCode\nset.seed(515) #for reproducibility\n\nns &lt;- seq(from=100, to=100000, by=100) #increasing sample size for vector\n\n# Initialize matrix\nmat_res &lt;- matrix( nrow=length(ns), ncol=2, dimnames = list( ns, c('mean','variance') ) ) #note: the dimnames argument allows us to name the rows, columns (in that order)\n\nfor( j in ns ){\n  # simulate the data\n  dat_sim &lt;- rpois( n=j, lambda=3.5 )\n  \n  # calculate the bias for the mean and variance\n  mean_bias &lt;- mean(dat_sim) - 3.5\n  var_bias &lt;- var(dat_sim) - 3.5\n  \n  # concatenate the results\n  mat_res[paste0(j),'mean'] &lt;- mean_bias\n  mat_res[paste0(j),'variance'] &lt;- var_bias\n}\n\n# Create plots\nplot( x=rownames(mat_res), y=mat_res[,'mean'], xlab='Sample Size', ylab='Bias', main='Bias for Poisson Mean', type='l')\n\n\n\n\n\nCode\nplot( x=rownames(mat_res), y=mat_res[,'variance'], xlab='Sample Size', ylab='Bias', main='Bias for Poisson Variance', type='l')\n\n\n\n\n\nAdmittedly, it may have been even better to add a column to the matrix for the sample size instead of calling rownames(mat_res) for the plots, but this again illustrates how many ways we can arrive at the same result."
  },
  {
    "objectID": "labs/prac2s/index.html#a",
    "href": "labs/prac2s/index.html#a",
    "title": "Week 2 Practice Problems: Solutions",
    "section": "2a",
    "text": "2a\nReproducibly simulate a sample of 10,000 from each of the following distributions\n\nNB(\\(r=5, p=0.6\\)), a negative binomial distribution where \\(r\\) is the target number of successful trials and \\(p\\) is the probability of success within each trial (i.e., using the default parameterization in R, see ?dnbinom)\nWeibull(\\(\\lambda=2, k=4\\)) where \\(\\lambda\\) represents the scale parameter and \\(k\\) the shape parameter\n\nSolution:\n\n\nCode\nset.seed(8675309)\nn &lt;- 10000\nnegbin &lt;- rnbinom(n=n, size=5, prob=0.6)\nweibull &lt;- rweibull(n=n, shape=4, scale=2)\n\n\nNote that many distributions have different parameterizations (i.e., ways of writing out the probability mass/density functions and specifying their parameters). Additionally, the arguments in the function may not always match what you think (e.g., the negative binomial has both n and size, where n is the number of simulated experiments and size is the target number of successful trials with the generated random data representing the observed number of total trials in each of your n experiments until you achieve size).\nThese two distributions aren’t ones we will necessarily work with a lot in BIOS 6611, but it can be helpful to have a brief summary:\n\nThe negative binomial distribution is a discrete probability distribution that models the number of failures in a sequence of independent and identically distributed Bernoulli trials before a specified (non-random) number of successes (denoted \\(r\\)) occurs. (Note: the Wikipedia page has a different parameterization which models the number of successes before a given number of failures is observed.)\nThe Weibull distribution is a continuous probability distribution that is often used in reliability/quality engineering, and also seen in survival analysis."
  },
  {
    "objectID": "labs/prac2s/index.html#b",
    "href": "labs/prac2s/index.html#b",
    "title": "Week 2 Practice Problems: Solutions",
    "section": "2b",
    "text": "2b\nDetermine the theoretical mean and standard deviation for each distribution and verify that the generated numbers have approximately the correct mean and standard deviation. Note, you can derive or look-up and cite your source for the theoretical mean and standard deviation.\nSolution:\n\n\nCode\n# Mean and SD for negative binomial\nmean(negbin)\nsd(negbin)\n\n# Mean and SD for Weibull\nmean(weibull)\nsd(weibull)\n\n\nThe theoretical mean for the negative binomial is \\(E(X) = \\frac{r(1-p)}{p} = \\frac{5(1-0.6)}{0.6} = 3.33\\). The standard deviation is \\(SD(X) = \\sqrt{Var(X)} = \\sqrt{\\frac{r(1-p)}{p^2}} = \\sqrt{\\frac{5(1-0.6)}{0.6^2}} = \\sqrt{5.55} = 2.36\\). These equations were pulled directly from the R Documentation (?rnbinom).\nThe theoretical mean for the Weibull is \\(E(X) = \\lambda \\Gamma\\left(1+\\frac{1}{k}\\right) = 2 \\Gamma\\left(1 + \\frac{1}{4} \\right) = 1.81\\), where \\(\\Gamma(x)\\) represents the gamma function:\n\\[ \\Gamma(x) = \\int_{0}^{\\infty} t^{x-1} \\exp(-t) dt. \\]\nThe standard deviation is\n\\[\\begin{align*}\nSD(X) =& \\sqrt{Var(X)} \\\\\n=& \\sqrt{\\lambda^2 \\left[ \\Gamma\\left(1+\\frac{2}{k}\\right) - \\left\\{ \\Gamma\\left(1 + \\frac{1}{k} \\right) \\right\\}^2 \\right]} \\\\\n=& \\sqrt{2^2 \\left[ \\Gamma\\left(1 + \\frac{2}{4} \\right) - \\left\\{ \\Gamma\\left(1 + \\frac{1}{4} \\right) \\right\\}^2 \\right]} \\\\\n=& \\sqrt{0.2586}  \\\\\n=& 0.51\n\\end{align*}\\]\nThese equations for \\(E(X)\\) and \\(SD(X)\\) were also pulled from the R Documentation (?rweibull), but also match the Wikipedia parameterization. A table summarizing our results shows that the estimated mean and SD from the simulated data sets closely matches the true theoretical mean and SD for each distribution.\n\n\n\n\n\n\n\n\n\n\nDistribution\nTheoretical Mean\nTheoretical SD\nSimulation Mean\nSimulation SD\n\n\n\n\nNegative Binomial\n3.33\n2.36\n3.349\n2.37\n\n\nWeibull\n1.81\n0.51\n1.818\n0.506"
  },
  {
    "objectID": "labs/prac2s/index.html#c",
    "href": "labs/prac2s/index.html#c",
    "title": "Week 2 Practice Problems: Solutions",
    "section": "2c",
    "text": "2c\nCreate a histogram and boxplot depicting each of the mock samples.\nSolution:\nUsing the graphics in base R we can create a panel figure to show all 4 plots. There are a variety of functions and packages to do this, but we will use the par(mfrow=c(2,2)) function to specify an overall figure with 2 rows and 2 columns to plot subfigures within:\n\n\nCode\npar(mfrow=c(2,2))\n\nhist(negbin, main='Histogram of negbin')\nhist(weibull, main='Histogram of weibull')\n\nboxplot(negbin, main='Boxplot of negbin')\nboxplot(weibull, main='Boxplot of weibull')\n\n\n\n\n\nWe could also do the same using ggplot2:\n\n\nCode\nlibrary(ggplot2)\nlibrary(patchwork) # for the panel figure\n\np1 &lt;- ggplot() + aes(negbin) + geom_histogram(binwidth=1, color='black', fill='white') + ggtitle('Histogram of negbin')\np2 &lt;- ggplot() + aes(weibull) + geom_histogram(binwidth=0.2, color='black', fill='white') + ggtitle('Histogram of weibull')\n\np3 &lt;- ggplot() + aes(negbin) + geom_boxplot() + ggtitle('Boxplot of negbin')\np4 &lt;- ggplot() + aes(weibull) + geom_boxplot() + ggtitle('Boxplot of weibull')\n\np1 + p2 + p3 + p4"
  },
  {
    "objectID": "labs/prac2s/index.html#a-1",
    "href": "labs/prac2s/index.html#a-1",
    "title": "Week 2 Practice Problems: Solutions",
    "section": "3a",
    "text": "3a\nAssume we are interested in an outcome that has a chi-squared distribution with 2 degrees of freedom (i.e., \\(df=2\\)). Generate and save a vector with 500 sample means (i.e., the mean of five-hundred simulated “experiments”), where each sample mean is from a sample size of 10 simulated from rchisq() in R."
  },
  {
    "objectID": "labs/prac2s/index.html#b-1",
    "href": "labs/prac2s/index.html#b-1",
    "title": "Week 2 Practice Problems: Solutions",
    "section": "3b",
    "text": "3b\nRepeat for sample sizes of n = 20, n = 30, n = 40, and n = 50. It may be helpful to use a for loop or apply statement to tackle parts a and b simultaneously.\nSolution for 2a/2b:\nLet’s start by seeing an example of what this might look like with a for loop:\n\n\nCode\nset.seed(6611) # set seed for reproducibility\nnsim &lt;- 500\nsizeVec &lt;- c(10,20,30,40,50)\nmeanMatrix &lt;- matrix(NA, nrow = 500, ncol = 5) # create matrix to save the results, with each row representing a simulated data set and each column representing a different sample size\ncolnames(meanMatrix) &lt;- sizeVec # name columns with the sample size for use in later summaries\n\nfor(j in 1:5){\n  for(i in 1:nsim){\n    chisqData &lt;- rchisq(n=sizeVec[j], df=2)\n    meanMatrix[i,j] &lt;- mean(chisqData) # save the i-th out of nsim simulations in the i-th row and in the j-th column for its corresponding sample size\n  }\n}\n\nhead(meanMatrix)\n\n\n           10       20       30       40       50\n[1,] 1.216040 2.327663 1.653050 1.787541 2.144370\n[2,] 2.341209 1.624362 1.814585 2.141050 2.024248\n[3,] 2.449218 1.499997 2.162688 2.297704 1.866892\n[4,] 2.852377 1.551701 1.786574 3.001661 1.903967\n[5,] 2.653542 2.332737 1.921918 2.001974 2.022062\n[6,] 2.000464 1.662547 2.120618 2.455170 1.600771\n\n\nWe could also implement this using sapply by creating a function to generate the 500 simulated means:\n\n\nCode\nset.seed(6611) # set seed for reproducibility\nnsim &lt;- 500\nsizeVec &lt;- c(10,20,30,40,50)\n\nchisq_sim_mean &lt;- function(nsim, n, df=2){\n## Writing a short function to simulate chi-squared data to calculate the mean for  \n# nsim: number of simulations to run and return mean for\n# n: sample size to generate\n# df: degrees of freedom for the chi-squared distribution, default is 2\n\n  sim_dat &lt;- matrix(rchisq(n=n*nsim, df=df), ncol=nsim, byrow=FALSE) # simulate all observations and save in nsim columns to calculate the mean for, we specify byrow=FALSE so it will match our for loop approach above\n  colMeans(sim_dat) # if we don't specify something to return, the last object is generally used by default\n}\n\nmeanMatrix_sapply &lt;- sapply(sizeVec, function(x) chisq_sim_mean(nsim=nsim, n=x))\n\nhead(meanMatrix_sapply)\n\n\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 1.216040 2.327663 1.653050 1.787541 2.144370\n[2,] 2.341209 1.624362 1.814585 2.141050 2.024248\n[3,] 2.449218 1.499997 2.162688 2.297704 1.866892\n[4,] 2.852377 1.551701 1.786574 3.001661 1.903967\n[5,] 2.653542 2.332737 1.921918 2.001974 2.022062\n[6,] 2.000464 1.662547 2.120618 2.455170 1.600771"
  },
  {
    "objectID": "labs/prac2s/index.html#c-1",
    "href": "labs/prac2s/index.html#c-1",
    "title": "Week 2 Practice Problems: Solutions",
    "section": "3c",
    "text": "3c\nCalculate the mean and standard deviation associated with each of the five sets of \\(\\bar{x}\\) values.\nSolution:\nSince we saved our results as a matrix above, we can easily use some of the optimized functions for applying functions to matrices. For example, the matrixStats package includes lots of options, but some are already included in base R (e.g., colMeans is included but colSds is not for calculating the mean and SD, respectively, for each column in a matrix).\n\n\nCode\nlibrary(matrixStats)\n\n\n\nAttaching package: 'matrixStats'\n\n\nThe following object is masked from 'package:dplyr':\n\n    count\n\n\nCode\ncolMeans(meanMatrix)\n\n\n      10       20       30       40       50 \n1.973315 2.028235 1.987795 2.003727 1.987145 \n\n\nCode\ncolSds(meanMatrix) # we need to load the matrixStats package for this calculation, notice how it doesn't carry the column names through for each sample size\n\n\n       10        20        30        40        50 \n0.6561354 0.4748230 0.3465965 0.3019478 0.2693243 \n\n\nIt is also possible to calculate the mean and SD by using the apply function:\n\n\nCode\napply(meanMatrix,2,mean)\n\n\n      10       20       30       40       50 \n1.973315 2.028235 1.987795 2.003727 1.987145 \n\n\nCode\napply(meanMatrix,2,sd)\n\n\n       10        20        30        40        50 \n0.6561354 0.4748230 0.3465965 0.3019478 0.2693243"
  },
  {
    "objectID": "labs/prac2s/index.html#d",
    "href": "labs/prac2s/index.html#d",
    "title": "Week 2 Practice Problems: Solutions",
    "section": "3d",
    "text": "3d\nCreate histograms of the sampling distribution of the mean, for each sample size n. Provide meaningful labeling (i.e., include a title and label the relevant axes).\nSolution:\nFor this example we’ll start with using base R graphics:\n\n\nCode\npar( mfrow=c(2,3) )\ninvisible( \n  sapply(1:5, function(x) hist(meanMatrix[,x], xlim=c(0,5), breaks=seq(-0.1,5.1,by=0.2), xlab='Sample Mean', main=paste0('n=',sizeVec[x]) ) ) \n) #invisible() suppresses the hist() function output to just provide the plots\n\n\n\n\n\nWe can do something similar with ggplot2 leveraged by creating a function and using lapply based on inspiration from various stack overflow posts:\n\n\nCode\nlibrary(ggplot2)\nlibrary(Rmisc) # to use multiplot function\n\nhistogram_data_column &lt;- function(matrix, column_num, column_label){\n## Transform the columns of a data frame into histograms\n# matrix: matrix to plot object from\n# column_num: column number to plot\n# column_label: label for the given column\n    ggplot() + aes(matrix[,column_num]) + \n        geom_histogram(binwidth=0.2, color='black', fill='white') +\n        ggtitle(paste0('n=',column_label)) + \n        xlab('Sample Mean') + xlim(0,5)\n}\n\nggplot_histograms &lt;- lapply(1:5, function(x) histogram_data_column(column_num=x, matrix = meanMatrix, column_label=sizeVec[x]))\n\nmultiplot(plotlist = ggplot_histograms, layout=matrix(c(1,2,3,4,5,NA), nrow=2, byrow=T))"
  },
  {
    "objectID": "labs/prac2s/index.html#e",
    "href": "labs/prac2s/index.html#e",
    "title": "Week 2 Practice Problems: Solutions",
    "section": "3e",
    "text": "3e\nIs there a value of the sample size n (i.e., 10, 20, 30, 40, or 50) where the distributions begin to look normal?\nSolution:\nThis is somewhat subjective since each person may find different outliers or points they find concerning. For example, at \\(n=30\\) is seems that our distribution is fairly normal. But one could claim \\(n=20\\) looks normal “enough” or even that the slight right skew even at \\(n=50\\) is concerning enough that perhaps a larger sample size is needed."
  },
  {
    "objectID": "labs/prac3s/index.html",
    "href": "labs/prac3s/index.html",
    "title": "Week 3 Practice Problems: Solutions",
    "section": "",
    "text": "This page includes the solutions to the optional practice problems for the given week. If you want to see a version without solutions please click here. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises focus on subsetting data objects and identifying relationships between the various assumptions in power calculations."
  },
  {
    "objectID": "labs/prac3s/index.html#a",
    "href": "labs/prac3s/index.html#a",
    "title": "Week 3 Practice Problems: Solutions",
    "section": "1a",
    "text": "1a\nRun the following code to combine a few state-related data sets that are part of R’s available data sets:\n\n\nCode\nstates &lt;- data.frame(state.x77, state.region, state.abb)"
  },
  {
    "objectID": "labs/prac3s/index.html#b",
    "href": "labs/prac3s/index.html#b",
    "title": "Week 3 Practice Problems: Solutions",
    "section": "1b",
    "text": "1b\nCalculate the mean (SD) life expectancy by state region.\nSolution:\n\n\nCode\nregion_vec &lt;- unique( states$state.region ) # create vector of unique regions in our data\n\nregion_sum &lt;- matrix( nrow=length(region_vec), ncol=2, dimnames=list(region_vec, c('mean','sd')))\n\nfor( i in region_vec ){\n  region_sum[i, 'mean'] &lt;- mean(states[ which(states$state.region==i), 'Life.Exp'])\n  region_sum[i, 'sd'] &lt;- sd(states[ which(states$state.region==i), 'Life.Exp'])  \n}\n\nregion_sum\n\n\n                  mean        sd\nSouth         69.70625 1.0221994\nWest          71.23462 1.3519715\nNortheast     71.26444 0.7438769\nNorth Central 71.76667 1.0367285"
  },
  {
    "objectID": "labs/prac3s/index.html#c",
    "href": "labs/prac3s/index.html#c",
    "title": "Week 3 Practice Problems: Solutions",
    "section": "1c",
    "text": "1c\nSubset the four corner states (Utah, Colorado, Arizona, and New Mexico) by row name. Which state has the largest population? Which state has the lowest high school graduation rate?\nSolution:\n\n\nCode\nstates[c('Utah','Colorado','Arizona','New Mexico'),]\n\n\n           Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\nUtah             1203   4022        0.6    72.90    4.5    67.3   137  82096\nColorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nNew Mexico       1144   3601        2.2    70.32    9.7    55.2   120 121412\n           state.region state.abb\nUtah               West        UT\nColorado           West        CO\nArizona            West        AZ\nNew Mexico         West        NM\n\n\nColorado has the largest population, whereas New Mexico has the lowest high school graduation rate."
  },
  {
    "objectID": "labs/prac3s/index.html#d",
    "href": "labs/prac3s/index.html#d",
    "title": "Week 3 Practice Problems: Solutions",
    "section": "1d",
    "text": "1d\nSubset states that either have an area greater than 90,000 miles\\(^2\\) or a percent of high school graduates below 50%. How many states meet this criteria?\nSolution:\n\n\nCode\nstates_sub1 &lt;- states[which(states$Area&gt;90000 | states$HS.Grad&lt;50),]\ndim( states_sub1 ) # check the number of rows and columns\n\n\n[1] 23 10\n\n\n23 states meet this criteria."
  },
  {
    "objectID": "labs/prac3s/index.html#e",
    "href": "labs/prac3s/index.html#e",
    "title": "Week 3 Practice Problems: Solutions",
    "section": "1e",
    "text": "1e\nSubset states that have an area greater than 90,000 miles\\(^2\\) and a percent of high school graduates below 50%. How many states meet this criteria?\nSolution:\n\n\nCode\nstates_sub2 &lt;- states[which(states$Area&gt;90000 & states$HS.Grad&lt;50),]\ndim( states_sub2 ) # check the number of rows and columns\n\n\n[1]  1 10\n\n\nCode\nstates_sub2\n\n\n      Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\nTexas      12237   4188        2.2     70.9   12.2    47.4    35 262134\n      state.region state.abb\nTexas        South        TX\n\n\n1 state (Texas) meets this criteria."
  },
  {
    "objectID": "labs/prac3s/index.html#f",
    "href": "labs/prac3s/index.html#f",
    "title": "Week 3 Practice Problems: Solutions",
    "section": "1f",
    "text": "1f\nAre there any states where the mean number of days with a minimum temperature below freezing is above 100, the murder rate is greater than 10 per 100,000 population, and the rate of illiteracy is below 1%?\nSolution:\n\n\nCode\nstates[ which(states$Frost&gt;100 & states$Murder&gt;10 & states$Illiteracy&lt;1),]\n\n\n         Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\nIllinois      11197   5107        0.9    70.14   10.3    52.6   127  55748\nMichigan       9111   4751        0.9    70.63   11.1    52.8   125  56817\nNevada          590   5149        0.5    69.03   11.5    65.2   188 109889\n          state.region state.abb\nIllinois North Central        IL\nMichigan North Central        MI\nNevada            West        NV\n\n\nYes, Illinois, Michigan, and Nevada all meet the criteria."
  },
  {
    "objectID": "labs/prac4s/index.html",
    "href": "labs/prac4s/index.html",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "",
    "text": "This page includes the solutions to the optional practice problems for the given week. If you want to see a version without solutions please click here. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises are focusing on the various diagnostic testing summaries and categorical data analyses covered in the lecture videos for this week. There is a lot of different material covered, so plenty of practice exercises are provided below to give you a wide range of exposure to the topics."
  },
  {
    "objectID": "labs/prac4s/index.html#a",
    "href": "labs/prac4s/index.html#a",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "1a",
    "text": "1a\nCreate a 2x2 table for arthroplasty knee (subset based on ahrq_ccs) that summarizes the in-hospital complication rate (complication) for surgeries by time of day (hour less than 12 for morning).\nSolution:\nFirst we will subset our data to those with arthroplasty knee surgery:\n\n\nCode\ndat1s &lt;- dat1[which(dat1$ahrq_ccs=='Arthroplasty knee'),]\n\n\nNext we will generate our 2x2 table. However, one thing to note is that creating a table for “character” variables is that R will automatically put them into an alphabetical order (or it will prioritize numeric if you start a variable with “1_foxes”, “2_aardvarks”, “3_belugawhales”, etc.).\n\n\nCode\ntab1_nofactor &lt;- table(morning = dat1s$hour&lt;12, complication = dat1s$complication )\ntab1_nofactor\n\n\n       complication\nmorning    0    1\n  FALSE  628   64\n  TRUE  2474  213\n\n\nNotice how this has our -/- in the top-left cell. If we want to set up the table in a way that mirrors our notes (with +/+ in the top left and -/- in the bottom right), we can tell R this information using factor():\n\n\nCode\ntab1_factor &lt;- table(morning = factor(dat1s$hour&lt;12, levels=c(TRUE,FALSE)), complication = factor(dat1s$complication, levels=c(1,0)) )\ntab1_factor\n\n\n       complication\nmorning    1    0\n  TRUE   213 2474\n  FALSE   64  628\n\n\nNote this table looks more like what we’ve seen in our notes (and enables us to directly use the formulas we discussed or apply functions in R).\nAs a coding note, in the above tab1_factor object we could have also defined the factor(...) on its own line instead of combining everything into one line of code."
  },
  {
    "objectID": "labs/prac4s/index.html#b",
    "href": "labs/prac4s/index.html#b",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "1b",
    "text": "1b\nCalculate the risk difference, risk ratio, and odds ratio with 95% CIs by hand comparing morning surgeries versus afternoon surgeries.\nSolution:\nRisk difference:\n\\(p_1 = p_{\\text{morning}} = \\frac{a}{n_1} = \\frac{213}{213+2474} = \\frac{213}{2687} = 0.079\\)\n\\(p_2 = p_{\\text{afternoon}} = \\frac{c}{n_2} = \\frac{64}{64+628} = \\frac{64}{692} = 0.092\\)\n\\(p_1 - p_2 = 0.079 - 0.092 = -0.013\\)\n\\(p = \\frac{a+c}{n_1 + n_2} = \\frac{213+64}{2687+692} = \\frac{277}{3379} = 0.082\\)\n\\(SE(p_1-p_2) = \\sqrt{p(1-p)\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)} = \\sqrt{0.082(1-0.082)\\left(\\frac{1}{2687}+\\frac{1}{692}\\right)} = 0.012\\)\n95% CI: \\((p_1-p_2) \\pm Z_{1-\\frac{\\alpha}{2}} SE(p_1-p_2) = -0.013 \\pm Z_{0.975}(0.012) = -0.013 \\pm 1.96(0.012) = (-0.037, 0.011)\\)\nRD Interpretation: The estimated risk difference is -1.3% (95% CI: -3.7% to 1.1%). Therefore the complication rate is 1.3% lower in the morning compared to the afternoon, however this is not significant since our 95% CI includes the null risk difference of 0%.\nRisk ratio:\n\\(RR = \\frac{p_1}{p_2} = \\frac{0.079}{0.092} = 0.859\\)\nThe 95% CI is calculated on the natural log scale, then transformed back to our original scale by exponentiating the bounds:\n\\(SE[\\log(RR)] = \\sqrt{\\frac{b}{a n_1} + \\frac{d}{c n_2}} = \\sqrt{\\frac{2474}{213 \\times 2687} + \\frac{628}{64 \\times 692}} = 0.136\\)\n95% CI: \\(\\exp\\left\\{ \\log(RR) \\pm Z_{0.975} SE[\\log(RR)] \\right\\} = \\exp\\left\\{ \\log(0.859) \\pm 1.96 \\times 0.136 \\right\\} = \\exp\\left\\{(-0.419, 0.115) \\right\\} = (0.658, 1.122)\\)\nRR Interpretation: The estimated risk ratio is 0.859 (95% CI: 0.658 to 1.122). Therefore morning procedures are a protective factor compared to afternoon procedures (i.e., RR&lt;1), however not significantly so since the 95% CI includes the null value of RR=1. Specifically, a morning arthroplasty knee surgery is 0.859 times as likely to have a complication than an afternoon surgery, but this is not significant.\nOdds ratio:\n\\(OR = \\frac{\\frac{p_1}{1-p_1}}{\\frac{p_2}{1-p_2}} = \\frac{ad}{bc} = \\frac{213 \\times 628}{2474 \\times 64} = 0.844\\)\nThe 95% CI is calculated on the natural log scale, then transformed back to our original scale by exponentiating the bounds:\n\\(SE[\\log(OR)] = \\sqrt{\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}} = \\sqrt{\\frac{1}{213} + \\frac{1}{2474} + \\frac{1}{64} + \\frac{1}{628}} = 0.149\\)\n95% CI: \\(\\exp\\left\\{ \\log(OR) \\pm Z_{0.975} SE[\\log(OR)] \\right\\} = \\exp\\left\\{ \\log(0.844) \\pm 1.96 \\times 0.149 \\right\\} = \\exp\\left\\{(-0.462, 0.122) \\right\\} = (0.630, 1.130)\\)\nOR Interpretation: The estimated odds ratio is 0.844 (95% CI: 0.630 to 1.130). Therefore morning procedures have a lower odds of complications than afternoon procedures (i.e., OR&lt;1), however not significantly so since the 95% CI includes the null value of OR=1. Specifically, the odds of having a complication with morning arthroplasty knee surgeries are 0.844 times the odds of having complications for afternoon surgeries, but this is not significant."
  },
  {
    "objectID": "labs/prac4s/index.html#c",
    "href": "labs/prac4s/index.html#c",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "1c",
    "text": "1c\nCheck your calculations by using a package in R (e.g., epi.2by2() in the epiR package).\nSolution:\n\n\nCode\nlibrary(epiR)\nepi.2by2(tab1_factor)\n\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          213         2474       2687        7.93 (6.93 to 9.01)\nExposed -           64          628        692       9.25 (7.20 to 11.66)\nTotal              277         3102       3379        8.20 (7.29 to 9.17)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 0.86 (0.66, 1.12)\nInc odds ratio                                 0.84 (0.63, 1.13)\nAttrib risk in the exposed *                   -1.32 (-3.71, 1.07)\nAttrib fraction in the exposed (%)            -16.67 (-52.32, 10.63)\nAttrib risk in the population *                -1.05 (-3.40, 1.30)\nAttrib fraction in the population (%)         -12.82 (-38.49, 8.09)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 1.277 Pr&gt;chi2 = 0.258\nFisher exact test that OR = 1: Pr&gt;chi2 = 0.276\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\nRecall, the Attrib risk * is our risk difference (\\(\\times 100\\), i.e., presented as a percent). Further our estimates match (rounding excepted) for the point estimates and confidence intervals."
  },
  {
    "objectID": "labs/prac4s/index.html#d",
    "href": "labs/prac4s/index.html#d",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "1d",
    "text": "1d\nDiscuss the similarity or the difference between the estimated RR and OR with respect to incidence of complications.\nSolution:\nIn this case the complication rate is \\(\\frac{277}{3379}=0.082\\), or 8.2%. Given the lower incidence of complications, we know that the odds ratio will approximate the risk ratio (i.e., they’re pretty similar with respect to their point estimate and confidence intervals)."
  },
  {
    "objectID": "labs/prac4s/index.html#a-1",
    "href": "labs/prac4s/index.html#a-1",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "2a",
    "text": "2a\nCreate a contingency table summarizing our data.\nSolution:\n\n\nCode\ntab2 &lt;- table( volume = dat2$TVol, recurrence = factor(dat2$Recurrence, levels=c(1,0)) )\ntab2\n\n\n      recurrence\nvolume   1   0\n     1   2  62\n     2  19 134\n     3  30  63\n\n\nIn Exercise 2 we are primarily interested in conducting a test of association on our categorical data, so we could potentially leave the levels as c(0,1) instead of using factor() to coerce the ordering since chisq.test, fisher.test, etc. are invariant to the order of the rows and columns."
  },
  {
    "objectID": "labs/prac4s/index.html#b-1",
    "href": "labs/prac4s/index.html#b-1",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "2b",
    "text": "2b\nDiscuss what test(s) could be used to evaluate the potential association between tumor volume and recurrence.\nSolution:\nBased on what we covered in class, potential candidates include the chi-squared test (with or without continuity correction), Fisher’s exact test, Barnard’s exact test, or McNemar’s test for paired data."
  },
  {
    "objectID": "labs/prac4s/index.html#c-1",
    "href": "labs/prac4s/index.html#c-1",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "2c",
    "text": "2c\nSelect the test you believe is most appropriate. State the null hypothesis (\\(H_0\\)) you are testing.\nSolution:\nSince we do not have paired data, we should not use McNemar’s test. We could use either the chi-squared or Fisher’s exact test, assuming that the chi-squared tests assumption that each expected cell has a count of at least 5. In general, it doesn’t hurt to use the continuity correction, so it seems the chi-squared test with a continuity correction may be most appropriate. The \\(H_0\\) is that there is no association between the tumor volume and cancer recurrence."
  },
  {
    "objectID": "labs/prac4s/index.html#d-1",
    "href": "labs/prac4s/index.html#d-1",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "2d",
    "text": "2d\nImplement the test using R and interpret your result.\nSolution:\n\n\nCode\nchisq.test(tab2)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tab2\nX-squared = 26.985, df = 2, p-value = 1.381e-06\n\n\nThe p-value is &lt;0.001, so we reject \\(H_0\\) and conclude that there is an association between recurrence of the prostate cancer and tumor volume."
  },
  {
    "objectID": "labs/prac4s/index.html#a-2",
    "href": "labs/prac4s/index.html#a-2",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "3a",
    "text": "3a\nFirst, remove all rows from the data frame that are missing vitamin D (VitaminD). One function in R that may be useful is is.na() which returns TRUE or FALSE for each item in a vector if it is missing or not, respectively.\nSolution:\n\n\nCode\ndat3s &lt;- dat3[!is.na(dat3$VitaminD),]\nnrow(dat3s) # check the number of records remaining\n\n\n[1] 62\n\n\nRemoving those with missing vitamin D data leaves only 62 out of the original 2,919 records. We can further check that out of these 62 records, only 4 have an SSI."
  },
  {
    "objectID": "labs/prac4s/index.html#b-2",
    "href": "labs/prac4s/index.html#b-2",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "3b",
    "text": "3b\nCreate an ROC curve for vitamin D (VitaminD) to predict SSI (SSI).\nSolution:\n\n\nCode\nlibrary(pROC)\nroc3 &lt;- roc(SSI ~ VitaminD, data=dat3s)\nplot(roc3)"
  },
  {
    "objectID": "labs/prac4s/index.html#c-2",
    "href": "labs/prac4s/index.html#c-2",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "3c",
    "text": "3c\nCalculate the AUC, does it suggest any potential benefit?\nSolution:\n\n\nCode\nroc3 # print the roc object output to extract the AUC\n\n\n\nCall:\nroc.formula(formula = SSI ~ VitaminD, data = dat3s)\n\nData: VitaminD in 58 controls (SSI 0) &lt; 4 cases (SSI 1).\nArea under the curve: 0.653\n\n\nCode\nauc(roc3) # specifically extract the AUC only\n\n\nArea under the curve: 0.653\n\n\nThe AUC is estimated to be 0.653. Since this is greater than 0.5 it suggests vitamin D levels may be better than random chance (i.e., a coin flip), but is not necessarily a home run as far as biomarkers go (i.e., it isn’t a perfect predictor with AUC=1)."
  },
  {
    "objectID": "labs/prac4s/index.html#d-2",
    "href": "labs/prac4s/index.html#d-2",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "3d",
    "text": "3d\nThe pROC package includes the ci.auc function that can calculate a confidence interval for your AUC. It does this in two ways, using either method=delong or method=bootstrap, where DeLong’s approach is based on asymptotics and the bootstrap is nonparametric. Calculate the 95% confidence interval for the AUC using either method. Based on this interval, if we are interested in testing \\(H_0: AUC=0.5\\) what would we conclude?\nSolution:\n\n\nCode\nci.auc(roc3)\n\n\n95% CI: 0.3089-0.9971 (DeLong)\n\n\nThe 95% CI estimated with DeLong’s method is (0.3089, 0.9971). Since this includes the null value of 0.5, we cannot conclude that the vitamin D is a predictor that is better than random chance.\nNote, in this problem we have both a small sample size after removing the NA values for vitamin D, but also only 4 patients with a surgical site infection. These smaller samples suggest we wouldn’t expect a very tight or narrow confidence interval in the first place."
  },
  {
    "objectID": "labs/prac4s/index.html#a-3",
    "href": "labs/prac4s/index.html#a-3",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "4a",
    "text": "4a\nSubset the data to only include the control group (treat=0).\nSolution:\n\n\nCode\ndat4s &lt;- dat4[which(dat4$treat==0),]"
  },
  {
    "objectID": "labs/prac4s/index.html#b-3",
    "href": "labs/prac4s/index.html#b-3",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "4b",
    "text": "4b\nCreate a 2x2 table summarizing an overweight BMI (i.e., BMI &gt; 25) to predict any throat pain at 30 minutes (i.e., pain &gt; 0).\nSolution:\n\n\nCode\ntab4 &lt;- table( overweight = factor(dat4s$preOp_calcBMI &gt; 25, levels=c(T,F)), pain = factor(dat4s$pacu30min_throatPain &gt; 0, levels=c(T,F))  )\n\ntab4\n\n\n          pain\noverweight TRUE FALSE\n     TRUE    24    44\n     FALSE   18    30"
  },
  {
    "objectID": "labs/prac4s/index.html#c-3",
    "href": "labs/prac4s/index.html#c-3",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "4c",
    "text": "4c\nCalculate the sensitivity and specificity by hand from our 2x2 table and interpret.\nSolution:\nSensitivity:\n\\(Se = P(T|D) = \\frac{a}{a+c} = \\frac{24}{24+18} = \\frac{24}{42} = 0.571\\)\nSensitivity Interpretation: If someone will have throat pain at 30 minutes, there is a 57.1% probability that their BMI &gt; 25.\nSpecificity:\n\\(Sp = P(\\bar{T}|\\bar{D}) = \\frac{d}{b+d} = \\frac{30}{30+44} = \\frac{30}{74} = 0.405\\)\nSpecificity Interpretation: If someone will not have throat pain at 30 minutes, there is a 40.5% probability that their BMI &lt; 25."
  },
  {
    "objectID": "labs/prac4s/index.html#d-3",
    "href": "labs/prac4s/index.html#d-3",
    "title": "Week 4 Practice Problems: Solutions",
    "section": "4d",
    "text": "4d\nDiscuss if there is any potential use of BMI as a predictor of throat pain in those who are treated with a sugar-water placebo.\nSolution:\nGiven the generally low sensitivity and specificity, there does not seem to be great potential utility of using BMI as a predictor for who will or will not have throat pain. In fact, these results suggest a false negative rate (\\(FNR=1-Se\\)) of 42.9% and a false positive rate (\\(FPR=1-Sp\\)) of 59.5%."
  },
  {
    "objectID": "labs/prac5s/index.html",
    "href": "labs/prac5s/index.html",
    "title": "Week 5 Practice Problems: Solutions",
    "section": "",
    "text": "This page includes the solutions to the optional practice problems for the given week. If you want to see a version without solutions please click here. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises are focusing on implementing bootstrap resampling and permutation testing to evaluate the odds ratio of an estimate."
  },
  {
    "objectID": "labs/prac5s/index.html#exercise-1-bootstrap-confidence-intervals",
    "href": "labs/prac5s/index.html#exercise-1-bootstrap-confidence-intervals",
    "title": "Week 5 Practice Problems: Solutions",
    "section": "Exercise 1: Bootstrap Confidence Intervals",
    "text": "Exercise 1: Bootstrap Confidence Intervals\nEstimate the 95% normal percentile and bootstrap percentile confidence intervals with 10,000 bootstrap samples to describe the variability of our estimate and:\na. compare the resulting CIs to the estimate from epi.2by2\nb. evaluate if the normal percentile CI has acceptable coverage\nc. evaluate if the bootstrap percentile CI has acceptable accuracy\nSolution:\nLet’s start by implementing our bootstrap:\n\n\nCode\nB &lt;- 10^4 #set number of bootstraps\nor_boot &lt;- numeric(B) #initialize vector to store results in\n\nnM &lt;- sum(dat1s$AK_morning==T) #sample size of morning knee procedures\nnA &lt;- sum(dat1s$AK_morning==F) #sample size of afternoon knee procedures\n\nset.seed(1013) #set seed for reproducibility\n\nfor (i in 1:B){\n    morning.boot &lt;- sample(dat1s[which(dat1s$AK_morning==T),'complication'], nM, replace=T)\n    afternoon.boot &lt;- sample(dat1s[which(dat1s$AK_morning==F),'complication'], nM, replace=T)\n    a &lt;- sum( morning.boot==1 )\n    b &lt;- sum( morning.boot==0 )\n    c &lt;- sum( afternoon.boot==1 )\n    d &lt;- sum( afternoon.boot==0 )\n    or_boot[i] &lt;- (a*d)/(b*c)\n}\n\n\nLet’s now visualize the shape of our bootstrap distribution:\n\n\nCode\npar(mfrow=c(1,2)) #create plotting area for 2 figures in one row\n\nhist(or_boot, main='Bootstrap Dist. of OR', xlab='OR')\nqqnorm(or_boot); qqline(or_boot)\n\n\n\n\n\nThe shapes of these plots suggest the odds ratios are NOT normally distributed, but are right skewed. However, recall that in our “by hand” confidence interval calculation we use \\(log(OR)\\), so perhaps we can take a quick detour to see the plots on the log-scale:\n\n\nCode\npar(mfrow=c(1,2)) #create plotting area for 2 figures in one row\n\nhist(log(or_boot), main='Bootstrap Dist. of log(OR)', xlab='log(OR)')\nqqnorm(log(or_boot)); qqline(log(or_boot))\n\n\n\n\n\nThese look more normally distributed on the log scale (hence our use of the transformation).\nNow back from our detour to the plots of log(OR)! Let’s first calculate the 95% normal percentile CI and its coverage:\n\n\nCode\n# Lower limit and coverage\nLL &lt;- mean(or_boot) - 1.96*sd(or_boot)\nLL\n\n\n[1] 0.6860789\n\n\nCode\nsum(or_boot &lt; LL)/B  # Coverage of CI at lower end\n\n\n[1] 0.017\n\n\nCode\n# Upper limit and coverage\nUL &lt;- mean(or_boot) + 1.96*sd(or_boot)\nUL\n\n\n[1] 1.011845\n\n\nCode\nsum(or_boot &gt; UL)/B  # Coverage of CI at upper end \n\n\n[1] 0.0321\n\n\nThen let’s calculate the 95% bootstrap percentile CI and its accuracy:\n\n\nCode\nmean(or_boot) # bootstrap mean OR\n\n\n[1] 0.8489622\n\n\nCode\nmean(or_boot)-obs_or # bias for OR\n\n\n[1] 0.004151132\n\n\nCode\nsd(or_boot) # bootstrap SE\n\n\n[1] 0.08310369\n\n\nCode\n(mean(or_boot)-obs_or) / sd(or_boot) # estimate of accuracy\n\n\n[1] 0.04995123\n\n\nCode\nquantile(or_boot, c(0.025,0.975)) # 95% bootstrap percentile CI\n\n\n    2.5%    97.5% \n0.697339 1.019918 \n\n\nSolution Part a:\nThe 95% normal percentile CI is (0.686, 1.012), the 95% bootstrap percentile CI is (0.697, 1.020), and our 95% CI from epi.2by2 was (0.63, 1.13). Both of these seem to suggest that our bootstrap estimates of the CIs on the OR-scale are biased towards the null compared the 95% confidence interval from epi.2by2 (i.e., 0.686 and 0.697 are closer to 1 than the epi.2by2 estimate of 0.63; 1.012 and 1.020 are closer to 1 than the epi.2by2 estimate of 1.13).\nAs yet another detour, what if we calculated the 95% intervals on the log-scale and then exponentiated back to the OR scale (like we do when we calculate our 95% CI by hand):\n\n\nCode\n### 95% normal percentile CI on the log(OR) scale then exponentiated\n\n# Lower limit and coverage\nlogLL &lt;- mean(log(or_boot)) - 1.96*sd(log(or_boot))\nexpLL &lt;- exp(logLL)\nexpLL\n## [1] 0.6976017\nsum(or_boot &lt; expLL)/B  # Coverage of CI at lower end\n## [1] 0.0252\n\n# Upper limit and coverage\nlogUL &lt;- mean(log(or_boot)) + 1.96*sd(log(or_boot))\nexpUL &lt;- exp(logUL)\nexpUL\n## [1] 1.023345\nsum(or_boot &gt; expUL)/B  # Coverage of CI at upper end\n## [1] 0.0234\n\n### 95% bootstrap percentile CI on the log(OR) scale then exponentiated\nexp( quantile(log(or_boot), c(0.025,0.975)) ) # 95% bootstrap percentile CI\n##     2.5%    97.5% \n## 0.697339 1.019918\n\n\nThere are few interesting points we can draw from this result:\n\nThe normal percentile interval estimated in this way has better coverage (2.52% on the lower tail, 2.34% on the upper tail), and its 95% interval is (0.698, 1.023). So while the interval here and from epi.2by2 still don’t match very well, the coverage is improved when using the \\(log(OR)\\) to estimate the interval. This “disconnect” between calculating the odds ratio at each iteration versus the log odds ratio is a direct application of Jensen’s inequality, where here we see that the exponentiated mean of log(OR) is not equivalent to the mean of the exponentiated log(OR) [i.e., the mean of our OR’s].\nThe bootstrap percentile interval is unchanged from before. This is because when calculating the quantiles, taking the log doesn’t change the ordering or affect our estimates of the mean or standard error like it does in the normal percentile interval. So the estimate for \\(log(OR)\\) and \\(OR\\) have the same ordering (from smallest to largest), and our nonparametric estimate is unaffected.\n\nNOTE: This does not mean that the CI from the epi.2by2 function is “wrong” or that our bootstrap estimates are “wrong”! Just that different approaches (and assumptions behind the approaches) can result in different estimates.\nSolution Part b:\nIn our original estimate on the original scale, the coverage is too low on the lower end (LL) at 1.7% (versus the expected 2.5% if normality held) and too high on the upper end (UL) at 3.21% (vs. 2.5%).\nSolution Part c:\nThe ratio of |bias/SE| is less than 0.10, so we expect the bootstrap percentile CI to have acceptable accuracy."
  },
  {
    "objectID": "labs/prac5s/index.html#exercise-2-permutation-test-p-value",
    "href": "labs/prac5s/index.html#exercise-2-permutation-test-p-value",
    "title": "Week 5 Practice Problems: Solutions",
    "section": "Exercise 2: Permutation Test P-value",
    "text": "Exercise 2: Permutation Test P-value\nImplement a permutation test with 10,000 resamples to estimate a p-value for if our observed OR is significantly different from its null value for:\na. a two-sided p-value.\nb. a one-sided p-value where we hypothesize that mornings have a lower odds of complications compared to the afternoon.\nSolution:\nWe will start by implementing our permutation test:\n\n\nCode\nB &lt;- 10^4 - 1 #set number of times to complete permutation sampling\nresult &lt;- numeric(B)\n\nnM &lt;- sum(dat1s$AK_morning==T) #sample size of morning knee procedures\n\na &lt;- sum( dat1s$AK_morning==T & dat1s$complication==1 )\nb &lt;- sum( dat1s$AK_morning==T & dat1s$complication==0 )\nc &lt;- sum( dat1s$AK_morning==F & dat1s$complication==1 )\nd &lt;- sum( dat1s$AK_morning==F & dat1s$complication==0 )\nobs_or &lt;- (a*d)/(b*c) # the observed OR calculated manually\n\nset.seed(1013) #set seed for reproducibility\npool_dat &lt;- dat1s$complication\n\nfor(j in 1:B){\n    index &lt;- sample(x=1:length(pool_dat), size=nM, replace=F)\n    morning_permute &lt;- pool_dat[index]\n    afternoon_permute &lt;- pool_dat[-index]\n    a &lt;- sum( morning_permute==1 ) # calculate each cell of our 2x2 table to calculate the OR\n    b &lt;- sum( morning_permute==0 )\n    c &lt;- sum( afternoon_permute==1 )\n    d &lt;- sum( afternoon_permute==0 )\n    result[j] &lt;- (a*d)/(b*c)\n}\n\n\nWe finish by visualizing our results:\n\n\nCode\n# Histogram\nhist( result, xlab='OR', \n      main='Permutation Distribution for OR')\nabline( v=obs_or, lty=2, col='blue', lwd=2)\n\n\n\n\n\nSolution Part a:\nBased on our permutation distribution, we will calculate the proportion that would fall into either tail, then multiply the larger value by 2 (to be more conservative):\n\n\nCode\n#note, we take the larger p-value and multiply by 2 (as compared to replacing &lt;= with &gt;)\n((sum(result &lt;= obs_or) + 1)/(B+1))\n\n\n[1] 0.1408\n\n\nCode\n((sum(result &gt;= (1/obs_or)) + 1)/(B+1))\n\n\n[1] 0.1644\n\n\nHere we see that the larger value is 0.1644, so our estimated 2-sided p-value is 0.3288. Therefore we would fail to reject the null hypothesis that the OR=1.\nSolution Part b:\nIn this case, we have a priori specified a one-sided test, so we can evaluate the proportion of our permutation distribution that falls below our observed OR of 0.844 from 2a. Here we see \\(p=0.1408\\), so we still fail to reject \\(H_0\\) that the odds of a complication are different between morning and afternoon in knee surgeries."
  },
  {
    "objectID": "labs/prac6s/index.html",
    "href": "labs/prac6s/index.html",
    "title": "Week 6 Practice Problems: Solutions",
    "section": "",
    "text": "This page includes the solutions to the optional practice problems for the given week. If you want to see a version without solutions please click here. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercise focuses on showing the total sums of squares is indeed equal to the model sums of squares plus the error sums of squares.\n\nExercise 1: Deriving the Sums of Squares\nIn our lecture slides we noted that SStotal = SSmodel + SSerror. For this problem, you will work through the math and perform some algebraic acrobatics to “prove” this is true.\nConsider \\(\\hat{Y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}X_{i}\\). Using material from the lecture slides, we can rewrite this in terms of our residual (\\(\\hat{e}_{i} = Y_{i} - \\hat{Y}_{i}\\)):\n\\(\\begin{aligned} \\frac{\\partial}{\\partial \\beta_{0}} SS_{Error} =& \\frac{\\partial}{\\partial \\beta_{0}} \\left( \\sum_{i=1}^{n} (Y_{i} - \\beta_{0} - \\beta_{1}X_{i})^2 \\right) = 0 \\to \\sum_{i=1}^{n} -2 (Y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}X_{i}) = -2 \\sum_{i=1}^{n} \\hat{e}_{i} = 0 \\\\ \\frac{\\partial}{\\partial \\beta_{1}} SS_{Error} =& \\frac{\\partial}{\\partial \\beta_{1}} \\left( \\sum_{i=1}^{n} (Y_{i} - \\beta_{0} - \\beta_{1}X_{i})^2 \\right) = 0 \\to \\sum_{i=1}^{n} -2 X_{i} (Y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}X_{i}) = -2 \\sum_{i=1}^{n} X_{i} \\hat{e}_{i} = 0 \\end{aligned}\\)\nAdditionally, we can use the following properties/definitions and hint:\n\nNote 1: \\(\\sum_{i=1}^{n} \\hat{e}_{i} = \\sum_{i=1}^{n} (Y_{i} - \\hat{Y}_{i}) = 0\\)\nNote 2: \\(\\sum_{i=1}^{n} X_{i} \\hat{e}_{i} = 0\\)\nNote 3: \\(SS_{Error} = \\sum_{i=1}^{n} (Y_{i} - \\hat{Y}_{i})^2\\)\nNote 4: \\(SS_{Model} = \\sum_{i=1}^{n} (\\hat{Y}_{i} - \\bar{Y})^2\\)\nHint:\n\n\\[\\begin{align}\nSS_{Total} =& \\sum_{i=1}^{n} (Y_{i} - \\bar{Y})^2 \\\\\n=& \\sum_{i=1}^{n} \\left( (Y_{i} - \\hat{Y}_{i}) + (\\hat{Y}_{i} - \\bar{Y}) \\right)^2 \\\\\n=& \\sum_{i=1}^{n} (Y_{i} - \\hat{Y}_{i})^2 + \\sum_{i=1}^{n} (\\hat{Y}_{i} - \\bar{Y})^2 + 2 \\sum_{i=1}^{n} (Y_{i} - \\hat{Y}_{i})(\\hat{Y}_{i} - \\bar{Y})\n\\end{align}\\]\nSolution:\nIn our equation for the total sums of squares, we can see that we have \\(SS_{Error} = \\sum_{i=1}^{n} (Y_{i} - \\hat{Y}_{i})^2\\) and \\(SS_{Model} = \\sum_{i=1}^{n} (\\hat{Y}_{i} - \\bar{Y})^2\\). Therefore, we really just need to focus on showing that\n\\[ 2 \\sum_{i=1}^{n} (Y_{i} - \\hat{Y}_{i})(\\hat{Y}_{i} - \\bar{Y}) = 0 \\]\nso that SStotal = SSmodel + SSerror.\nThe first step, we can divide each side of our problem by 2 (to get rid of the constant):\n\\[ \\frac{2 \\sum_{i=1}^{n} (Y_{i} - \\hat{Y}_{i})(\\hat{Y}_{i} - \\bar{Y})}{2} = \\frac{0}{2} \\]\nThis simplifies our problem to\n\\[ \\sum_{i=1}^{n} (Y_{i} - \\hat{Y}_{i})(\\hat{Y}_{i} - \\bar{Y}) = 0 \\]\nNext, we can substitute the definition of \\(\\hat{e}_{i}\\) into our problem:\n\\[ \\sum_{i=1}^{n} (Y_{i} - \\hat{Y}_{i})(\\hat{Y}_{i} - \\bar{Y}) = \\sum_{i=1}^{n} \\hat{e}_{i}(\\hat{Y}_{i} - \\bar{Y}) = \\sum_{i=1}^{n} \\hat{Y}_{i} \\hat{e}_{i} - \\sum_{i=1}^{n} \\bar{Y} \\hat{e}_{i}  \\]\nThis can be further simplified by recognizing that \\(\\bar{Y}\\) is a constant that does not depend on \\(i\\), so it can be factored out to get:\n\\[ \\sum_{i=1}^{n} \\hat{Y}_{i} \\hat{e}_{i} - \\sum_{i=1}^{n} \\bar{Y} \\hat{e}_{i} = \\sum_{i=1}^{n} \\hat{Y}_{i} \\hat{e}_{i} - \\bar{Y} \\sum_{i=1}^{n}  \\hat{e}_{i} \\]\nFor \\(\\hat{Y}_{i}\\), we can plug in its definition for the fitted regression line:\n\\[\\begin{align}\n\\sum_{i=1}^{n} \\hat{Y}_{i} \\hat{e}_{i} - \\bar{Y} \\sum_{i=1}^{n}  \\hat{e}_{i} =& \\sum_{i=1}^{n} (\\hat{\\beta}_{0} + \\hat{\\beta}_{1}X_{i}) \\hat{e}_{i} - \\bar{Y} \\sum_{i=1}^{n} \\hat{e}_{i} \\\\\n=& \\sum_{i=1}^{n} (\\hat{\\beta}_{0}\\hat{e}_{i} + \\hat{\\beta}_{1}X_{i}\\hat{e}_{i}) - \\bar{Y} \\sum_{i=1}^{n} \\hat{e}_{i} \\\\\n=& \\hat{\\beta}_{0} \\sum_{i=1}^{n} \\hat{e}_{i} + \\hat{\\beta}_{1} \\sum_{i=1}^{n} X_{i}\\hat{e}_{i} - \\bar{Y} \\sum_{i=1}^{n} \\hat{e}_{i}\n\\end{align}\\]\nSimilar to \\(\\bar{Y}\\), \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) can be thought of as constant values that do not depend on \\(i\\), so can be factored out of the summation.\nWe can now apply Notes 1 and 2 to see that all our summation components are equal to 0:\n\\[ \\hat{\\beta}_{0} \\sum_{i=1}^{n} \\hat{e}_{i} + \\hat{\\beta}_{1} \\sum_{i=1}^{n} X_{i}\\hat{e}_{i} - \\bar{Y} \\sum_{i=1}^{n} \\hat{e}_{i} = \\hat{\\beta}_{0} (0) + \\hat{\\beta}_{1}(0) - \\bar{Y} (0) = 0 \\]\nTherefore, since we showed \\(2 \\sum_{i=1}^{n} (Y_{i} - \\hat{Y}_{i})(\\hat{Y}_{i} - \\bar{Y}) = 0\\), it is true that SStotal = SSmodel + SSerror."
  },
  {
    "objectID": "labs/prac7s/index.html",
    "href": "labs/prac7s/index.html",
    "title": "Week 7 Practice Problems: Solutions",
    "section": "",
    "text": "This page includes the solutions to the optional practice problems for the given week. If you want to see a version without solutions please click here. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course."
  },
  {
    "objectID": "labs/prac7s/index.html#a-fitting-the-model",
    "href": "labs/prac7s/index.html#a-fitting-the-model",
    "title": "Week 7 Practice Problems: Solutions",
    "section": "1a: Fitting the Model",
    "text": "1a: Fitting the Model\nFit the simple linear regression model for an outcome of surgery duration with a single predictor for BMI. Print the summary table output for reference in the following questions.\nSolution:\nWe can fit our simple linear regression model using either the lm or glm functions (notice the output is very similar):\n\n\nCode\n# Fit using lm\nlm1 &lt;- lm(DurationSurgery ~ BMI, data=dat)\nsummary(lm1)\n\n\n\nCall:\nlm(formula = DurationSurgery ~ BMI, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2213 -1.3396 -0.1916  1.1093 11.0863 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.705949   0.153718  17.603  &lt; 2e-16 ***\nBMI         0.032403   0.005537   5.853 5.38e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.935 on 2917 degrees of freedom\nMultiple R-squared:  0.01161,   Adjusted R-squared:  0.01127 \nF-statistic: 34.25 on 1 and 2917 DF,  p-value: 5.378e-09\n\n\nCode\n# Fit using glm\nglm1 &lt;- glm(DurationSurgery ~ BMI, data=dat)\nsummary(glm1)\n\n\n\nCall:\nglm(formula = DurationSurgery ~ BMI, data = dat)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.705949   0.153718  17.603  &lt; 2e-16 ***\nBMI         0.032403   0.005537   5.853 5.38e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 3.743385)\n\n    Null deviance: 11048  on 2918  degrees of freedom\nResidual deviance: 10919  on 2917  degrees of freedom\nAIC: 12141\n\nNumber of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "labs/prac7s/index.html#b-fitted-least-squares-regression-equation",
    "href": "labs/prac7s/index.html#b-fitted-least-squares-regression-equation",
    "title": "Week 7 Practice Problems: Solutions",
    "section": "1b: Fitted Least-Squares Regression Equation",
    "text": "1b: Fitted Least-Squares Regression Equation\nWrite down the least-squares regression equation that describes the relationship between surgery duration and BMI based on your output.\nSolution:\nFrom our regression output, we can write this in a few ways:\n\\[ \\hat{Y} = 2.71 + 0.03 \\times X_{1}, \\]\nwhere \\(\\hat{Y}\\) is our outcome of surgery duration in hours and \\(X_1\\) is BMI in kg/m2. Or:\n\\[ \\text{Surgery Duration} = 2.71 + 0.03 \\times \\text{BMI}, \\]\nwhere we are still aware that the proper notation would have a hat over our outcome."
  },
  {
    "objectID": "labs/prac7s/index.html#c-intercept-interpretation",
    "href": "labs/prac7s/index.html#c-intercept-interpretation",
    "title": "Week 7 Practice Problems: Solutions",
    "section": "1c: Intercept Interpretation",
    "text": "1c: Intercept Interpretation\nWhat is the estimated intercept and how would you interpret it?\nSolution:\nOur estimated intercept is 2.71. It represents that the predicted surgery duration for an individual with a BMI of 0 kg/m2 is 2.71 hours. Note, this interpretation is not meaningful because one cannot plausibly have a BMI of 0, and even if they could it would be extrapolating outside our observed data."
  },
  {
    "objectID": "labs/prac7s/index.html#d-slope-interpretation",
    "href": "labs/prac7s/index.html#d-slope-interpretation",
    "title": "Week 7 Practice Problems: Solutions",
    "section": "1d: Slope Interpretation",
    "text": "1d: Slope Interpretation\nWhat is the estimated slope and how would you interpret it?\nSolution:\nOur estimated slope if 0.03. For every 1 kg/m2 increase in BMI, surgery duration increases, on average, by 0.03 hours (i.e., about 1.8 minutes)."
  },
  {
    "objectID": "labs/prac7s/index.html#e-slope-hypothesis-test",
    "href": "labs/prac7s/index.html#e-slope-hypothesis-test",
    "title": "Week 7 Practice Problems: Solutions",
    "section": "1e: Slope Hypothesis Test",
    "text": "1e: Slope Hypothesis Test\nTest the hypothesis that the true slope is 0.\nSolution:\nFrom our output we see that \\(t=5.853\\) and \\(p&lt;0.001\\) for our slope term (i.e., the BMI row). Since \\(p&lt;0.05\\), we reject our null hypothesis that the true slope is 0."
  },
  {
    "objectID": "labs/prac7s/index.html#f-ci-for-slope-with-t",
    "href": "labs/prac7s/index.html#f-ci-for-slope-with-t",
    "title": "Week 7 Practice Problems: Solutions",
    "section": "1f: CI for Slope with \\(t\\)",
    "text": "1f: CI for Slope with \\(t\\)\nFor the estimated slope, calculate a 95% confidence interval by hand based on the output with the \\(t\\)-distribution (i.e., the “correct” calculation).\nSolution:\nThe 95% confidence interval for our slope is\n\\[ \\hat{\\beta}_{1} \\pm t_{n-2,1-\\alpha/2} SE(\\hat{\\beta}_{1}) = 0.03 \\]\nIn our case we can calculate the \\(t\\)-statistic using qt(p=0.975, df=2919-2) to be 1.9607776:\n\\[ 0.032403 \\pm 1.960778 \\times 0.005537 = (0.02154617, 0.04325983) \\]\nOur interpretation of this confidence interval is: We are 95% confident that surgery duration increases, on average, between 0.022 and 0.043 hours for a 1 kg/m2 increase in BMI."
  },
  {
    "objectID": "labs/prac7s/index.html#g-ci-for-slope-with-z",
    "href": "labs/prac7s/index.html#g-ci-for-slope-with-z",
    "title": "Week 7 Practice Problems: Solutions",
    "section": "1g: CI for Slope with \\(Z\\)",
    "text": "1g: CI for Slope with \\(Z\\)\nFor the estimated slope, calculate a 95% confidence interval by hand based on the output with the \\(Z\\)-distribution (i.e., pretend we forgot to use the \\(t\\)-distribution and went with the simpler standard normal distribution). How different is your estimated interval and why might it be more similar (or more different) for our given context?\nSolution:\nIf we replace \\(t_{n-2,1-\\alpha/2}\\) with \\(Z_{1-\\alpha/2}\\) we will use qnorm(0.975)=1.959964:\n\\[ 0.032403 \\pm 1.959964 \\times 0.005537 = (0.02155068, 0.04325532) \\]\nIn this case, our intervals are extremely similar since the \\(t\\)-distribution with 2917 degrees of freedom looks extremely like the normal distribution. The 95% confidence interval is slightly wider for 1f, which would be more pronounced if we had a much smaller sample size."
  },
  {
    "objectID": "labs/prac7s/index.html#h-ci-for-slope-in-r",
    "href": "labs/prac7s/index.html#h-ci-for-slope-in-r",
    "title": "Week 7 Practice Problems: Solutions",
    "section": "1h: CI for Slope in R",
    "text": "1h: CI for Slope in R\nFor the estimated slope, calculate a 95% confidence interval using a function in R. Which approach (in f or g) is this confidence interval most like?\nSolution:\nUsing confint we can calculate the 95% confidence interval for our model fit in 1a:\n\n\nCode\n# Calculate the confidence interval for lm first\nconfint(lm1)\n\n\n                 2.5 %     97.5 %\n(Intercept) 2.40454149 3.00735660\nBMI         0.02154707 0.04325881\n\n\nCode\n# Calculate the confidence interval for glm second\nconfint(glm1)\n\n\nWaiting for profiling to be done...\n\n\n                 2.5 %    97.5 %\n(Intercept) 2.40466655 3.0072315\nBMI         0.02155158 0.0432543\n\n\nWoah! Shouldn’t our two simple linear regression models have the same confidence intervals regardless of using lm or glm?\nIt turns out that the lm function is specific to linear (regression) models. For this reason, it uses the \\(t\\)-distribution to correctly calculate the confidence interval.\nThe glm function is more general since it fits generalized linear models (e.g., next semester you will learn about logistic regression, Poisson regression, etc. that are all fit with this function). Due to this generality, it assumes the standard normal distribution for its calculation of the confidence interval.\nWhich is right? Technically, both! In large samples, like we have here, the difference is fairly trivial. However, if we have small samples the approach using the \\(t\\)-distribution would technically be most appropriate."
  },
  {
    "objectID": "labs/prac7s/index.html#i-summary-for-slope",
    "href": "labs/prac7s/index.html#i-summary-for-slope",
    "title": "Week 7 Practice Problems: Solutions",
    "section": "1i: Summary for Slope",
    "text": "1i: Summary for Slope\nWrite a brief, but complete, summary of the effect of BMI on surgery duration.\nSolution:\nThere is a significant association between BMI and surgery duration (p&lt;0.001). On average, surgery duration increases 0.03 hours per every 1 kg/m2 increase in BMI (95% CI: 0.022, 0.043 hours)."
  },
  {
    "objectID": "labs/prac7s/index.html#j-prediction",
    "href": "labs/prac7s/index.html#j-prediction",
    "title": "Week 7 Practice Problems: Solutions",
    "section": "1j: Prediction",
    "text": "1j: Prediction\nWhat is the estimated surgery duration for someone with a BMI of 27.5?\nSolution:\nWe can simply plug in \\(X_{1}=27.5\\) to our fitted least-squares regression equation from 1b:\n\\[ \\hat{Y} = 2.71 + 0.03 \\times 27.5 = 3.535 \\text{ hours} \\]\nSince we rounded to fewer digits, our answers would differ from using the predict function:\n\n\nCode\npredict(lm1, # specify model to predict from\n        newdata=data.frame(BMI=27.5)) # create new data frame of data to predict, here it is only 1 observation\n\n\n      1 \n3.59703"
  },
  {
    "objectID": "labs/prac7s/index.html#k-confidence-interval-around-prediction",
    "href": "labs/prac7s/index.html#k-confidence-interval-around-prediction",
    "title": "Week 7 Practice Problems: Solutions",
    "section": "1k: Confidence Interval Around Prediction",
    "text": "1k: Confidence Interval Around Prediction\nCalculate the 95% confidence interval around the mean surgery duration for the population with a BMI of 27.5.\nSolution:\nOne approach is to leverage the predict function in R and specify interval='confidence':\n\n\nCode\n# prediction with 95% CI from model for BMI=27.5\npredict(lm1, newdata=data.frame(BMI=27.5), interval='confidence')\n\n\n      fit      lwr      upr\n1 3.59703 3.526604 3.667456\n\n\nWe are 95% confident that the mean surgery duration is between 3.53 and 3.67 hours, on average, for someone with a BMI of 27.5.\nAnother approach is to estimate the mean squared error (MSE) from our model and calculate the interval “by hand”:\n\n\nCode\n# calculate our mean squared error\nsum( lm1$residuals^2 ) / (nrow(dat) - 2)\n\n\n[1] 3.743385\n\n\nWe must also calculate the mean and variance for BMI in our sample:\n\n\nCode\n# calculate mean; variance BMI\nmean( dat$BMI ); var(dat$BMI)\n\n\n[1] 27.00055\n\n\n[1] 41.85108\n\n\nNow we can use our formula to calculate the standard error of our mean surgery duration for someone with a BMI of 27.5:\n\\(\\begin{aligned} SE\\left({\\hat{\\mu}}_{Y|X=27.5}\\right) =& \\sqrt{\\frac{\\hat{\\sigma}_{Y|X}^{2}}{n} + \\frac{\\hat{\\sigma}_{Y|X}^{2}}{n-1} \\left( \\frac{(X_0 - \\bar{X})^2}{\\hat{\\sigma}_{X}^{2}} \\right)} \\\\ =& \\sqrt{\\frac{3.743385}{2919} + \\frac{3.743385}{2918} \\left( \\frac{(27.5-27.00055)^2}{41.85108} \\right)} \\\\ \\approx& 0.0359175 \\end{aligned}\\)\nThen we can calculate our 95% confidence interval, considering \\(t_{2919-2,0.975}=1.960778\\), our 95% CI is\n\\[ 3.535 \\pm 1.960778(0.0359175) = (3.465, 3.606) \\]\nNote, rounding at various steps leads to the similar, but different, interval from what R precisely returns."
  },
  {
    "objectID": "labs/prac7s/index.html#l-prediction-interval-around-prediction",
    "href": "labs/prac7s/index.html#l-prediction-interval-around-prediction",
    "title": "Week 7 Practice Problems: Solutions",
    "section": "1l: Prediction Interval Around Prediction",
    "text": "1l: Prediction Interval Around Prediction\nCalculate the 95% prediction interval around the mean surgery duration for a single individual with a BMI of 27.5.\nSolution:\nOne approach is to leverage the predict function in R and specify interval='predict':\n\n\nCode\n# prediction with 95% CI from model for BMI=27.5\npredict(lm1, newdata=data.frame(BMI=27.5), interval='predict')\n\n\n      fit        lwr      upr\n1 3.59703 -0.1973025 7.391362\n\n\nWe are 95% confident that the surgery duration for an individual with a BMI of 27.5 is between -0.20 and 7.39 hours.\nThis prediction interval must have the assumption or normality met (i.e., we are not leveraging the central limit theorem like we are for the 95% confidence interval estimating the mean). And given the greater variability involved, we see here that we also have a non-nonsensical negative surgery duration."
  },
  {
    "objectID": "labs/prac7s/index.html#m-scattplot-and-fitted-line",
    "href": "labs/prac7s/index.html#m-scattplot-and-fitted-line",
    "title": "Week 7 Practice Problems: Solutions",
    "section": "1m: Scattplot and Fitted Line",
    "text": "1m: Scattplot and Fitted Line\nCreate a scatterplot with the fitted linear regression line.\nSolution:\nOne option is to create this with base R graphics and use abline to automatically add the fitted regression line:\n\n\nCode\n# Create scatterplot\nplot(x=dat$BMI, y=dat$DurationSurgery, xlab=expression(BMI~(kg/m^2)), ylab='Surgery Duration (hours)' )\n\n# Add regression fit\nabline(lm1, lwd=2, col='orangered2')\n\n\n\n\n\nWe can see from our scatterplot and fitted regression line that there does appear to be a slight positive trend as BMI increases with respect to surgery duration. Additionally, there are some potential observations that seem very different from much of the data (which we will discuss how to handle more towards the end of the semester)."
  },
  {
    "objectID": "labs/prac8s/index.html",
    "href": "labs/prac8s/index.html",
    "title": "Week 8 Practice Problems: Solutions",
    "section": "",
    "text": "This page includes the solutions to the optional practice problems for the given week. If you want to see a version without solutions please click here. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises are focusing on examining the diagnostic plots for various regression models simulated to violate various regression assumptions. The interpretation of models with transformations are also explored for further practice."
  },
  {
    "objectID": "labs/prac8s/index.html#a.-scenario-1",
    "href": "labs/prac8s/index.html#a.-scenario-1",
    "title": "Week 8 Practice Problems: Solutions",
    "section": "1a. Scenario 1",
    "text": "1a. Scenario 1\n\n\nCode\n# Code to generate figures for violation of linearity\nset.seed(303)\nx &lt;- rnorm(n=100, mean=10, sd=3) # simulate a single continuous predictor with mean=10 and SD=3\nreg &lt;- 15 + 5 * x - 1 * x^2 - 0.25 * x^3 # set the regression equation so the intercept=10 and the slope=3\ny &lt;- rnorm(n=100, mean=reg, sd=8) # simulate the outcome based on the conditional mean\n\nmod1 &lt;- glm(y ~ x)\n\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))\nplot(x=x, y=y, xlab='X', ylab='Y', main='Scatterplot', cex=0.7); abline( mod1 )\n\nplot(x=x, y=rstudent(mod1), xlab='X', ylab='Jackknife Residual', \n     main='Residual Plot', cex=0.7); abline(h=0, lty=2, col='gray65')\n\nhist(rstudent(mod1), xlab='Jackknife Residual', \n     main='Histogram of Residuals', freq=F, breaks=seq(-6,1,0.25)); \n  curve( dnorm(x,mean=0,sd=1), lwd=2, col='blue', add=T)\n\nplot( ppoints(length(rstudent(mod1))), sort(pnorm(rstudent(mod1))), \n      xlab='Observed Cumulative Probability', \n      ylab='Expected Cumulative Probability', \n      main='Normal Probability Plot', cex=0.7); \n  abline(a=0,b=1, col='gray65', lwd=1)\n\n\n\n\n\nSolution:\nThe plots show a clear departure from our assumption of linearity with the quadratic-type trend in the scatterplot and residual plot. Due to this departure of linearity, we also see normality is grossly violated in both the histogram of residuals and the normal probability plot.\nIn fact, if we examine the code we see that the true data generating mechanism assumes a cubic trend: \\(Y = 15 + 5X - X^2 - 0.25X^3 + \\epsilon\\)."
  },
  {
    "objectID": "labs/prac8s/index.html#b.-scenario-2",
    "href": "labs/prac8s/index.html#b.-scenario-2",
    "title": "Week 8 Practice Problems: Solutions",
    "section": "1b. Scenario 2",
    "text": "1b. Scenario 2\n\n\nCode\n# Code to generate figures for violation of normality\nset.seed(515)\nx &lt;- rnorm(n=100, mean=10, sd=3) # simulate a single continuous predictor with mean=10 and SD=3\nreg &lt;- 15 + 5 * x # set the regression equation so the intercept=10 and the slope=3\ny &lt;- rexp(n=100, rate=1/reg) # simulate the outcome based on the conditional mean\n\nmod1 &lt;- glm(y ~ x)\n\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))\nplot(x=x, y=y, xlab='X', ylab='Y', main='Scatterplot', cex=0.7); abline( mod1 )\n\nplot(x=x, y=rstudent(mod1), xlab='X', ylab='Jackknife Residual', \n     main='Residual Plot', cex=0.7); abline(h=0, lty=2, col='gray65')\n\nhist(rstudent(mod1), xlab='Jackknife Residual', \n     main='Histogram of Residuals', freq=F, breaks=seq(-4,7,0.25)); \n  curve( dnorm(x,mean=0,sd=1), lwd=2, col='blue', add=T)\n\nplot( ppoints(length(rstudent(mod1))), sort(pnorm(rstudent(mod1))), \n      xlab='Observed Cumulative Probability', \n      ylab='Expected Cumulative Probability', \n      main='Normal Probability Plot', cex=0.7); \n  abline(a=0,b=1, col='gray65', lwd=1)\n\n\n\n\n\nSolution:\nThe plots show normality appears to be violated based on the histogram of residuals and the normal probability plot. There may also be some extreme outliers with jackknife residual values above 2.\nIn fact, if we examine the code we see that the true data generating mechanism assumes exponentially generated data for the errors instead of normal data."
  },
  {
    "objectID": "labs/prac8s/index.html#c.-scenario-3",
    "href": "labs/prac8s/index.html#c.-scenario-3",
    "title": "Week 8 Practice Problems: Solutions",
    "section": "1c. Scenario 3",
    "text": "1c. Scenario 3\n\n\nCode\n# Code to generate figures for no violations\nset.seed(720)\nx &lt;- rnorm(n=100, mean=10, sd=3) # simulate a single continuous predictor with mean=10 and SD=3\nreg &lt;- 15 + 5 * x  # set the regression equation so the intercept=10 and the slope=3\ny &lt;- rnorm(n=100, mean=reg, sd=8) # simulate the outcome based on the conditional mean\n\nmod1 &lt;- glm(y ~ x)\n\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))\nplot(x=x, y=y, xlab='X', ylab='Y', main='Scatterplot', cex=0.7); abline( mod1 )\n\nplot(x=x, y=rstudent(mod1), xlab='X', ylab='Jackknife Residual', \n     main='Residual Plot', cex=0.7); abline(h=0, lty=2, col='gray65')\n\nhist(rstudent(mod1), xlab='Jackknife Residual', \n     main='Histogram of Residuals', freq=F, breaks=seq(-4,4,0.25)); \n  curve( dnorm(x,mean=0,sd=1), lwd=2, col='blue', add=T)\n\nplot( ppoints(length(rstudent(mod1))), sort(pnorm(rstudent(mod1))), \n      xlab='Observed Cumulative Probability', \n      ylab='Expected Cumulative Probability', \n      main='Normal Probability Plot', cex=0.7); \n  abline(a=0,b=1, col='gray65', lwd=1)\n\n\n\n\n\nSolution:\nThese plots are simulated to have no violations of the assumptions. And, across the 5 scenarios, they generally look the best. However, it is worth highlighting that humans are super good at finding patterns and one may be worried that there isn’t “perfect” normality in the histogram of residuals and the normal probability plots."
  },
  {
    "objectID": "labs/prac8s/index.html#d.-scenario-4",
    "href": "labs/prac8s/index.html#d.-scenario-4",
    "title": "Week 8 Practice Problems: Solutions",
    "section": "1d. Scenario 4",
    "text": "1d. Scenario 4\n\n\nCode\n# Code to generate figures for violation of homoscedasticity\nset.seed(660)\nx &lt;- rnorm(n=100, mean=10, sd=3) # simulate a single continuous predictor with mean=10 and SD=3\nreg &lt;- 15 + 5 * x  # set the regression equation so the intercept=10 and the slope=3\ny &lt;- rnorm(n=100, mean=reg, sd=x^2) # simulate the outcome based on the conditional mean\n\nmod1 &lt;- glm(y ~ x)\n\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))\nplot(x=x, y=y, xlab='X', ylab='Y', main='Scatterplot', cex=0.7); abline( mod1 )\n\nplot(x=x, y=rstudent(mod1), xlab='X', ylab='Jackknife Residual', \n     main='Residual Plot', cex=0.7); abline(h=0, lty=2, col='gray65')\n\nhist(rstudent(mod1), xlab='Jackknife Residual', \n     main='Histogram of Residuals', freq=F, breaks=seq(-6,4,0.25)); \n  curve( dnorm(x,mean=0,sd=1), lwd=2, col='blue', add=T)\n\nplot( ppoints(length(rstudent(mod1))), sort(pnorm(rstudent(mod1))), \n      xlab='Observed Cumulative Probability', \n      ylab='Expected Cumulative Probability', \n      main='Normal Probability Plot', cex=0.7); \n  abline(a=0,b=1, col='gray65', lwd=1)\n\n\n\n\n\nSolution:\nThere appears to be non-constant variance in the scatterplot and residual plot, where larger values of \\(X\\) have greater variability. Otherwise, the histogram of residuals and normal probability plot are not terrible, but they do have some perturbations that are likely based on some of these more extreme values.\nIn reviewing the code, we can note that the error standard deviation (in this set-up, represented by the rnorm for y) is not constant but varies directly with the observed value of x to be squared."
  },
  {
    "objectID": "labs/prac8s/index.html#e.-scenario-5",
    "href": "labs/prac8s/index.html#e.-scenario-5",
    "title": "Week 8 Practice Problems: Solutions",
    "section": "1e. Scenario 5",
    "text": "1e. Scenario 5\n\n\nCode\n# Code to generate figures for violation of homoscedasticity, linearity, and normality\nset.seed(660)\nx &lt;- rnorm(n=100, mean=10, sd=3) # simulate a single continuous predictor with mean=10 and SD=3\nreg &lt;- 15 + 5 * x + 0.5 * x^2  # set the regression equation so the intercept=10 and the slope=3\ny &lt;- rgamma(n=100, shape=reg, scale=x^2) # simulate the outcome based on the conditional mean\n\nmod1 &lt;- glm(y ~ x)\n\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))\nplot(x=x, y=y, xlab='X', ylab='Y', main='Scatterplot', cex=0.7); abline( mod1 )\n\nplot(x=x, y=rstudent(mod1), xlab='X', ylab='Jackknife Residual', \n     main='Residual Plot', cex=0.7); abline(h=0, lty=2, col='gray65')\n\nhist(rstudent(mod1), xlab='Jackknife Residual', \n     main='Histogram of Residuals', freq=F, breaks=seq(-2,7,0.25)); \n  curve( dnorm(x,mean=0,sd=1), lwd=2, col='blue', add=T)\n\nplot( ppoints(length(rstudent(mod1))), sort(pnorm(rstudent(mod1))), \n      xlab='Observed Cumulative Probability', \n      ylab='Expected Cumulative Probability', \n      main='Normal Probability Plot', cex=0.7); \n  abline(a=0,b=1, col='gray65', lwd=1)\n\n\n\n\n\nSolution:\nWow…so much to say about these figures. We have stumbled upon the holy grail of assumption violations, since this data was simulated to violate homoscedasticity, linearity, and normality.\nWe see that there is a non-linear trend in the scatterplot and residual plot. Further, we see that the variability appears to increase slightly as the values of \\(X\\) increase. Finally, the histogram of residuals and the normal probability plot are decidedly non-normal.\nThese findings are not surprising when we look at the data generating mechanism in the code. It assumes a quadratic relationship between \\(X\\) and \\(Y\\) (i.e., \\(E(Y)=15 + 5X + 0.5X^2\\)), while using a gamma distribution to simulate the errors with the observed value of \\(X^2\\) directly used as the scale parameter."
  },
  {
    "objectID": "labs/prac8s/index.html#a.-y-x",
    "href": "labs/prac8s/index.html#a.-y-x",
    "title": "Week 8 Practice Problems: Solutions",
    "section": "2a. \\(Y\\), \\(X\\)",
    "text": "2a. \\(Y\\), \\(X\\)\n\n\nCode\nsummary(lm(PVol ~ Age, data=dat))\n\n\n\nCall:\nlm(formula = PVol ~ Age, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.527 -15.817  -4.367   6.451 205.105 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -13.3823    14.1790  -0.944    0.346    \nAge           1.1443     0.2308   4.959 1.18e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.1 on 305 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.07461,   Adjusted R-squared:  0.07158 \nF-statistic: 24.59 on 1 and 305 DF,  p-value: 1.178e-06\n\n\nSolution:\nThe intercept of -13.38 is the mean of prostate volume in grams if age is 0 (which is extrapolating beyond the range of our data and is not really biologically meaningful anyway since it would represent a newborn, further the estimate is negative).\nThe slope is the average increase in prostate volume for a one year increase in age. In our problem, it represents that prostate volume increases on average by 1.14 grams for a one year increase in age.\nFor this problem, there is no need to transform the data back to the original prostate volume scale since that is already the outcome."
  },
  {
    "objectID": "labs/prac8s/index.html#b.-logy-x",
    "href": "labs/prac8s/index.html#b.-logy-x",
    "title": "Week 8 Practice Problems: Solutions",
    "section": "2b. \\(log(Y)\\), \\(X\\)",
    "text": "2b. \\(log(Y)\\), \\(X\\)\n\n\nCode\nsummary(lm(log(PVol) ~ Age, data=dat))\n\n\n\nCall:\nlm(formula = log(PVol) ~ Age, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.85316 -0.25833 -0.02639  0.18992  1.56272 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.83345    0.18124   15.63  &lt; 2e-16 ***\nAge          0.01820    0.00295    6.17 2.17e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3719 on 305 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.111, Adjusted R-squared:  0.108 \nF-statistic: 38.07 on 1 and 305 DF,  p-value: 2.169e-09\n\n\nCode\n# confidence interval to use in back-transformation\nconfint(lm(log(PVol) ~ Age, data=dat))\n\n\n                 2.5 %     97.5 %\n(Intercept) 2.47681662 3.19008051\nAge         0.01239449 0.02400252\n\n\nSolution:\nThe intercept of 2.83 is the mean of log(prostate volume) in log(grams) if age is 0 (which is extrapolating beyond the range of our data and is not really biologically meaningful anyway since it would represent a newborn). If we were to exponentiate the estimate, it would represent the geometric mean of the prostate volume when age is 0 (i.e., \\(e^{2.83}=16.95\\))\nThe slope is the average increase in log(prostate volume) for a one year increase in age. In our problem, it represents that log(prostate volume) increases on average by 0.0182 log(grams) for a one year increase in age.\nIf we exponentiate the slope, our interpretation changes from additive change to multiplicative change: \\(e^{0.0182}=1.018367\\). On average, a one year increase in age results in a prostate volume that is 1.84% higher (i.e., 1.0184 times higher).\nThis also applies to our 95% confidence interval: \\(e^{(0.0124, 0.0240)} = (1.0125,1.0243)\\). We are 95% confident that a one year increase in age results in a prostate volume (in grams) that is 1.25% to 2.43% higher (i.e., 1.0125 to 1.0243 higher)."
  },
  {
    "objectID": "labs/prac8s/index.html#c.-sqrty-x",
    "href": "labs/prac8s/index.html#c.-sqrty-x",
    "title": "Week 8 Practice Problems: Solutions",
    "section": "2c. \\(\\sqrt{Y}\\), \\(X\\)",
    "text": "2c. \\(\\sqrt{Y}\\), \\(X\\)\n\n\nCode\nsummary(lm(sqrt(PVol) ~ Age, data=dat))\n\n\n\nCall:\nlm(formula = sqrt(PVol) ~ Age, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6025 -0.9659 -0.1915  0.5813  8.4549 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.05356    0.75625   4.038 6.83e-05 ***\nAge          0.07016    0.01231   5.700 2.82e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.552 on 305 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.09628,   Adjusted R-squared:  0.09332 \nF-statistic:  32.5 on 1 and 305 DF,  p-value: 2.819e-08\n\n\nSolution:\nThe intercept of 3.05 is the mean of the square root of prostate volume in \\(\\sqrt{\\text{grams}}\\) if age is 0 (which is extrapolating beyond the range of our data and is not really biologically meaningful anyway since it would represent a newborn).\nThe slope is the average increase in the square root of prostate volume for a one year increase in age. In our problem, it represents that the square root of prostate volume increases on average by 0.07 \\(\\sqrt{\\text{grams}}\\) for a one year increase in age.\nFor this problem, while we could attempt to transform the data back to the original scale, it does not have a nice interpretation like the log-transformation and is not trivial to complete."
  },
  {
    "objectID": "labs/prac8s/index.html#d.-y-logx",
    "href": "labs/prac8s/index.html#d.-y-logx",
    "title": "Week 8 Practice Problems: Solutions",
    "section": "2d. \\(Y\\), \\(log(X)\\)",
    "text": "2d. \\(Y\\), \\(log(X)\\)\n\n\nCode\nsummary(lm(PVol ~ log(Age), data=dat))\n\n\n\nCall:\nlm(formula = PVol ~ log(Age), data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.901 -15.970  -4.164   6.732 206.188 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -215.75      55.63  -3.878 0.000129 ***\nlog(Age)       66.33      13.55   4.895 1.59e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.13 on 305 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.07285,   Adjusted R-squared:  0.06981 \nF-statistic: 23.97 on 1 and 305 DF,  p-value: 1.593e-06\n\n\nSolution:\nThe intercept of -215.75 is the mean of prostate volume in grams if log(age) is 0 (which is extrapolating beyond the range of our data and is not really biologically meaningful anyway since it would represent a newborn, further the estimate is negative).\nThe slope is the average increase in prostate volume for a one year increase in log(age). In our problem, it represents that prostate volume increases on average by 66.3 grams for a one year increase in log(age). Note, by transforming \\(X\\) to the log-scale, the range goes from 38.4-79.0 to 3.65-4.37, a much narrower range. However, the interpretation is still for a one-unit increase in the predictor of interest.\nFor this problem, there is no need to transform the data back to the original prostate volume scale since that is already the outcome."
  },
  {
    "objectID": "labs/prac8s/index.html#e.-logy-logx",
    "href": "labs/prac8s/index.html#e.-logy-logx",
    "title": "Week 8 Practice Problems: Solutions",
    "section": "2e. \\(log(Y)\\), \\(log(X)\\)",
    "text": "2e. \\(log(Y)\\), \\(log(X)\\)\n\n\nCode\nsummary(lm(log(PVol) ~ log(Age), data=dat))\n\n\n\nCall:\nlm(formula = log(PVol) ~ log(Age), data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8618 -0.2634 -0.0178  0.1913  1.5541 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.4363     0.7104  -0.614     0.54    \nlog(Age)      1.0673     0.1730   6.169 2.18e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3719 on 305 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.1109,    Adjusted R-squared:  0.108 \nF-statistic: 38.06 on 1 and 305 DF,  p-value: 2.181e-09\n\n\nCode\n# confidence interval to use in back-transformation\nconfint(lm(log(PVol) ~ log(Age), data=dat))\n\n\n                 2.5 %    97.5 %\n(Intercept) -1.8341165 0.9615298\nlog(Age)     0.7268795 1.4077864\n\n\nSolution:\nThe intercept of -0.436 is the mean of log(prostate volume) in log(grams) if log(age) is 0 (which is extrapolating beyond the range of our data and is not really biologically meaningful anyway since it would represent a newborn).\nThe slope is the average increase in log(prostate volume) for a one year increase in log(age). In our problem, it represents that log(prostate volume) increases on average by 1.07 log(grams) for a one unit increase in log(age).\nIf we exponentiate the slope, our interpretation changes from additive change to multiplicative change: \\(e^{1.0673}=2.907519\\). On average, a one unit increase in log(age) results in a prostate volume that is 2.91 times higher.\nThis also applies to our 95% confidence interval: \\(e^{(0.727, 1.408)} = (2.069, 4.088)\\). We are 95% confident that a one unit increase in log(age) results in a prostate volume (in grams) that is 2.069 to 4.088 times higher (i.e., 106.9% to 308.8%)."
  },
  {
    "objectID": "labs/prac9s/index.html",
    "href": "labs/prac9s/index.html",
    "title": "Week 9 Practice Problems: Solutions",
    "section": "",
    "text": "This page includes the solutions to the optional practice problems for the given week. If you want to see a version without solutions please click here. Data sets, if needed, are provided on the BIOS 6618 Canvas page for students registered for the course.\nThis week’s extra practice exercises are focusing on implementing and interpreting a multiple linear regression (MLR) model, both using existing functions and by coding our own matrices."
  },
  {
    "objectID": "labs/prac9s/index.html#exercise-1-multiple-linear-regression-example",
    "href": "labs/prac9s/index.html#exercise-1-multiple-linear-regression-example",
    "title": "Week 9 Practice Problems: Solutions",
    "section": "Exercise 1: Multiple Linear Regression Example",
    "text": "Exercise 1: Multiple Linear Regression Example\n\n1a: The True Regression Equation\nWrite the the multiple linear regression model for the outcome of throat pain (i.e., dependent variable) and independent variables for ASA score, gender, age, and treatment status. Be sure to define all terms.\nSolution:\nThe true regression model is\n\\[ Y_i = \\beta_0 + \\beta_1 X_{\\text{Licorice},i} + \\beta_2 X_{\\text{Female},i} + \\beta_3 X_{\\text{Age},i} + \\epsilon_i \\]\nwhere \\(\\epsilon_i \\sim N(0,\\sigma^{2}_{Y|X})\\) and the subscript for each variable corresponds to the designated variable (e.g., the group if categorical, or the value if continuous).\n\n\n1b: Fitting the Model\nFit the multiple linear regression model for the outcome of throat pain (i.e., dependent variable) and independent variables for ASA score, gender, age, and treatment status. Print the summary table output for reference in the following questions.\nSolution:\nWe could fit this model with either lm or glm:\n\n\nCode\n# Fit with lm function\nlm_full &lt;- lm(pacu30min_throatPain ~ treat + preOp_gender + preOp_age, data=dat)\nsummary(lm_full)\n\n\n\nCall:\nlm(formula = pacu30min_throatPain ~ treat + preOp_gender + preOp_age, \n    data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1968 -0.8539 -0.3559  0.5968  5.1079 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.229559   0.317234   3.876 0.000139 ***\ntreat        -0.744559   0.156179  -4.767 3.32e-06 ***\npreOp_gender -0.259238   0.159345  -1.627 0.105134    \npreOp_age    -0.001819   0.005051  -0.360 0.719128    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.19 on 229 degrees of freedom\nMultiple R-squared:  0.1023,    Adjusted R-squared:  0.09054 \nF-statistic: 8.699 on 3 and 229 DF,  p-value: 1.728e-05\n\n\n\n\nCode\n# Fit with glm function\nglm_full &lt;- glm(pacu30min_throatPain ~ treat + preOp_gender + preOp_age, data=dat)\nsummary(glm_full)\n\n\n\nCall:\nglm(formula = pacu30min_throatPain ~ treat + preOp_gender + preOp_age, \n    data = dat)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.229559   0.317234   3.876 0.000139 ***\ntreat        -0.744559   0.156179  -4.767 3.32e-06 ***\npreOp_gender -0.259238   0.159345  -1.627 0.105134    \npreOp_age    -0.001819   0.005051  -0.360 0.719128    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.415709)\n\n    Null deviance: 361.14  on 232  degrees of freedom\nResidual deviance: 324.20  on 229  degrees of freedom\nAIC: 748.19\n\nNumber of Fisher Scoring iterations: 2\n\n\nNotice, since we haven’t changed any of the variable types, it is treating treat and preOp_gender as numeric (i.e., it doesn’t specify the group in the variable name) since the values are 0/1. Since they are binary, we can see we arrive at identical results if we coerce the variables to categorical with as.factor():\n\n\nCode\n# Fit with lm function but with treat and preOp_gender as factors\nlm_full_factor &lt;- lm(pacu30min_throatPain ~ as.factor(treat) + as.factor(preOp_gender) + preOp_age, data=dat)\nsummary(lm_full_factor)\n\n\n\nCall:\nlm(formula = pacu30min_throatPain ~ as.factor(treat) + as.factor(preOp_gender) + \n    preOp_age, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1968 -0.8539 -0.3559  0.5968  5.1079 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               1.229559   0.317234   3.876 0.000139 ***\nas.factor(treat)1        -0.744559   0.156179  -4.767 3.32e-06 ***\nas.factor(preOp_gender)1 -0.259238   0.159345  -1.627 0.105134    \npreOp_age                -0.001819   0.005051  -0.360 0.719128    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.19 on 229 degrees of freedom\nMultiple R-squared:  0.1023,    Adjusted R-squared:  0.09054 \nF-statistic: 8.699 on 3 and 229 DF,  p-value: 1.728e-05\n\n\nSince our later questions ask for a “brief, but complete” interpretation, we can also output the confidence intervals:\n\n\nCode\n# Print confidence intervals to use for later responses\nconfint(lm_full)\n\n\n                   2.5 %       97.5 %\n(Intercept)   0.60448955  1.854629278\ntreat        -1.05229143 -0.436826568\npreOp_gender -0.57320823  0.054731424\npreOp_age    -0.01177002  0.008132942\n\n\n\n\n1c: Predicted Regression Equation\nWrite down the predicted regression equation that describes the relationship between throat pain and your predictors based on the output.\nSolution:\nOur predicted regression equation is\n\\[ \\hat{Y}_i = 1.230 + -0.745 X_{\\text{Licorice},i} + -0.259 X_{\\text{Female},i} + -0.002 X_{\\text{Age},i} \\]\n\n\n1d: Intercept Interpretation and Hypothesis Test\nWrite the hypothesis being tested in the regression output for this coefficient. What is the estimated intercept and how would you interpret it (provide a brief, but complete, interpretation)?\nSolution:\nOur hypotheses are \\(H_0\\colon \\beta_0 = 0\\) versus \\(H_1\\colon \\beta_0 \\neq 0\\).\nThe average throat pain after 30 minutes in the PACU is 1.230 (95% CI: 0.604, 1.855) when all other predictors are 0. Since p&lt;0.001, we reject the null hypothesis and conclude it is significantly different from 0.\nThis interpretation, however, is nonsensical since it both extrapolates to ages we do not have (i.e., 0) and a newborn would not be able to report their pain at 30 minutes in the PACU.\nRecall, our brief, but complete, interpretation involves (1) the point estimate, (2) an interval estimate, (3) a decision, and (4) a measure of uncertainty.\n\n\n1e: Coefficient Interpretation and Hypothesis Test for Binary Predictor\nWrite the hypothesis being tested in the regression output for this coefficient. What is the estimated effect of treatment and how would you interpret it (provide a brief, but complete, interpretation)?\nSolution:\nOur hypotheses are \\(H_0\\colon \\beta_1 = 0\\) versus \\(H_1\\colon \\beta_1 \\neq 0\\).\nThose treated with licorice have a significantly lower throat pain score after 30 minutes in the PACU that is, on average, 0.745 lower (95% CI: 0.437 to 1.052 lower) than those treated with sugar water after adjusting for gender and age (p&lt;0.001).\n\n\n1f: Coefficient Interpretation and Hypothesis Test for Continuous Predictor\nWrite the hypothesis being tested in the regression output for this coefficient. What is the estimated effect of age and how would you interpret it (provide a brief, but complete, interpretation)?\nSolution:\nOur hypotheses are \\(H_0\\colon \\beta_3 = 0\\) versus \\(H_1\\colon \\beta_3 \\neq 0\\).\nFor a one year increase in age, throat pain score after 30 minutes in the PACU is 0.002 higher (95% CI: -0.012 lower to 0.008 higher) after adjusting for treatment group and gender. Since p=0.719, we fail to reject the null hypothesis that the slope is different from 0.\n\n\n1g: The Partial F-test\nEvaluate if the addition of age and gender contribute significantly to the prediction of throat pain over and above that achieved by treatment group alone. Write out the null and alternative hypotheses being tested and your conclusion.\nSolution:\nWe can do this “by hand” or using R (or SAS) to do the calculations for us. We will start with our hand calculation and compare to R’s result.\nThe null hypothesis we are evaluating is \\(H_0\\colon \\beta_2 = \\beta_3 = 0\\) versus the alternative hypothesis (\\(H_1\\)) that at least one coefficient of \\(\\beta_2\\) and \\(\\beta_3\\) is not equal to 0.\nFirst, we must fit the reduced model and create the ANOVA tables for both the full and reduced models:\n\n\nCode\n# Create ANOVA table for full model\nlinreg_anova_func(lm_full)\n\n\n\n\n\nSource\nSums of Squares\nDegrees of Freedom\nMean Square\nF-value\np-value\n\n\n\n\nModel\n36.94\n3\n12.31\n8.7\n&lt;0.001\n\n\nError\n324.20\n229\n1.42\n\n\n\n\nTotal\n361.14\n232\n\n\n\n\n\n\n\n\n\n\nCode\n# Fit reduced model with age and gender removed\nlm_red &lt;- lm(pacu30min_throatPain ~ treat, data=dat)\n\n# Create ANOVA table for reduced model\nlinreg_anova_func(lm_red)\n\n\n\n\n\nSource\nSums of Squares\nDegrees of Freedom\nMean Square\nF-value\np-value\n\n\n\n\nModel\n32.97\n1\n32.97\n23.21\n&lt;0.001\n\n\nError\n328.17\n231\n1.42\n\n\n\n\nTotal\n361.14\n232\n\n\n\n\n\n\n\n\n\n\nTaking the relevant quantities from our ANOVA tables, we can calculate our partial \\(F\\) statistic:\n\\[ F = \\frac{SS_{model}(full) - SS_{model}(reduced)]/k}{MS_{error}(full)} = \\frac{(36.94 - 32.97) / 2}{1.42} = 1.40 \\sim F_{2,233-1-2-1} = F_{2,229} \\]\nSince we are using software, we can calculate the p-value directly that \\(P(F \\geq F_{2,229})\\) using pf(1.40,2,229,lower.tail=F): 0.249. Since we see that p&gt;0.05, we fail to reject the null hypothesis that the addition of age and gender contribute significantly to the prediction of throat pain over and above that achieved by treatment group alone.\nAnother approach we could use is to calculate the critical value represented by \\(F_{2,229}\\) for an \\(\\alpha=0.05\\) to compare our test statistic to using qf(0.95,2,229): 3.035. Since \\(F=1.40 &lt; 3.035 = F_{0.95,2,229}\\), we would still fail to reject our null hypothesis.\nA third approach would be to use the anova function in R to solve this without doing any calculations by hand:\n\n\nCode\nanova(lm_full, lm_red, test='F')\n\n\nAnalysis of Variance Table\n\nModel 1: pacu30min_throatPain ~ treat + preOp_gender + preOp_age\nModel 2: pacu30min_throatPain ~ treat\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    229 324.20                           \n2    231 328.17 -2   -3.9728 1.4031 0.2479\n\n\nWe see our results closely match our by hand work above (with some slight differences due to rounding), where we still have \\(p&gt;0.05\\) and fail to reject the null hypothesis.\n\n\n1h: The Overall F-test\nEvaluate if the the entire set of independent variables (i.e., predictors) contribute significantly to the prediction of throat pain. Write out the null and alternative hypotheses being tested and your conclusion.\nSolution:\nThe null hypothesis is that \\(H_0\\colon \\beta_1 = \\beta_2 = \\beta_3 = 0\\) versus the alternative hypothesis (\\(H_1\\)) that at least one of these \\(\\beta_k; k=1,2,3\\) is not equal to 0.\nFor this problem, we can see that in our lm_full summary that \\(F=8.699\\) and its corresponding \\(p&lt;0.001\\). Since \\(p&lt;0.05\\), we reject the null hypothesis and conclude that at least one \\(\\beta_k\\) is significantly different from 0.\nFurther, we could combine our findings from the previous problem to note that adding age and gender were not significant, so it seems that a model with just treatment would be better and more parsimonious, even if the overall \\(F\\)-test for all three predictors is significant.\n\n\n1i: Multicollinearity\nCalculate the variance inflation factors (VIFs) for the independent variables in the model. Does it appear that multicollinearity may be a concern?\nSolution:\n\n\nCode\ncar::vif(lm_full)\n\n\n       treat preOp_gender    preOp_age \n    1.003602     1.002208     1.002467 \n\n\nSince all VIFs are less than 10, it appears multicollinearity is not a concern in our full model.\n\n\n1j: Diagnostic Plots\nEvaluate the assumptions of our multiple linear regression model by creating diagnostic plots.\nSolution:\n\n\nCode\npar(mfrow=c(2,3), mar=c(4.1,4.1,3.1,2.1))\n\n## X1 Partial Plot MLR\nx1_mlr_step1 &lt;- lm(pacu30min_throatPain ~ preOp_gender + preOp_age, data=dat)\nx1_mlr_step2 &lt;- lm(treat ~ preOp_gender + preOp_age, data=dat)\nplot(x=residuals(x1_mlr_step2), y=residuals(x1_mlr_step1),\n     main=expression('Partial Plot for X'[1]), ylab='Partial Dependent Residual', \n     xlab='Partial Regressor Residual')\nabline(lm(residuals(x1_mlr_step1) ~ residuals(x1_mlr_step2)))\n\n\n## X2 Partial Plot MLR\nx2_mlr_step1 &lt;- lm(pacu30min_throatPain ~ treat + preOp_age, data=dat)\nx2_mlr_step2 &lt;- lm(preOp_gender ~ treat + preOp_age, data=dat)\nplot(x=residuals(x2_mlr_step2), y=residuals(x2_mlr_step1),\n     main=expression('Partial Plot for X'[2]), ylab='Partial Dependent Residual', \n     xlab='Partial Regressor Residual')\nabline(lm(residuals(x2_mlr_step1) ~ residuals(x2_mlr_step2)))\n\n\n## X3 Partial Plot MLR\nx3_mlr_step1 &lt;- lm(pacu30min_throatPain ~ treat + preOp_gender, data=dat)\nx3_mlr_step2 &lt;- lm(preOp_age ~ treat + preOp_gender, data=dat)\nplot(x=residuals(x3_mlr_step2), y=residuals(x3_mlr_step1),\n     main=expression('Partial Plot for X'[3]), ylab='Partial Dependent Residual', \n     xlab='Partial Regressor Residual')\nabline(lm(residuals(x3_mlr_step1) ~ residuals(x3_mlr_step2)))\n\n\n## Scatterplot of residuals by predicted values\nplot(x=predict(lm_full), y=rstudent(lm_full), xlab=expression(hat(Y)), ylab='Jackknife Residual', \n     main='Residual Plot by Y-hat', cex=1); abline(h=0, lty=2, col='gray65')\n\n## Histogram of jackknife residuals with normal curve\nhist(rstudent(lm_full), xlab='Jackknife Residual', \n     main='Histogram of Residuals', freq=F, breaks=seq(-5,5,0.25)); \n  curve( dnorm(x,mean=0,sd=1), lwd=2, col='blue', add=T)\n\n## PP-plot\nplot( ppoints(length(rstudent(lm_full))), sort(pnorm(rstudent(lm_full))), \n      xlab='Observed Cumulative Probability', \n      ylab='Expected Cumulative Probability', \n      main='Normal Probability Plot', cex=1); \n  abline(a=0,b=1, col='gray65', lwd=1)\n\n\n\n\n\nThese plots are somewhat challenging to decipher since we have two categorical predictors and only one continuous predictor. However, the lower figures all suggest that normality may be violated and that this model, as it currently is, may not be the most appropriate. It is possible that a data transformation may result in a model more appropriate for linear regression. Adding other variables (that may or may not be measured), may also improve the model fit. It is also entirely possible that our outcome of a 0-10 Likert pain scale is not normal and an entirely different approach to modeling may be needed (e.g., zero-inflated Poisson regression)."
  },
  {
    "objectID": "labs/prac9s/index.html#exercise-2-but-now-with-matrices",
    "href": "labs/prac9s/index.html#exercise-2-but-now-with-matrices",
    "title": "Week 9 Practice Problems: Solutions",
    "section": "Exercise 2: But Now With Matrices!",
    "text": "Exercise 2: But Now With Matrices!\nUsing your results in Exercise 1 to check your answers, complete the following parts using matrices you code yourself.\n\n2a: The Design Matrix\nCreate the design matrix that we will use for our regression calculations.\nSolution:\nOur design matrix, \\(\\mathbf{X}\\), includes a columns of 1’s for the intercept and then our three predictors:\n\n\nCode\nX &lt;- as.matrix( cbind(1, dat[,c('treat','preOp_gender','preOp_age')]) )\n\n\n\n\n2b: Beta Coefficients\nCalculate the estimated beta coefficients via matrix algebra.\nSolution:\nWe need to first create a vector for our outcome of throat pain (Y). Then we will calculate \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{Y}\\):\n\n\nCode\nY &lt;- dat$pacu30min_throatPain\nbeta_est &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\nbeta_est\n\n\n                     [,1]\n1             1.229559412\ntreat        -0.744558998\npreOp_gender -0.259238404\npreOp_age    -0.001818539\n\n\nWe see that our estimates match those from the lm function.\n\n\n2c: Standard Error of Beta Coefficients\nCalculate the standard error of the beta coefficients via matrix algebra.\nSolution:\nWe will first calculate our MSE as \\(\\hat{\\sigma}_{Y|X}^{2} = \\frac{SSE}{n-p-1} = \\frac{\\mathbf{Y}^\\top\\mathbf{Y}-\\hat{\\boldsymbol{\\beta}}^\\top\\mathbf{X}^\\top\\mathbf{Y}}{n-p-1}\\):\n\n\nCode\nmse &lt;- (t(Y) %*% Y - t(beta_est) %*% t(X) %*% Y) / (nrow(dat) - ncol(X))\nmse\n\n\n         [,1]\n[1,] 1.415709\n\n\nNotice, that \\(n-p-1\\) can be pulled from our objects as \\(n=\\)nrow(dat) and \\(p-1=\\)ncol(X) (since our design matrix already includes the intercept).\nThe variance of our beta coefficients is therefore \\(Var(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}_{Y|X}^{2} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\), which is our variance-covariance matrix:\n\n\nCode\nbeta_var &lt;- as.numeric( mse ) * solve(t(X) %*% X)\nbeta_var\n\n\n                        1         treat  preOp_gender     preOp_age\n1             0.100637153 -1.384092e-02 -8.509392e-03 -1.470583e-03\ntreat        -0.013840916  2.439202e-02 -1.028729e-03  3.498007e-05\npreOp_gender -0.008509392 -1.028729e-03  2.539084e-02 -1.935834e-05\npreOp_age    -0.001470583  3.498007e-05 -1.935834e-05  2.550802e-05\n\n\nWe can compare this to the vcov output:\n\n\nCode\nvcov(lm_full)\n\n\n              (Intercept)         treat  preOp_gender     preOp_age\n(Intercept)   0.100637153 -1.384092e-02 -8.509392e-03 -1.470583e-03\ntreat        -0.013840916  2.439202e-02 -1.028729e-03  3.498007e-05\npreOp_gender -0.008509392 -1.028729e-03  2.539084e-02 -1.935834e-05\npreOp_age    -0.001470583  3.498007e-05 -1.935834e-05  2.550802e-05\n\n\nThe standard error is then the square root of the variance terms along the diagonal:\n\n\nCode\nbeta_se &lt;- sqrt(diag(beta_var))\nbeta_se\n\n\n           1        treat preOp_gender    preOp_age \n 0.317233593  0.156179445  0.159345030  0.005050546 \n\n\nThis matches our lm output.\n\n\n2d: Test Statistics and p-values\nCalculate the \\(t\\)-statistic and associated p-value based on the previous estimates from 1b and 1c.\nSolution:\nWe know that our \\(t\\)-statistic is the beta coefficient estimate divided by its standard error:\n\n\nCode\nt_stat &lt;- beta_est / beta_se\nt_stat\n\n\n                   [,1]\n1             3.8758802\ntreat        -4.7673303\npreOp_gender -1.6268998\npreOp_age    -0.3600678\n\n\nThe p-value is then calculated for our two-sided test:\n\n\nCode\npval &lt;- 2 * (1-pt( abs(t_stat), df = length(Y) - ncol(X)) ) # df = n-p-1\npval\n\n\n                     [,1]\n1            1.387762e-04\ntreat        3.319843e-06\npreOp_gender 1.051336e-01\npreOp_age    7.191283e-01\n\n\nThese results match the p-values in our regression output.\n\n\n2e: Confidence and Prediction Interval\nCalculate the 95% confidence and prediction interval for a male in the treatment group who is 50 years old. Compare this result to the calculation provided by R when using the predict function.\nSolution:\nWe first need to define \\(x_0\\) for our given combination of covariates:\n\n\nCode\nx0 &lt;- c(1,1,0,50) # intercept, treatment, gender, age\npred_val &lt;- t(x0) %*% beta_est # predicted throat pain score\npred_val\n\n\n          [,1]\n[1,] 0.3940735\n\n\nWe can then calculate our variances for a predicted mean and individual value. The variance for a fitted value (i.e., the expected mean \\(\\mu\\) for a given value of \\(X=x_0\\)) is given by \\[ Var(\\mu_{Y|x_0}) = x_0^\\top [Var(\\hat{\\boldsymbol{\\beta}})] x_0 = \\hat{\\sigma}^{2}_{Y|X} x_{0}^\\top (\\mathbf{X}^\\top \\mathbf{X})^{-1} x_{0} \\]\nThe variance of a future predicted value of \\(Y\\) for a given \\(x_0\\) for an individual is given by: \\[ Var(\\hat{Y} | x_0 ) = \\hat{\\sigma}^{2}_{Y|X} \\left[ 1 + x_{0}^\\top (\\mathbf{X}^\\top \\mathbf{X})^{-1} x_{0}\\right] \\]\n\n\nCode\nvar_mean &lt;- t(x0) %*% beta_var %*% x0\nvar_ind &lt;- as.numeric( mse ) * (1 + t(x0) %*% solve(t(X) %*% X) %*% x0)\n\n\nThe 95% confidence and prediction intervals are:\n\n\nCode\n## Note, we don't necessarily need to wrap pred_val and var_mean or var_ind in as.numeric(), but it avoids a warning about deprecated functions\n# 95% CI\nas.numeric(pred_val) + qnorm(c(0.025,0.975)) * sqrt(as.numeric(var_mean))\n\n\n[1] 0.1343719 0.6537750\n\n\nCode\n# 95% PI\nas.numeric(pred_val) + qnorm(c(0.025,0.975)) * sqrt(as.numeric(var_ind))\n\n\n[1] -1.952378  2.740525\n\n\nWe can compare this to the predict function:\n\n\nCode\npredict(lm_full, newdata = data.frame(treat=1, preOp_gender=0, preOp_age=50), interval='confidence')\n\n\n        fit       lwr       upr\n1 0.3940735 0.1329921 0.6551548\n\n\nCode\npredict(lm_full, newdata = data.frame(treat=1, preOp_gender=0, preOp_age=50), interval='predict')\n\n\n        fit       lwr      upr\n1 0.3940735 -1.964845 2.752992\n\n\nWe see that our matrix approach and the lm::predict function nearly match. The difference being due to using the standard normal (\\(Z\\)) distribution instead of the \\(t\\)-distribution in our “by hand” calculation. If we use the \\(t_{229}\\) distribution instead, we see they do exactly match:\n\n\nCode\n## Note, we don't necessarily need to wrap pred_val and var_mean or var_ind in as.numeric(), but it avoids a warning about deprecated functions\n# 95% CI\nas.numeric(pred_val) + qt(c(0.025,0.975), df=229) * sqrt(as.numeric(var_mean))\n\n\n[1] 0.1329921 0.6551548\n\n\nCode\n# 95% PI\nas.numeric(pred_val) + qt(c(0.025,0.975), df=229) * sqrt(as.numeric(var_ind))\n\n\n[1] -1.964845  2.752992"
  },
  {
    "objectID": "recitation/index.html",
    "href": "recitation/index.html",
    "title": "BIOS 6618 Recitation",
    "section": "",
    "text": "This page includes the responses to various recitation requests submitted over various semesters of teaching BIOS 6618. These topics were covered during Thursday classes, so each request below may appear to come out of left field but related to topics worked on by students during the semester. The questions are broadly broken down by theme, but you may also use the Quarto search feature to try and find relevant pages throughout the course website. Responses often reinforce the content from lecture slides or extend the material to advanced topics. When possible, case studies or simulation studies are used to assist in answering the questions. There is also a page with BIOS 6618 class labs and practice problems (with solutions) on our BIOS 6618 labs page that provides additional examples corresponding to each week’s material."
  },
  {
    "objectID": "recitation/index.html#r-related-questions",
    "href": "recitation/index.html#r-related-questions",
    "title": "BIOS 6618 Recitation",
    "section": "R-Related Questions",
    "text": "R-Related Questions\n\nApproaches to Generating Data for a Simulation: for loops and apply Functions\nCreating Tables in Rmd Documents\nWhy Do We Set Seeds?\nWays to Enter Data for t.test\nDistribution Functions in R (e.g., dnorm, qnorm, pnorm, rnorm)"
  },
  {
    "objectID": "recitation/index.html#prerequisite-materials",
    "href": "recitation/index.html#prerequisite-materials",
    "title": "BIOS 6618 Recitation",
    "section": "Prerequisite Materials",
    "text": "Prerequisite Materials\n\nZ-Scores\nPoisson Approximations to Exact Binomial Probabilities"
  },
  {
    "objectID": "recitation/index.html#week-1-introduction-to-bios-6618-estimators",
    "href": "recitation/index.html#week-1-introduction-to-bios-6618-estimators",
    "title": "BIOS 6618 Recitation",
    "section": "Week 1: Introduction to BIOS 6618, Estimators",
    "text": "Week 1: Introduction to BIOS 6618, Estimators\n\nVariance, Consistency, and Bias of Estimators\nMini-Simulation Study for Bias, Consistency, and Efficiency of the Sample Median versus the Sample Mean\nPopulation Moments\nBrief, but Complete, Interpretations: A t-test Example\nMissing Data Considerations"
  },
  {
    "objectID": "recitation/index.html#week-2-normal-distribution-and-the-clt",
    "href": "recitation/index.html#week-2-normal-distribution-and-the-clt",
    "title": "BIOS 6618 Recitation",
    "section": "Week 2: Normal Distribution and the CLT",
    "text": "Week 2: Normal Distribution and the CLT\n\nIs the CLT Only for the Mean?\nSampling Distributions of the Mean, Median, and Variance\nCentral Limit Theorem Example: Chi-Squared Distributed Data"
  },
  {
    "objectID": "recitation/index.html#week-3-hypothesis-testing-and-power",
    "href": "recitation/index.html#week-3-hypothesis-testing-and-power",
    "title": "BIOS 6618 Recitation",
    "section": "Week 3: Hypothesis Testing and Power",
    "text": "Week 3: Hypothesis Testing and Power\n\nLong-Run Properties of \\(\\alpha\\) and \\(\\beta\\)\nPower and Type I Error with Comparison of Known and Unknown One-Sample Mean Calculations\nPower and Type I Error Rate Calculations via Simulation Studies\nMultiple Testing Correction and Their Use in Randomized Trials\nMultiple Testing Adjusting in R with p.adjust\nWhat are Some Alternatives to p-values?\nHow Are t.test and power.t.test Different?"
  },
  {
    "objectID": "recitation/index.html#week-4-conditional-probability-diagnostic-testing-2x2-tables",
    "href": "recitation/index.html#week-4-conditional-probability-diagnostic-testing-2x2-tables",
    "title": "BIOS 6618 Recitation",
    "section": "Week 4: Conditional Probability, Diagnostic Testing, 2x2 Tables",
    "text": "Week 4: Conditional Probability, Diagnostic Testing, 2x2 Tables\n\nMcNemar’s Paired Test and Discordant Pairs\nROC Curves Under Different Strengths of Predictors\nCreating ROC Curves from Tabular Data\nDiagnostic Testing: Likelihood Ratios, Odds, and Bayes’ Theorem\nTests of Association for 2x2 Tables\nWhat Are Some R Packages for Diagnostic Testing Calculations?\nStudy Designs and RD, RR, and OR Terminology"
  },
  {
    "objectID": "recitation/index.html#week-5-bootstrap-sampling-and-nonparametric-tests",
    "href": "recitation/index.html#week-5-bootstrap-sampling-and-nonparametric-tests",
    "title": "BIOS 6618 Recitation",
    "section": "Week 5: Bootstrap Sampling and Nonparametric Tests",
    "text": "Week 5: Bootstrap Sampling and Nonparametric Tests\n\nNonparametric Test Choices\nWilcoxon Rank Sum Examples and Using Vectorized Data in Functions\nBootstraps: the Normal Percentile Interval and Coverage, the Bootstrap Percentile Interval and Accuracy\nPermuation Test p-values\nConfidence Interval Interpretations and Calculations\nSimulation vs. Bootstrap vs. Permutation"
  },
  {
    "objectID": "recitation/index.html#week-6-simple-linear-regression-and-derivations",
    "href": "recitation/index.html#week-6-simple-linear-regression-and-derivations",
    "title": "BIOS 6618 Recitation",
    "section": "Week 6: Simple Linear Regression and Derivations",
    "text": "Week 6: Simple Linear Regression and Derivations\n\nSimple Linear Regression Assumptions"
  },
  {
    "objectID": "recitation/index.html#week-7-slr-diagnostics-confidence-and-prediction-intervals-anova",
    "href": "recitation/index.html#week-7-slr-diagnostics-confidence-and-prediction-intervals-anova",
    "title": "BIOS 6618 Recitation",
    "section": "Week 7: SLR Diagnostics, Confidence and Prediction Intervals, ANOVA",
    "text": "Week 7: SLR Diagnostics, Confidence and Prediction Intervals, ANOVA\n\nConfidence versus Prediction Intervals in Linear Regression\nConfidence Interval Differences in lm and glm\nThe ANOVA Table and the F-test in SLR\nPost-Hoc Testing Justification for One-Way ANOVA"
  },
  {
    "objectID": "recitation/index.html#week-8-slr-examples-logy-transformation",
    "href": "recitation/index.html#week-8-slr-examples-logy-transformation",
    "title": "BIOS 6618 Recitation",
    "section": "Week 8: SLR Examples, Log(Y) Transformation",
    "text": "Week 8: SLR Examples, Log(Y) Transformation\n\nLog Transformations and Interpretations in Linear Regression"
  },
  {
    "objectID": "recitation/index.html#week-9-multiple-linear-regression-and-matrix-approaches-to-regression",
    "href": "recitation/index.html#week-9-multiple-linear-regression-and-matrix-approaches-to-regression",
    "title": "BIOS 6618 Recitation",
    "section": "Week 9: Multiple Linear Regression and Matrix Approaches to Regression",
    "text": "Week 9: Multiple Linear Regression and Matrix Approaches to Regression\n\nHat and Design Matrices in Linear Regression"
  },
  {
    "objectID": "recitation/index.html#week-10-categorical-predictors-and-anova",
    "href": "recitation/index.html#week-10-categorical-predictors-and-anova",
    "title": "BIOS 6618 Recitation",
    "section": "Week 10: Categorical Predictors and ANOVA",
    "text": "Week 10: Categorical Predictors and ANOVA\n\nHypothesis Testing in Intercept-Only, Simple, and Multiple Linear Regression Models\nANOVA versus Linear Regression\nGeneral Linear Hypothesis Testing (GLHT) and Matrices, Contrasts, and F-Tests\nCategorical Predictors in Linear Regression\nHypothesis Testing for F-tests"
  },
  {
    "objectID": "recitation/index.html#week-11-confounding-mediation-interactions-and-polynomials",
    "href": "recitation/index.html#week-11-confounding-mediation-interactions-and-polynomials",
    "title": "BIOS 6618 Recitation",
    "section": "Week 11: Confounding, Mediation, Interactions, and Polynomials",
    "text": "Week 11: Confounding, Mediation, Interactions, and Polynomials\n\nInterpreting Polynomial Regression Models, Selecting Your Highest Order Polynomial Term, and Calculus in R\nClassical Criteria of Confounding\nPrecision Variables\nDegrees of Freedom with Interactions, Polynomials, and Categorical Variables\nEstimating Pure Error (Polynomials)\nThe Three Models for Mediation and Confounding\nIs It Possible to Tell The Role of a Variable (e.g., PEV, Confounder, etc.) From the Regression Model?\nMediation Modeling Example with Bootstrap CI Estimate"
  },
  {
    "objectID": "recitation/index.html#week-12-model-selection-variable-selection-outliersinfluential-points",
    "href": "recitation/index.html#week-12-model-selection-variable-selection-outliersinfluential-points",
    "title": "BIOS 6618 Recitation",
    "section": "Week 12: Model Selection, Variable Selection, Outliers/Influential Points",
    "text": "Week 12: Model Selection, Variable Selection, Outliers/Influential Points\n\nCollinearity in Regression"
  },
  {
    "objectID": "recitation/index.html#week-13-segmented-regression-quantile-regression-splines-and-advanced-bootstrap-topics",
    "href": "recitation/index.html#week-13-segmented-regression-quantile-regression-splines-and-advanced-bootstrap-topics",
    "title": "BIOS 6618 Recitation",
    "section": "Week 13: Segmented Regression, Quantile Regression, Splines, and Advanced Bootstrap Topics",
    "text": "Week 13: Segmented Regression, Quantile Regression, Splines, and Advanced Bootstrap Topics\n\nSplines in Linear Regression"
  },
  {
    "objectID": "recitation/index.html#week-14-bayesian-linear-regression",
    "href": "recitation/index.html#week-14-bayesian-linear-regression",
    "title": "BIOS 6618 Recitation",
    "section": "Week 14: Bayesian Linear Regression",
    "text": "Week 14: Bayesian Linear Regression\nNo current recitation topics."
  },
  {
    "objectID": "recitation/index.html#week-15-exponential-families-and-generalized-linear-models",
    "href": "recitation/index.html#week-15-exponential-families-and-generalized-linear-models",
    "title": "BIOS 6618 Recitation",
    "section": "Week 15: Exponential Families and Generalized Linear Models",
    "text": "Week 15: Exponential Families and Generalized Linear Models\n\nGeneralized Linear Models and Their Connection to Ordinary Least Squares Linear Regression"
  },
  {
    "objectID": "recitation/r10/index.html",
    "href": "recitation/r10/index.html",
    "title": "Creating Tables for Rmd Files",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r10/index.html#markdown-style-tables",
    "href": "recitation/r10/index.html#markdown-style-tables",
    "title": "Creating Tables for Rmd Files",
    "section": "Markdown Style Tables",
    "text": "Markdown Style Tables\nOne approach to creating tables in Rmd is to use Markdown-specific syntax. The Tables Generator website includes online tools where you can enter your information and copy the code.\nThe code to create the table of length of stay data from Homework 6 might look something like:\n| **Hospital (Treatment)** | **LOS (days)**                                    |\n|:-------------------------|:--------------------------------------------------|\n| Cauchy General (New)     | 3, 3, 4, 5, 5, 5, 6, 7, 7, 8, 9, 15               |\n| Skellam Memorial (SOC)   | 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10, 11, 13, 13, 15 |\nwhich produces\n\n\n\n\n\n\n\nHospital (Treatment)\nLOS (days)\n\n\n\n\nCauchy General (New)\n3, 3, 4, 5, 5, 5, 6, 7, 7, 8, 9, 15\n\n\nSkellam Memorial (SOC)\n6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10, 11, 13, 13, 15\n\n\n\nA limitation of this table type is that you may have “wider” tables or columns than you want and it is hard to change."
  },
  {
    "objectID": "recitation/r10/index.html#latex-style-tables",
    "href": "recitation/r10/index.html#latex-style-tables",
    "title": "Creating Tables for Rmd Files",
    "section": "Latex Style Tables",
    "text": "Latex Style Tables\nIf you are creating a PDF file in Rmd, you can enter the Latex syntax directly:\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{|l|l|}\n\\hline\n\\textbf{Hospital (Treatment)} & \\textbf{LOS (days)} \\\\ \\hline\nCauchy General (New)          & 3, 3, 4, 5, 5, 5, 6, 7, 7, 8, 9, 15  \\\\ \\hline\nSkellam Memorial (SOC)        & 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10, 11, 13, 13, 15 \\\\ \\hline\n\\end{tabular}\n\\end{table}\nSince this document is an HTML, it won’t generate here. In some cases, you also need to add {=latex} to the code chunk."
  },
  {
    "objectID": "recitation/r10/index.html#use-kableextra-package",
    "href": "recitation/r10/index.html#use-kableextra-package",
    "title": "Creating Tables for Rmd Files",
    "section": "Use kableExtra Package",
    "text": "Use kableExtra Package\nWe can create latex-esque table using various packages. One older package is known as kableExtra. We can create our table by first making a data frame to plot:\n\n\nCode\nlibrary(kableExtra)\n\nmy_table &lt;- data.frame(\n  'Hospital (Treatment)' = c('Cauchy General (New)','Skellam Memorial (SOC)'),\n  'LOS (Days)' = c('3, 3, 4, 5, 5, 5, 6, 7, 7, 8, 9, 15' , '6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10, 11, 13, 13, 15')\n)\n\nmy_table # check what is in data frame\n\n\n    Hospital..Treatment.                                        LOS..Days.\n1   Cauchy General (New)               3, 3, 4, 5, 5, 5, 6, 7, 7, 8, 9, 15\n2 Skellam Memorial (SOC) 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10, 11, 13, 13, 15\n\n\nCode\nmy_table %&gt;%\n  kbl(col.names=c('Hospital (Treatment)','LOS (Days)')) %&gt;%\n  kable_styling() %&gt;%\n  kable_styling(full_width = F)\n\n\n\n\n\nHospital (Treatment)\nLOS (Days)\n\n\n\n\nCauchy General (New)\n3, 3, 4, 5, 5, 5, 6, 7, 7, 8, 9, 15\n\n\nSkellam Memorial (SOC)\n6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10, 11, 13, 13, 15\n\n\n\n\n\n\n\nYou can also change the styling very easily:\n\n\nCode\nmy_table %&gt;%\n  kbl(col.names=c('Hospital (Treatment)','LOS (Days)')) %&gt;%\n  kable_classic_2() %&gt;%\n  kable_styling(full_width = F)\n\n\n\n\n\nHospital (Treatment)\nLOS (Days)\n\n\n\n\nCauchy General (New)\n3, 3, 4, 5, 5, 5, 6, 7, 7, 8, 9, 15\n\n\nSkellam Memorial (SOC)\n6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10, 11, 13, 13, 15\n\n\n\n\n\n\n\nkableExtra supports tables in HTML (with extra functionality, check the link out!), PDF, and Word."
  },
  {
    "objectID": "recitation/r12/index.html",
    "href": "recitation/r12/index.html",
    "title": "Wilcoxon Rank Sum Examples and Using Vectorized Data in Functions",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r12/index.html#homework-5-question-4c",
    "href": "recitation/r12/index.html#homework-5-question-4c",
    "title": "Wilcoxon Rank Sum Examples and Using Vectorized Data in Functions",
    "section": "Homework 5, Question 4c",
    "text": "Homework 5, Question 4c\nLet’s see how we can compare our length of stay data for HW 5, Question 4c, by implementing the WRS “by-hand” before checking our answers with a function in R.\nFirst, let’s start by ordering our data by group:\n\n\nCode\nlibrary(kableExtra)\n\nwilcox_rank &lt;- data.frame(\n  'Rank' = 1:27,\n  'Cauchy LOS' = c(3,3,4,5,5,5,6,NA,7,7,NA,NA,NA,8,NA,NA,NA,9,NA,NA,NA,NA,NA,NA,NA,15,NA),\n  'Skellam LOS' = c(NA,NA,NA,NA,NA,NA,NA,6,NA,NA,7,7,7,NA,8,8,8,NA,9,9,10,10,11,13,13,NA,15),\n  'Rank Range' = c(rep('1-2',2), rep('3-3',1), rep('4-6',3), rep('7-8',2), rep('9-13',5), rep('14-17',4), rep('18-20',3), rep('21-22',2), rep('23-23',1), rep('24-25',2), rep('26-27',2) ),\n  'Avg. Rank' = c(rep(1.5,2), rep(3,1), rep(5,3), rep(7.5,2), rep(11,5), rep(15.5,4), rep(19,3), rep(21.5,2), rep(23,1), rep(24.5,2), rep(26.5,2) )\n)\n\noptions(knitr.kable.NA = '') # make NA's in kable table below be blank instead of presenting as \"NA\"\n\nwilcox_rank %&gt;%\n  kbl( col.names=c('Rank','Cauchy LOS','Skellam LOS','Rank Range','Avg. Rank'), align='ccccc' ) %&gt;%\n  kable_classic_2() %&gt;%\n  kable_styling(full_width = F)\n\n\n\n\n\nRank\nCauchy LOS\nSkellam LOS\nRank Range\nAvg. Rank\n\n\n\n\n1\n3\n\n1-2\n1.5\n\n\n2\n3\n\n1-2\n1.5\n\n\n3\n4\n\n3-3\n3.0\n\n\n4\n5\n\n4-6\n5.0\n\n\n5\n5\n\n4-6\n5.0\n\n\n6\n5\n\n4-6\n5.0\n\n\n7\n6\n\n7-8\n7.5\n\n\n8\n\n6\n7-8\n7.5\n\n\n9\n7\n\n9-13\n11.0\n\n\n10\n7\n\n9-13\n11.0\n\n\n11\n\n7\n9-13\n11.0\n\n\n12\n\n7\n9-13\n11.0\n\n\n13\n\n7\n9-13\n11.0\n\n\n14\n8\n\n14-17\n15.5\n\n\n15\n\n8\n14-17\n15.5\n\n\n16\n\n8\n14-17\n15.5\n\n\n17\n\n8\n14-17\n15.5\n\n\n18\n9\n\n18-20\n19.0\n\n\n19\n\n9\n18-20\n19.0\n\n\n20\n\n9\n18-20\n19.0\n\n\n21\n\n10\n21-22\n21.5\n\n\n22\n\n10\n21-22\n21.5\n\n\n23\n\n11\n23-23\n23.0\n\n\n24\n\n13\n24-25\n24.5\n\n\n25\n\n13\n24-25\n24.5\n\n\n26\n15\n\n26-27\n26.5\n\n\n27\n\n15\n26-27\n26.5\n\n\n\n\n\n\n\nSince we have \\(n_{\\text{Cauchy}} = 12 \\ge 10\\) and \\(n_{\\text{Skellam}} = 15 \\ge 10\\), we can use the asymptotic approach. Here, we will use the Cauchy General Hospital as our group for the calculations.\nFirst, we can sum up the average ranks corresponding to our Cauchy LOS data: [ R_{} = 1.5+1.5+3+5+5+5+7.5+11+11+15.5+19+26.5 = 111.5 ]\nThen, we can calculate our expected value for \\(R_{\\text{Cauchy}}\\): [ E= = = = 168 ].\nFinally, we can calculate the variance for our statistic (recall, \\(g\\) is values with ties and \\(t_i\\) is the number of ties at a given value):\n\\(\\begin{aligned} V\\left[R_{\\text{Cauchy}}\\right]=& \\;\\left(\\frac{n_{\\text{Cauchy}} n_{\\text{Skellam}}}{12}\\right)\\left[n_{\\text{Cauchy}}+n_{\\text{Skellam}}+1-\\frac{\\sum_{i=1}^{g}\\left(t_i^3-t_i\\right)}{\\left(n_{\\text{Cauchy}}+n_{\\text{Skellam}}\\right)\\left(n_{\\text{Cauchy}}+n_{\\text{Skellam}}-1\\right)}\\right] \\\\ =& \\; \\left(\\frac{\\left(12\\right)15}{12}\\right)\\left[12 +15 + 1 -\\frac{\\left(2^3-2\\right)+\\left({5}^3-5\\right)+\\left(4^3-4\\right)+\\left(3^3-3\\right)+\\left(2^3-2\\right)}{(12+15)(12+15-1)}\\right] \\\\ =& \\; \\left(\\frac{\\left(12\\right)15}{12}\\right)\\left[28 -\\frac{6 + 120 + 60 + 24 + 6}{(27)(26)}\\right] \\\\ =& \\; 415.3846 \\end{aligned}\\)\nSince we are using the asymptotic version, we then calculate our \\(Z\\) statistic based on the standard normal distribution:\n\\[ Z = \\frac{|R_{\\text{Cauchy}} - E[R_{\\text{Cauchy}}]| - 0.5}{\\sqrt{V[R_{\\text{Cauchy}}]}}=\\frac{|111.5 - 168]| - 0.5}{\\sqrt{415.3846}}= \\frac{56}{\\sqrt{415.3846}} = 2.75 \\]\nFinally, we can take our \\(Z\\) statistic and estimate our p-value:\n\\(p=2\\times\\left(1-\\Phi\\left(2.75\\right)\\right)=2\\times\\left(1-0.9970202\\right)=0.0059596\\)\nNote, we can calculate \\(\\Phi\\left(2.75\\right)\\) using pnorm(2.75).\nFinally, let’s compare the result to the wilcox_test function from the coin package:\n\n\nCode\nwdf &lt;- data.frame(cauchy=factor(c(rep(1,12), rep(0,15))),\n                  los=c(3, 3, 4, 5, 5, 5, 6, 7, 7, 8, 9, 15, 6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10, 11, 13, 13, 15))\n\ncoin::wilcox_test(los ~ cauchy, data=wdf)\n\n\n\n    Asymptotic Wilcoxon-Mann-Whitney Test\n\ndata:  los by cauchy (0, 1)\nZ = 2.7752, p-value = 0.005517\nalternative hypothesis: true mu is not equal to 0\n\n\nWe see we are pretty close to the function output (with differences likely due to rounding in our “by-hand” calculations)."
  },
  {
    "objectID": "recitation/r12/index.html#exact-version-of-wilcoxon-rank-sum",
    "href": "recitation/r12/index.html#exact-version-of-wilcoxon-rank-sum",
    "title": "Wilcoxon Rank Sum Examples and Using Vectorized Data in Functions",
    "section": "Exact Version of Wilcoxon Rank Sum",
    "text": "Exact Version of Wilcoxon Rank Sum\nIf our sample sizes are too small (e.g., either &lt;10 based on our lecture slides), it may be more appropriate to use an exact approach instead of the normal approximation.\nIn our case, the normal approximation would be considered acceptable. However, for illustration purpose let’s assume from our nonparametric lecture slide 8/27, we have \\(n_1=9\\) and \\(n_2=10\\) (i.e., Table 12 from the Rosner textbook).\nIn this case, for \\(\\alpha=0.05\\), we have \\(T_{l}-T_{r} = 65-115\\). For \\(T=R_1=415.3846\\) we would note that \\(415.3846 = T &gt; T_r = 115\\). Then, we’d conclude from our table reference that we reject \\(H_0\\) and there is a difference in the mean ranks between our two groups."
  },
  {
    "objectID": "recitation/r12/index.html#the-wrs-in-practice",
    "href": "recitation/r12/index.html#the-wrs-in-practice",
    "title": "Wilcoxon Rank Sum Examples and Using Vectorized Data in Functions",
    "section": "The WRS In Practice",
    "text": "The WRS In Practice\nWe’ve noted many, many times that the WRS is NOT a test of the medians unless we make the somewhat strong assumption that the shapes of the distributions are identical (i.e., only the “location” of the median is changing). In practice, we often evaluate this graphically by making histograms or boxplots of our data:\n\n\nCode\n# Create vectors of our data\nnew &lt;- c(3, 3, 4, 5, 5, 5, 6, 7, 7, 8, 9, 15)\nsoc &lt;- c(6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10, 11, 13, 13, 15)\n\n# Create plot\npar(mfrow=c(2,1), mar=c(4, 3, 3, 1) )\nhist(new, xlim=c(0,16), breaks=seq(0,16,by=2), xlab='LOS (days)',\nmain='Cauchy General: LOS Histogram')\nhist(soc, xlim=c(0,16), breaks=seq(0,16,by=2), xlab='LOS (days)',\nmain='Skellam Memorial: LOS Histogram')\n\n\n\n\n\nThe big challenge is that with smaller samples, it can be really hard to determine if the shapes are really identical. If we are able or willing to get more data, we might be able to more confidently decide if the shapes are identical enough to meet the assumption. However, you may also be veering into sample sizes where the central limit theorem kicks in for the t-test (i.e., even if the underlying data isn’t normal, we know that the sample means are normally distributed with large enough sample sizes). Alternatively, we can use other tests (e.g., Mood’s test of the median between two independent samples, quantile regression) that are already testing the median."
  },
  {
    "objectID": "recitation/r12/index.html#creating-data-frames-with-vectorized-data",
    "href": "recitation/r12/index.html#creating-data-frames-with-vectorized-data",
    "title": "Wilcoxon Rank Sum Examples and Using Vectorized Data in Functions",
    "section": "Creating Data Frames with Vectorized Data",
    "text": "Creating Data Frames with Vectorized Data\nWhen using some functions, we have flexibility to provide vectors, matrices, and/or data frames for data elements. In certain cases, a function may require a specific formatting of the data. For example, coin::wilcox_test() expects data in the form of a formulaic representation (i.e., likely using a data frame for the data).\nFortunately, for our homework problem the data is relatively short and we can manually create a data frame in a few ways.\nApproach 1/1b creates a single data frame by replicating the site (Cauchy=1/Skellam=0 as a factor meet the wilcox_test expected type of data) and copying the vector of values directly or as objects:\n\n\nCode\n# Approach 1 (Alex's preferred): combine the data in one DF\ndf1 &lt;- data.frame(cauchy=factor(c(rep(1,12), rep(0,15))), \n                  los=c(3, 3, 4, 5, 5, 5, 6, 7, 7, 8, 9, 15,\n                        6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10, 11, 13, 13, 15))\ndf1[c(1:3,25:27),]\n\n\n   cauchy los\n1       1   3\n2       1   3\n3       1   4\n25      0  13\n26      0  13\n27      0  15\n\n\nCode\n# Approach 1b: we can also use the vectors of new and soc\nnew &lt;- c(3, 3, 4, 5, 5, 5, 6, 7, 7, 8, 9, 15)\nsoc &lt;- c(6, 7, 7, 7, 8, 8, 8, 9, 9, 10, 10, 11, 13, 13, 15)\n\ndf1b &lt;- data.frame(cauchy=factor(c(rep(1,12), rep(0,15))), \n                  los=c(new, soc))\ndf1b[c(1:3,25:27),]\n\n\n   cauchy los\n1       1   3\n2       1   3\n3       1   4\n25      0  13\n26      0  13\n27      0  15\n\n\nApproach 2 creates two separate data frames and then merges them together (but we need to be careful they have the same column names!):\n\n\nCode\n# Approach 2: create two separate data frames and merge\nnew &lt;- data.frame(cauchy=factor(1), \n                  los = new)\nsoc &lt;- data.frame(cauchy=factor(0),\n                  los = soc)\n\ndf2 &lt;- rbind( new, soc) # use rbind to merge two objects with same column names\ndf2[c(1:3,25:27),]\n\n\n   cauchy los\n1       1   3\n2       1   3\n3       1   4\n25      0  13\n26      0  13\n27      0  15\n\n\nCode\nclass(df2$cauchy) # check data type is still factor\n\n\n[1] \"factor\""
  },
  {
    "objectID": "recitation/r14/index.html",
    "href": "recitation/r14/index.html",
    "title": "Confidence versus Prediction Intervals in Linear Regression",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r14/index.html#base-r-plot-version-of-n20-plot",
    "href": "recitation/r14/index.html#base-r-plot-version-of-n20-plot",
    "title": "Confidence versus Prediction Intervals in Linear Regression",
    "section": "Base R Plot Version of N=20 Plot",
    "text": "Base R Plot Version of N=20 Plot\nWe can also create the figure using R’s built-in plotting functions:\n\n\nCode\nset.seed(515)\nx &lt;- rnorm(1000, mean=0, sd=10)\nerror &lt;- rnorm(1000,mean=0,sd=100)\ny &lt;- -3 + 5*x + error\n\n# create data frame of simulated data, subset to first 20 rows for small N example\ndf &lt;- data.frame(y=y, x=x)\ndf20 &lt;- df[1:20,]\n\n# fit small sample size with first 20 observations\nlm20 &lt;- lm(y ~ x, data=df20)\n\ntemp_pred &lt;- predict(lm20, interval='predict')\ndf20_pred &lt;- cbind(df20, temp_pred)\n\nplot(x=df20_pred$x, y=df20_pred$y, \n     xlab='X', ylab='Y',\n     ylim=c(-250,300)) # need to adjust y-axis limits based on CI/PI\n\nabline( lm20, lwd=2 ) # add linear regression line\n\n## calculate 95% CI and add to figure\n# first we can define a sequence of X values:\n# NOTE: By setting the grid we (1) can change the smoothness of our plotting intervals and (2) we ensure base R can plot the data since it is already in order from smallest to largest (otherwise R might jump around values of X instead of making a nice line)\nxgrid &lt;- seq(min(df20_pred$x),max(df20_pred$x),by = 1)\n\n# then we can use the predict function to get the 95% CI\n# note we define newdata as a data frame with the wtkg equal to xgrid\nconf_interval &lt;- predict(lm20, newdata=data.frame(x=xgrid), \n                         interval=\"confidence\", level = 0.95)\n\n# add the 95% CI lines\nmatlines(xgrid, conf_interval[,2:3], col = \"blue\", lty=2, lwd=2)\n\n\n## calculate 95% PI and add to figure\n# we can change interval type to prediction\npred_interval &lt;- predict(lm20, newdata=data.frame(x=xgrid), \n                         interval=\"predict\", level = 0.95)\n\n# add the 95% PI lines\nmatlines(xgrid, pred_interval[,2:3], col = \"orangered2\", lty=4)\n\n## Add legend for completeness\nlegend('top', horiz=T, inset=-0.1, xpd=T, \n       pch=c(1,NA,NA,NA), lty=c(NA,1,2,4), \n       lwd=c(NA,3,2,1), \n       col=c('black','black','blue','orangered2'),\n       legend=c('Observation','Regression Fit','95% CI','95% PI'),\n       bty='n', cex=0.8)"
  },
  {
    "objectID": "recitation/r16/index.html",
    "href": "recitation/r16/index.html",
    "title": "Hypothesis Testing in Intercept-Only, Simple, and Multiple Linear Regression Models",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r16/index.html#the-intercept-only-model",
    "href": "recitation/r16/index.html#the-intercept-only-model",
    "title": "Hypothesis Testing in Intercept-Only, Simple, and Multiple Linear Regression Models",
    "section": "The Intercept Only Model",
    "text": "The Intercept Only Model\nThe simplest model to predict what we expect the outcome to be based on our data is a linear regression model that only contains the intercept. This is equivalent to calculating the mean for our outcome, \\(\\bar{Y}\\), as we will see below.\nThe true regression model here is: \\(Y = \\beta_0 + \\epsilon ; \\; \\epsilon \\sim N(0,\\sigma^{2}_{e})\\).\nLet’s fit our intercept-only linear regression model and write our estimated/predicted regression equation. Then, we’ll highlight some points to consider with respect to the model and what we test.\n\n\nCode\nmod_intonly &lt;- lm(y ~ 1) # the \"1\" indicates that only an intercept should be fit (i.e., no predictors)\nsummary(mod_intonly) \n\n\n\nCall:\nlm(formula = y ~ 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-111.415  -13.067    4.244   34.839   83.965 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   111.08      11.58   9.592 1.02e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 51.79 on 19 degrees of freedom\n\n\nOur estimated regression model based on our data is: \\(\\hat{Y} = \\hat{\\beta}_{0} = 111.08\\).\n\nSample Mean and the Intercept (in an Intercept-Only Model)\nWe can see that this is equal to our sample mean for \\(\\bar{Y}\\):\n\n\nCode\nmean(y) # compare to mean\n\n\n[1] 111.0772\n\n\n\n\nWhere is my Overall \\(F\\)-test (in an Intercept-Only Model)?\nNotice that the regression summary output does not include the overall \\(F\\)-test. This is because the overall \\(F\\)-test evaluates the hypothesis that the beta coefficients for all our predictors are equal to 0 (i.e., \\(H_0 \\colon \\beta_1 = \\beta_2 = \\cdots = \\beta_{p} = 0\\)). However, we have no predictors in this model! If we think back to the ANOVA table:\n\n\nCode\nlibrary(kableExtra)\n\nanova_df &lt;- data.frame( 'Source' = c('Model','Error','Total'),\n                        'Sums of Squares' = c('SS~Model~','SS~Error~','SS~Total~'),\n                        'Degrees of Freedom' = c('p','n-p-1','n-1'),\n                        'Mean Square' = c('MS~Model~','MS~Error~',''),\n                        'F Value' = c('MS~Model~/MS~Error~','',''),\n                        'p-value' = c('Pr(F~p,n-p-1~ &gt; F)','',''))\n\nkbl(anova_df, col.names=c('Source','Sums of Squares','Degrees of Freedom','Mean Square','F-value','p-value'),\n    align='lccccc', escape=F) %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, position = \"left\")\n\n\n\n\n\n\n\nSource\n\n\nSums of Squares\n\n\nDegrees of Freedom\n\n\nMean Square\n\n\nF-value\n\n\np-value\n\n\n\n\n\n\nModel\n\n\nSSModel\n\n\np\n\n\nMSModel\n\n\nMSModel/MSError\n\n\nPr(Fp,n-p-1 &gt; F)\n\n\n\n\nError\n\n\nSSError\n\n\nn-p-1\n\n\nMSError\n\n\n\n\n\n\n\n\nTotal\n\n\nSSTotal\n\n\nn-1\n\n\n\n\n\n\n\n\n\n\n\nWe can see that we have no Model degrees of freedom since \\(p=0\\).\n\n\n\n\nHypothesis Testing for the Intercept\nWe can still test the null hypothesis that the intercept is equal to 0. In other words, \\(H_0\\colon \\beta_0=0\\) versus \\(H_1\\colon \\beta_0 \\neq 0\\). While we can conduct these tests “by hand”, we can also leverage our regression output from above to note that \\(t=\\) 9.592 and \\(p=\\) 1.02e-08 \\(&lt;0.001\\).\nIf we wanted to verify or calculate these values by hand, we see that \\(t = \\frac{111.08}{11.58} = 9.592\\) and we can compare this to a \\(t_{n-p-1} = t_{n-1} = t_{19}\\) distribution to calculate our p-value: 2*pt(9.592, df=19, lower.tail=F) = 1.0251487^{-8}. Recall, we double our p-value since we have a two-sided hypothesis test.\nIn this case, since \\(p&lt;0.001\\), we reject our null hypothesis and conclude that the intercept is significantly different from 0.\n\n\nHypothesis Testing for Other Null Values\nThe default value we tend to use for our hypothesis test of the intercept and slope terms is against a null of 0. However, we can test whatever value we wish (and might be more biologically meaningful). For example, in our calculation above “by hand” we could test \\(H_0\\colon \\beta_0=100\\) versus \\(H_1\\colon \\beta_0 \\neq 100\\):\n\\[ t = \\frac{\\hat{\\beta}_{0} - \\beta_{0}}{SE(\\hat{\\beta}_{0})} = \\frac{111.08 - 100}{11.58} = \\frac{11.08}{11.58} = 0.9568221 \\]\nWe can calculate our p-value similarly to our above example: 2*pt(0.9568221, df=19, lower.tail=F) = 0.3506744. Since \\(p&gt;0.05\\), we fail to reject the null hypothesis that the intercept is significantly different from 100.\nThis idea also extends to our simple and multiple linear regression models, where we can test any of our beta coefficients against any proposed null value of interest."
  },
  {
    "objectID": "recitation/r16/index.html#the-simple-linear-regression-model",
    "href": "recitation/r16/index.html#the-simple-linear-regression-model",
    "title": "Hypothesis Testing in Intercept-Only, Simple, and Multiple Linear Regression Models",
    "section": "The Simple Linear Regression Model",
    "text": "The Simple Linear Regression Model\nFrom our scatterplots above (and because we simulated the data), we know that there is additional information beyond just our outcome that is useful to incorporate. For example, \\(X_1\\) in our simulation has \\(\\beta_1 = 2\\), so the outcome should be better described by incorporating this information. This, in turn, provides a more accurate estimate of what an observation’s outcome should be than just naively using \\(\\bar{Y}\\) for everyone.\nThe true regression model here is: \\(Y = \\beta_0 + \\beta_1 X_{1} + \\epsilon ; \\; \\epsilon \\sim N(0,\\sigma^{2}_{e})\\).\nLet’s fit our simple linear regression model and write our estimated/predicted regression equation. Then, we’ll highlight some points to consider with respect to the model and what we test.\n\n\nCode\nmod_slr &lt;- lm(y ~ x1)\nsummary(mod_slr) \n\n\n\nCall:\nlm(formula = y ~ x1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.181 -10.477  -2.215   9.139  31.422 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 104.6118     3.6339   28.79  &lt; 2e-16 ***\nx1            2.6131     0.1956   13.36 8.82e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.11 on 18 degrees of freedom\nMultiple R-squared:  0.9084,    Adjusted R-squared:  0.9033 \nF-statistic: 178.4 on 1 and 18 DF,  p-value: 8.824e-11\n\n\nOur estimated regression model based on our data is: \\(\\hat{Y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}X_{1} = 104.6118 + 2.6131 X_{1}\\).\nWe can add this line to earlier scatterplot before examining how to test various questions of interest:\n\n\nCode\nplot(x1,y)\nabline(mod_slr)\n\n\n\n\n\n\nHypothesis Testing for the Intercept\nLike in our intercept-only model, we can test a null hypothesis that the intercept is equal to 0:\n\\[ H_0\\colon \\beta_0 = 0 \\text{ vs. } H_1\\colon \\beta_0 \\neq 0 \\]\nThis information can be easily extracted from our summary table output, where we see that \\(t=28.79\\) and \\(p&lt;0.001\\). Therefore, we would reject the null hypothesis that the intercept is equal to 0. In other words, based on our sample and the simple linear regression model we fit, when the predictor \\(X_1 = 0\\), the predicted average value for our outcome is 104.6118.\nIn many cases, the context implies that the intercept is nonsensical to interpret or it relies on extrapolation of our data. For example, if we had a sample of 30-60 year olds where we measured the amount of time they spent using a computer each week, the intercept would represent the average computer use for a 0 year old. This makes no sense given a 0 year old was literally just born and is highly unlikely to actively use a computer on their own. Further, our sample only included 30-60 year olds, so even if a 0 year old could use a computer, we would have to extrapolate our findings down to a 0 year old, which may not reflect their actual use pattern anyway.\n\n\nHypothesis Testing for the Slope\nMore often, we are interested in testing if the beta coefficient for our predictor (often called the slope) is significantly different from 0:\n\\[ H_0\\colon \\beta_1 = 0 \\text{ vs. } H_1\\colon \\beta_1 \\neq 0 \\]\nIn the context of linear regression, a slope of 0 would indicate that there is no change in the outcome (\\(Y\\)) across the range of the predictor (\\(X_1\\)).\nFor our SLR model, we can likewise extract the information from our summary table, where we see that \\(t=13.36\\) and \\(p&lt;0.001\\). Therefore, we would reject the null hypothesis that the slope is 0.\nWe can also compare the output p-value to one we calculate in R based on the \\(t\\) statistic: 2*pt(summary(mod_slr)$coefficients['x1','t value'], df=18, lower.tail=F)= 8.824259^{-11}. Recall, we are using a \\(t_{n-p-1} = t_{20-1-1} = t_{18}\\) distribution as our reference.\n\n\nHypothesis Testing for the Overall Model Fit (the Overall \\(F\\)-test)\nA final question that may be of interest if is the overall model with all predictors is better at predicting the outcome than just the sample mean of the outcome alone. In other words, is it useful to include all the predictors I may have in my model? Or have I made something more complicated than just using \\(\\bar{Y}\\) that doesn’t give me any benefit?\nTo address this question, we can conduct an overall \\(F\\)-test. This examines the hypothesis for our simple linear regression that\n\\[ H_0\\colon \\beta_1 = 0 \\text{ vs. } H_1\\colon \\beta_1 \\neq 0 \\]\nThis should look familiar, since it is the same hypotheses we evaluated for hypothesis testing of the slope! For simple linear regression, we actually have a direct relationship between the \\(t\\) and \\(F\\) statistics, since they are evaluating identical hypotheses.\nFrom our summary output, we see that \\(F=178.4\\), which is equal (after considering the output has rounded to only 2 decimal places) to \\(t^2 = (13.36)^2 = 178.4896\\).\n\n\nWhat About SLR \\(X_2\\) and \\(X_3\\)?\nIn our simulation, we set both \\(X_2\\) and \\(X_3\\) to have beta coefficients equal to 0. If we run a simple linear regression model and examine the summary output, we see:\n\n\nCode\nsummary(lm(y ~ x2))\n\n\n\nCall:\nlm(formula = y ~ x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-73.402 -41.576   2.492  37.701  70.352 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  150.273     20.925   7.182  1.1e-06 ***\nx2            -8.101      3.730  -2.172   0.0435 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 47.36 on 18 degrees of freedom\nMultiple R-squared:  0.2076,    Adjusted R-squared:  0.1636 \nF-statistic: 4.717 on 1 and 18 DF,  p-value: 0.04347\n\n\nSince \\(p=0.0435 &lt; 0.05\\) we would reject the null hypothesis that the slope is equal to 0. Since we simulated a null setting, this represents a type I error!\n\n\nCode\nsummary(lm(y ~ x3))\n\n\n\nCall:\nlm(formula = y ~ x3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-93.839 -34.215   5.726  29.999 101.541 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    93.50      13.89    6.73 2.62e-06 ***\nx3             43.94      21.97    2.00   0.0608 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48.12 on 18 degrees of freedom\nMultiple R-squared:  0.1819,    Adjusted R-squared:  0.1364 \nF-statistic: 4.002 on 1 and 18 DF,  p-value: 0.06077\n\n\nSince \\(p=0.0608 &gt; 0.05\\), we would fail to reject the null hypothesis."
  },
  {
    "objectID": "recitation/r16/index.html#the-multiple-linear-regression-model",
    "href": "recitation/r16/index.html#the-multiple-linear-regression-model",
    "title": "Hypothesis Testing in Intercept-Only, Simple, and Multiple Linear Regression Models",
    "section": "The Multiple Linear Regression Model",
    "text": "The Multiple Linear Regression Model\nEven though we know from our simulation that \\(X_2\\) and \\(X_3\\) are not used in the calculation of \\(Y\\), in the real world we would not know this information. Rather, we might want to fit a multiple linear regression model that includes all possible predictors. Under this model, we are assuming that the true regression model is: \\(Y = \\beta_0 + \\beta_1 X_{1} + \\beta_{2} X_{2} + \\beta_{3} X_{3} + \\epsilon ; \\; \\epsilon \\sim N(0,\\sigma^{2}_{e})\\). NOTE: We know that this is technically the “wrong” true model based on our simulation, but it is the underlying model we are trying to estimate when fitting this multiple linear regression model.\nLet’s fit our multiple linear regression model and write our estimated/predicted regression equation. Then, we’ll highlight some points to consider with respect to the model and what we test.\n\n\nCode\nmod_mlr &lt;- lm(y ~ x1 + x2 + x3)\nsummary(mod_mlr) \n\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.392 -13.180   1.295   7.935  24.401 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  93.1351     9.7269   9.575 5.01e-08 ***\nx1            2.7714     0.2491  11.127 6.09e-09 ***\nx2            2.1181     1.5659   1.353    0.195    \nx3            2.0920     8.4005   0.249    0.807    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.18 on 16 degrees of freedom\nMultiple R-squared:  0.9178,    Adjusted R-squared:  0.9023 \nF-statistic: 59.52 on 3 and 16 DF,  p-value: 6.722e-09\n\n\nOur estimated regression model based on our data is:\n\\[ \\hat{Y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}X_{1} + \\hat{\\beta}_{2}X_{2} + \\hat{\\beta}_{3}X_{3}= 93.1351 + 2.7714 X_{1} + 2.1181 X_{2} + 2.0920 X_{3}. \\]\n\nTesting the Overall Model Fit (the Overall \\(F\\)-Test)\nOne question we might ask, is if any of our predictors contribute meaningfully to the prediction of \\(Y\\) above and beyond just using \\(\\bar{Y}\\) alone. This is evaluated using our overall \\(F\\)-test, where the hypotheses are\n\\[ H_0\\colon \\beta_1 = \\beta_2 = \\beta_3 = 0 \\text{ vs. } H_1\\colon \\text{ at least one } \\beta_{k} \\neq 0 \\]\nWe can directly extract this information from our output, where we see \\(F=59.52\\) with \\(p&lt;0.001\\). Therefore, we reject \\(H_0\\) and conclude that at least one of our beta coefficients is significantly different from 0. However, we have no idea which beta coefficient(s) are significantly different from 0 without further investigation.\nFrom the output, it is worth highlighting that we are making a comparison to an \\(F_{k,n-k-1}\\) distribution, where \\(k\\) is the number of predictors: \\(F_{3,20-3-1} = F_{3,16}\\).\n\n\nTesting a Subset of the Predictors (the Partial \\(F\\)-Test)\nAnother question we may be interested in answering in a multiple linear regression model, is if some subset of our predictors contribute meaningfully to the prediction of \\(Y\\) above and beyond the reduced model that excludes those predictors. This can be tested with the partial \\(F\\)-test. For example, we may want to test if adding \\(X_{2}\\) and \\(X_{3}\\) is meaningful above and beyond just including \\(X_{1}\\) in our model:\n\\[ H_0\\colon \\beta_2 = \\beta_3 = 0 \\text{ vs. } H_1\\colon \\text{ at least one } \\beta_{k} \\neq 0 \\; (k \\in 2,3) \\]\nIn our Week 10 Practice Problem Solutions, we have an example of implementing a partial F-test where you can generate the ANOVA tables for the full and reduced models and calculate the partial F-statistic by hand. For brevity, we’ll focus here on using the anova function to compare our models:\n\n\nCode\nanova(mod_mlr, mod_slr, test='F')\n\n\nAnalysis of Variance Table\n\nModel 1: y ~ x1 + x2 + x3\nModel 2: y ~ x1\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     16 4190.3                           \n2     18 4669.5 -2   -479.19 0.9149 0.4205\n\n\nHere we see that our partial \\(F\\)-test has \\(F=0.9149\\) and \\(p=0.4205\\). In other words, since \\(p&gt;0.05\\) we fail to reject the null hypothesis that at least one of our beta coefficients for \\(X_2\\) and \\(X_3\\) is not equal to 0. This suggests that we could focus on the simple linear regression model with just \\(X_1\\) for a simpler, more parsimonious model. (The big potential caveat here for the real world is that variables may still be meaningful if they serve as a confounder, mediator, moderator, precision variable, etc. and your context may still suggest you keep a statistically insignificant predictor in your model!)\n\n\nTesting a Single Predictor (the \\(t\\)-test)\nThe third question we may want to investigate is if a specific predictor is significantly adding to our model, above and beyond the information contributed by other variables in the model. Assuming we still have our full multiple linear regression model with 3 predictors, the major change here is our interpretation from a simple linear regression model.\nFocusing on \\(X_1\\), we see that \\(t=11.127\\) and \\(p&lt;0.001\\), so we would reject the null hypothesis that the slope is 0 (i.e., that \\(\\beta_1 = 0\\)). This estimate of our estimate and standard error (and therefore the \\(t\\)-value and p-value) are dependent upon the other predictors in our model. For example, our t-test here is based on \\(t_{n-p-1} = t_{20-3-1} = t_{16}\\), but in our SLR it was based on \\(t_{18}\\).\nWith regards to interpretation, it is mostly similar with a small bit of added language:\n\nSLR Interpretation of \\(\\beta_1\\): For a one unit increase in \\(X_1\\), the average change in \\(Y\\) is \\(\\beta_{1}\\).\nMLR Interpretation of \\(\\beta_1\\): For a one unit increase in \\(X_1\\), the average change in \\(Y\\) is \\(\\beta_{1}\\) after accounting for other variables in the model.\n\nFor the MLR interpretation, other similar phrasings include after adjusting for other variables in the model, after fixing our other predictors, after fixing \\(X_{2}\\) and \\(X_{3}\\), etc. We need some form of language that notes the estimate is conditional on the other predictors included in our model."
  },
  {
    "objectID": "recitation/r16/index.html#sanity-check-what-if-we-didnt-simulate-error",
    "href": "recitation/r16/index.html#sanity-check-what-if-we-didnt-simulate-error",
    "title": "Hypothesis Testing in Intercept-Only, Simple, and Multiple Linear Regression Models",
    "section": "Sanity Check: What If We Didn’t Simulate Error?",
    "text": "Sanity Check: What If We Didn’t Simulate Error?\nAs a final note, what would happen if we didn’t include the error term in our calculation of y above? Let’s see what our regression model would return for the SLR case:\n\n\nCode\ny_no_error &lt;- 100 + 2*x1 + 0*x2 + 0*x3 #exclude our earlier error term\nsummary(lm(y_no_error ~ x1))\n\n\nWarning in summary.lm(lm(y_no_error ~ x1)): essentially perfect fit: summary\nmay be unreliable\n\n\n\nCall:\nlm(formula = y_no_error ~ x1)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-2.275e-14 -1.142e-14 -6.265e-15 -4.132e-15  1.560e-13 \n\nCoefficients:\n             Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept) 1.000e+02  8.609e-15 1.162e+16   &lt;2e-16 ***\nx1          2.000e+00  4.635e-16 4.315e+15   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.816e-14 on 18 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 1.862e+31 on 1 and 18 DF,  p-value: &lt; 2.2e-16\n\n\nThere is a warning noting we have a perfect fit, so the summary may be unreliable. Indeed, we see that without the error term, we can perfectly predict our beta coefficients and have no variability around the estimates:\n\n\nCode\nplot(x1, y_no_error)\nabline(lm(y_no_error ~ x1))\n\n\n\n\n\nIn reality, this is very rare (if not almost impossible). Not only is there usually natural variability in the data even if we had perfect measurement, there are other sources of error (e.g., measurement error in how we collect data that adds variability). If you do wind up with a solution with perfect prediction, it is worth thinking through the model, data, and results before too widely disseminating your findings."
  },
  {
    "objectID": "recitation/r18/index.html",
    "href": "recitation/r18/index.html",
    "title": "General Linear Hypothesis Testing (GLHT) and Matrices, Contrasts, and F-Tests",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r18/index.html#contrasts",
    "href": "recitation/r18/index.html#contrasts",
    "title": "General Linear Hypothesis Testing (GLHT) and Matrices, Contrasts, and F-Tests",
    "section": "Contrasts",
    "text": "Contrasts\nIn the most general sense, contrasts are differences between regression coefficients. Further, the comparison of coefficients must sum to 0: \\(\\sum c_i = 0\\).\nFor example, if our regression model was comparing some outcome between never, former, and current smokers we might have:\n\\[ \\hat{Y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{\\text{former}} I_{F} + \\hat{\\beta}_{\\text{current}} I_{C} \\]\nIf we wanted to compare (1) never and former or (2) never and current smokers we would be able to simply evaluate our output’s regression coefficient table and interpret the \\(t\\)-test/p-value provided.\nIf we wanted to compare if the mean levels were the same between former and current smokers, we could propose a contrast: \\((0, 1, -1)\\). This would translate to \\(H_0 \\colon \\hat{\\beta}_{\\text{former}} - \\hat{\\beta}_{\\text{current}} = 0\\).\nWe can make a more direct connection to the mean for each group if we recall:\n\n\\(\\mu_{\\text{never}} = \\hat{\\beta}_{0}\\)\n\\(\\mu_{\\text{former}} = \\hat{\\beta}_{0} + \\hat{\\beta}_{\\text{former}}\\)\n\\(\\mu_{\\text{current}} = \\hat{\\beta}_{0} + \\hat{\\beta}_{\\text{current}}\\)\n\nThen we can see what is being compared for each pairwise comparison (and a more direct connection to ANOVA):\n\nNever vs. Former: \\((\\hat{\\beta}_{0} + \\hat{\\beta}_{\\text{former}}) - \\hat{\\beta}_{0} = \\hat{\\beta}_{\\text{former}}\\)\nNever vs. Current: \\((\\hat{\\beta}_{0} + \\hat{\\beta}_{\\text{current}}) - \\hat{\\beta}_{0} = \\hat{\\beta}_{\\text{current}}\\)\nCurrent vs. Former: \\((\\hat{\\beta}_{0} + \\hat{\\beta}_{\\text{former}}) - (\\hat{\\beta}_{0} + \\hat{\\beta}_{\\text{current}}) = \\hat{\\beta}_{\\text{former}} - \\hat{\\beta}_{\\text{current}}\\)\n\nOur default hypothesis test for each of these is that there is no difference (e.g., 0).\nWhile this is meant to be a general comparison of regression coefficients, it is restricted in that the contrast must sum to 0. We are also restricted to testing if the difference is 0.\n\nContrasts Example\nLet’s fit a reference cell model first between beta-carotene in the diet as our outcome and smoking status as our predictor:\n\n\nCode\nmod1 &lt;- glm(betadiet ~ AK_smoke_f, data=dat)\nsummary(mod1)\n\n\n\nCall:\nglm(formula = betadiet ~ AK_smoke_f, data = dat)\n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         2.1934     0.1168  18.771   &lt;2e-16 ***\nAK_smoke_fFormer    0.1618     0.1797   0.901   0.3685    \nAK_smoke_fCurrent  -0.4899     0.2520  -1.944   0.0528 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 2.143603)\n\n    Null deviance: 682.12  on 314  degrees of freedom\nResidual deviance: 668.80  on 312  degrees of freedom\nAIC: 1139.1\n\nNumber of Fisher Scoring iterations: 2\n\n\nWe see from our output, that there is no statistically significant difference between never smokers and former or current smokers (p=0.3685, p=0.0528, respectively). However, if we wished to compare the former and current smokers we would need to specify our contrast:\n\n\nCode\nc &lt;- matrix( c(0,1,-1), nrow=1 )\nsummary(glht(mod1,linfct=c))\n\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: glm(formula = betadiet ~ AK_smoke_f, data = dat)\n\nLinear Hypotheses:\n       Estimate Std. Error z value Pr(&gt;|z|)  \n1 == 0   0.6517     0.2617    2.49   0.0128 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nWe see here that p=0.0128, so we’d reject the null hypothesis that former and current smokers have equal means for the dietary beta-carotene.\n\n\nContrasts with Cell Means Models\nOne of the natural applications for contrasts is with cell means models for regression:\n\n\nCode\nmod1_cellmeans &lt;- glm(betadiet ~ AK_smoke_f - 1, data=dat)\nsummary(mod1_cellmeans)\n\n\n\nCall:\nglm(formula = betadiet ~ AK_smoke_f - 1, data = dat)\n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nAK_smoke_fNever     2.1934     0.1168   18.77  &lt; 2e-16 ***\nAK_smoke_fFormer    2.3552     0.1365   17.25  &lt; 2e-16 ***\nAK_smoke_fCurrent   1.7035     0.2233    7.63 2.87e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 2.143603)\n\n    Null deviance: 2186.8  on 315  degrees of freedom\nResidual deviance:  668.8  on 312  degrees of freedom\nAIC: 1139.1\n\nNumber of Fisher Scoring iterations: 2\n\n\nWe see in our summary output that all of our means are significantly different from 0, but we haven’t completed any pairwise comparisons. We can specify three contrasts to replicate our results above from the reference cell model:\n\n\nCode\nc_cellmeans &lt;- matrix( c(1,-1,0, \n                         1,0,-1, \n                         0,1,-1), \n                       nrow=3, byrow=T ) # specify a matrix with 3 rows (one for each contrast)\n\npairwise_cellmeans &lt;- glht(mod1_cellmeans, linfct=c_cellmeans)\nsummary(pairwise_cellmeans, test=adjusted(\"none\"))\n\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: glm(formula = betadiet ~ AK_smoke_f - 1, data = dat)\n\nLinear Hypotheses:\n       Estimate Std. Error z value Pr(&gt;|z|)  \n1 == 0  -0.1618     0.1797  -0.901   0.3678  \n2 == 0   0.4899     0.2520   1.944   0.0519 .\n3 == 0   0.6517     0.2617   2.490   0.0128 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- none method)\n\n\nThis reflects our results from before!\nWe can also note that summary.glht includes options to adjust for multiplicity (the default is a “single-step method” adjustment):\n\n\nCode\nsummary(pairwise_cellmeans)\n\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: glm(formula = betadiet ~ AK_smoke_f - 1, data = dat)\n\nLinear Hypotheses:\n       Estimate Std. Error z value Pr(&gt;|z|)  \n1 == 0  -0.1618     0.1797  -0.901   0.6353  \n2 == 0   0.4899     0.2520   1.944   0.1233  \n3 == 0   0.6517     0.2617   2.490   0.0333 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nWith these specific results, we see that our conclusions would not change for each pairwise test even after accounting for multiple testing."
  },
  {
    "objectID": "recitation/r18/index.html#general-linear-hypothesis-testing",
    "href": "recitation/r18/index.html#general-linear-hypothesis-testing",
    "title": "General Linear Hypothesis Testing (GLHT) and Matrices, Contrasts, and F-Tests",
    "section": "General Linear Hypothesis Testing",
    "text": "General Linear Hypothesis Testing\n\nTesting Differences Other Than 0\nWe can generalize contrasts to test for differences that aren’t just 0. For example, we might wish to revise our cell means result to compare to a difference of 0.5 between groups:\n\n\nCode\nc_cellmeans &lt;- matrix( c(1,-1,0, \n                         1,0,-1, \n                         0,1,-1), \n                       nrow=3, byrow=T ) # specify a matrix with 3 rows (one for each contrast)\npairwise_cellmeans_glht &lt;- glht(mod1_cellmeans, linfct=c_cellmeans, rhs=c(0.5,0.5,0.5))\nsummary(pairwise_cellmeans_glht, test=adjusted(\"none\"))\n\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: glm(formula = betadiet ~ AK_smoke_f - 1, data = dat)\n\nLinear Hypotheses:\n         Estimate Std. Error z value Pr(&gt;|z|)    \n1 == 0.5  -0.1618     0.1797  -3.683 0.000231 ***\n2 == 0.5   0.4899     0.2520  -0.040 0.967883    \n3 == 0.5   0.6517     0.2617   0.580 0.562171    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- none method)\n\n\nWe can see that the linear hypothesis test is now comparing our estimates to 0.5 instead of 0. It makes sense that the first pairwise difference (Never-Former) is now significant, but the other two are not. However, direction does matter since we aren’t testing if the difference is either positive or negative 0.5, just +0.5.\n\n\nComparisons with \\(\\sum c_i \\neq 0\\)\nWith GLHT, we can also compare any combination of interest, even if it doesn’t sum to 0.\nFor example, we can compare if the means for current smokers is equal to half that of never smokers. This null hypothesis is \\(\\frac{1}{2}\\mu_{N} = \\mu_{C} \\implies \\frac{1}{2}\\mu_{N} + 0 \\mu_F + -1 \\mu_{C} = 0\\):\n\n\nCode\nc3_cellmeans &lt;- matrix(c(0.5,0,-1), nrow=1)\nsummary(glht(mod1_cellmeans,linfct=c3_cellmeans))\n\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: glm(formula = betadiet ~ AK_smoke_f - 1, data = dat)\n\nLinear Hypotheses:\n       Estimate Std. Error z value Pr(&gt;|z|)   \n1 == 0  -0.6068     0.2308  -2.629  0.00855 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nWe can also specify this in terms of the reference cell model with\n\\[ \\frac{1}{2}\\mu_{N} + 0 \\mu_F + -1 \\mu_{C} = 0 \\implies \\frac{1}{2}\\hat{\\beta}_{0} - (\\hat{\\beta}_{0} + \\hat{\\beta}_{C}) = 0 \\implies -\\frac{1}{2}\\hat{\\beta}_{0} - \\hat{\\beta}_{C} = 0 \\]\n\n\nCode\nc3 &lt;- matrix(c(-0.5,0,-1), nrow=1)\nsummary(glht(mod1,linfct=c3))\n\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: glm(formula = betadiet ~ AK_smoke_f, data = dat)\n\nLinear Hypotheses:\n       Estimate Std. Error z value Pr(&gt;|z|)   \n1 == 0  -0.6068     0.2308  -2.629  0.00855 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nWe see the reference cell and cell means model results match, even though our contrasts were:\n\\[\n\\begin{matrix} \\text{Reference Cell} \\\\ \\text{Cell Means} \\end{matrix}\n\\begin{matrix} = \\\\ = \\end{matrix}\n\\begin{pmatrix}\n-0.5 & 0 & -1 \\\\\n0.5 & 0 & -1\n\\end{pmatrix}\n\\]\n\n\nMore Complex Models\nWe can also play around with far more complex models and use our GLHT to estimate various effects, combinations of predictors, and/or compare to hypotheses that are equal to 0 or any other value. Consider:\n\n\nCode\nglm_be_cray &lt;- glm(betadiet ~ AK_smoke_f + bmi + age, data=dat)\nsummary(glm_be_cray)\n\n\n\nCall:\nglm(formula = betadiet ~ AK_smoke_f + bmi + age, data = dat)\n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        2.012354   0.495050   4.065  6.1e-05 ***\nAK_smoke_fFormer   0.161407   0.180366   0.895   0.3715    \nAK_smoke_fCurrent -0.461002   0.257023  -1.794   0.0738 .  \nbmi               -0.003575   0.013861  -0.258   0.7966    \nage                0.005400   0.005750   0.939   0.3484    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 2.150732)\n\n    Null deviance: 682.12  on 314  degrees of freedom\nResidual deviance: 666.73  on 310  degrees of freedom\nAIC: 1142.1\n\nNumber of Fisher Scoring iterations: 2\n\n\nIf we wanted to compare a 35-year old nonsmoker and 50-year old former smoker with the same BMI, we would have:\n\\[\n\\begin{align}\n&\\hat{\\beta}_0 &+ 0 \\hat{\\beta}_{F} &+ 0 \\hat{\\beta}_{C} &+ X_{BMI}\\hat{\\beta}_{BMI} &+ 35 \\hat{\\beta}_{age} \\\\\n- (&\\hat{\\beta}_0 &+ 1 \\hat{\\beta}_{F} &+ 0 \\hat{\\beta}_{C} &+ X_{BMI}\\hat{\\beta}_{BMI} &+ 50 \\hat{\\beta}_{age}) \\\\\n\\hline\n&0\\hat{\\beta}_0 & -1 \\hat{\\beta}_{F} & \\;\\;\\;\\;0 \\hat{\\beta}_{C} & 0 \\hat{\\beta}_{BMI} & -15 \\hat{\\beta}_{age}\n\\end{align}\n\\]\n\n\nCode\nc4 &lt;- matrix(c(0,-1,0,0,-15), nrow=1)\nsummary(glht(glm_be_cray,linfct=c4))\n\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: glm(formula = betadiet ~ AK_smoke_f + bmi + age, data = dat)\n\nLinear Hypotheses:\n       Estimate Std. Error z value Pr(&gt;|z|)\n1 == 0  -0.2424     0.2012  -1.205    0.228\n(Adjusted p values reported -- single-step method)\n\n\nFrom this test result we see that there is no significant difference between these two combinations based on our regression output.\nWe could also calculate a confidence interval around our estimate:\n\n\nCode\nconfint(glht(glm_be_cray,linfct=c4))\n\n\n\n     Simultaneous Confidence Intervals\n\nFit: glm(formula = betadiet ~ AK_smoke_f + bmi + age, data = dat)\n\nQuantile = 1.96\n95% family-wise confidence level\n \n\nLinear Hypotheses:\n       Estimate lwr     upr    \n1 == 0 -0.2424  -0.6367  0.1519\n\n\nNote that it summarize our results as “simultaneous CIs” to control the 95% family-wise confidence level. In other words, if we were testing multiple hypotheses, it would apply a correction to not just our p-value, but also the confidence intervals:\n\n\nCode\nc4_plus2random &lt;- matrix(c(0,-1,0,0,-15, \n                           1,0,0,10,0, \n                           0,-1,1,10,10), \n                         nrow=3, byrow=T)\nconfint(glht(glm_be_cray, linfct=c4_plus2random))\n\n\n\n     Simultaneous Confidence Intervals\n\nFit: glm(formula = betadiet ~ AK_smoke_f + bmi + age, data = dat)\n\nQuantile = 2.3513\n95% family-wise confidence level\n \n\nLinear Hypotheses:\n       Estimate lwr     upr    \n1 == 0 -0.2424  -0.7155  0.2307\n2 == 0  1.9766   1.0396  2.9136\n3 == 0 -0.6042  -1.3575  0.1492\n\n\nThe first CI is now wider, reflecting the correction for multiple testing to maintain the family-wise type I error rate."
  },
  {
    "objectID": "recitation/r18/index.html#joint-testing",
    "href": "recitation/r18/index.html#joint-testing",
    "title": "General Linear Hypothesis Testing (GLHT) and Matrices, Contrasts, and F-Tests",
    "section": "Joint Testing",
    "text": "Joint Testing\nOne aspect we haven’t covered yet, what if you wish to jointly test multiple hypotheses at the same time (e.g., like an overall \\(F\\)-test)? This would account for the potential of multiple testing and may better reflect specific combinations of hypotheses.\nIn one case, we might have a specific question of interest that relies on specifying multiple hypotheses. For instance, what if we believed that our dietary beta-carotene should have a linear trend where former smokers have a 0.5 higher level than never smokers, and current smokers are 1.0 higher level than never smokers:\n\n\nCode\n# returning to our simpler reference cell model with just smoking status\nK &lt;- matrix( c(0,1,0,\n               0,0,1),\n             nrow=2, byrow=T)\nhyp_val &lt;- c(0.5,1.0)\n\njoint_test &lt;- glht(mod1, linfct = K, rhs = hyp_val)\n\n# Specify a joint test with test=Ftest() that conducts an overall F-test for the two hypotheses\nsummary(joint_test, test=Ftest())\n\n\n\n     General Linear Hypotheses\n\nLinear Hypotheses:\n         Estimate\n1 == 0.5   0.1618\n2 == 1    -0.4899\n\nGlobal Test:\n      F DF1 DF2    Pr(&gt;F)\n1 17.48   2 312 6.366e-08\n\n\nThis \\(F\\)-test indicates that we would reject the joint null hypothesis, at least one of the null hypotheses would be rejected.\nA natural follow-up question would be, which one(s) do we reject? To correct for multiple comparisons, we see\n\n\nCode\n# For comparison, summarize the family-wise corrected individual tests\nsummary(joint_test)\n\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: glm(formula = betadiet ~ AK_smoke_f, data = dat)\n\nLinear Hypotheses:\n         Estimate Std. Error z value Pr(&gt;|z|)    \n1 == 0.5   0.1618     0.1797  -1.882    0.113    \n2 == 1    -0.4899     0.2520  -5.912 6.76e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nIn other words, we would reject our second null hypothesis that \\(H_0 \\colon \\beta_{C} = 1\\), but not the first that \\(H_0 \\colon \\beta_{F} = 0.5\\).\nIn practice, I tend to favor conducting the hypothesis test that directly addresses the question of interest (especially if you end up correcting for multiple comparisons anyway). However, there are likely cases where a global test of a joint hypothesis is most appropriate and efficient."
  },
  {
    "objectID": "recitation/r18/index.html#f-tests-with-matrices-versus-anova-and-connections-to-joint-testing",
    "href": "recitation/r18/index.html#f-tests-with-matrices-versus-anova-and-connections-to-joint-testing",
    "title": "General Linear Hypothesis Testing (GLHT) and Matrices, Contrasts, and F-Tests",
    "section": "F-tests with Matrices versus anova and Connections to Joint Testing",
    "text": "F-tests with Matrices versus anova and Connections to Joint Testing\nThe \\(F\\)-test can be used to test the GLH: \\[\\begin{equation*}\nF = \\frac{[(\\mathbf{c}\\hat{\\boldsymbol{\\beta}}-\\mathbf{d})^T(\\mathbf{c}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{c}^T)^{-1}(\\mathbf{c}\\hat{\\boldsymbol{\\beta}}-\\mathbf{d})/r]}{\\hat{\\sigma}^2_{Y|X}} \\sim F_{r,n-p-1}\n\\end{equation*}\\]\nwhere \\(\\hat{\\sigma}^2_{Y|X}\\) is our MSE from the full model. This reduces to our Partial \\(F\\)-test for testing a group of variables, because\n\\[\\begin{equation*}\n(\\mathbf{c}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{c}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{c}^T)^{-1}(\\mathbf{c}\\hat{\\boldsymbol{\\beta}}) = SS_{model}(full)-SS_{model}(reduced)\n\\end{equation*}\\]\nTherefore, a test of a single linear hypothesis for a single parameter also reduces to our \\(t\\)-test.\nNote, the above formulas do NOT require us to refit a “reduced” model to compare to our full model. Instead, all of our needed information is contained within our design matrix and our \\(\\mathbf{c}_{r \\times p}\\) matrix of rank \\(r\\) where \\(r \\leq p\\) (i.e., number of rows that are “unique” (i.e., not made up of other combinations of rows)).\nLet’s compare the overall F-test for the following model using matrices or by fitting a reduced model and using the anova() function:\n\n\nCode\n### Model-based approach\nglm_be_cray &lt;- glm(betadiet ~ AK_smoke_f + bmi + age, data=dat)\nglm_be_null &lt;- glm(betadiet ~ 1, data=dat)\nanova(glm_be_cray, glm_be_null, test='F')\n\n\nAnalysis of Deviance Table\n\nModel 1: betadiet ~ AK_smoke_f + bmi + age\nModel 2: betadiet ~ 1\n  Resid. Df Resid. Dev Df Deviance      F Pr(&gt;F)\n1       310     666.73                          \n2       314     682.12 -4  -15.388 1.7887 0.1309\n\n\nCode\n### Matrix approach\n# Create indicators for smoking categories\ndat$AK_smoke_former &lt;- dat$smoke==2\ndat$AK_smoke_current &lt;- dat$smoke==3\n\nY &lt;- dat$betadiet\nYtY &lt;- t(Y) %*% Y\nX &lt;- cbind(1, dat$AK_smoke_former, dat$AK_smoke_current, dat$bmi, dat$age)\nXt &lt;- t(X)\nXtX &lt;- t(X) %*% X\nXtX_inv &lt;- solve(XtX)\nbeta_vec &lt;- XtX_inv %*% t(X) %*% Y\nYhat &lt;- X %*% beta_vec\nmse &lt;- ( YtY - t(beta_vec) %*% t(X) %*% Y ) / ( length(Y) - ncol(X) )\nc &lt;- matrix( c(0,1,0,0,0,\n               0,0,1,0,0,\n               0,0,0,1,0,\n               0,0,0,0,1), ncol=ncol(X), byrow=T)\nd &lt;- matrix( c(0,0,0,0), ncol=1 )\nFstat_num &lt;- ( t(c%*%beta_vec - d) %*% solve(c %*% XtX_inv %*% t(c)) %*% (c%*%beta_vec - d) ) / nrow(c)\nFstat &lt;- Fstat_num / mse\nFstat\n\n\n         [,1]\n[1,] 1.788729\n\n\nWe can see that we’ve arrived at the same result, but without having to fit the reduced model!\nThe connection between the partial F-test and the GLHT may help us better draw connections between our formulas and the two approaches:\n\\[ F = \\frac{[SS_{model}(full) - SS_{model}(reduced)]/k^{*}}{MS_{error}(full)} \\sim F_{k^{*},n-p^{*}-k^{*}-1} \\] \\[ F = \\frac{[(\\mathbf{c}\\hat{\\boldsymbol{\\beta}}-\\mathbf{d})^T(\\mathbf{c}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{c}^T)^{-1}(\\mathbf{c}\\hat{\\boldsymbol{\\beta}}-\\mathbf{d})/r]}{\\hat{\\sigma}^2_{Y|X}} \\sim F_{r,n-p-1} \\]\nFrom our notation we see for the partial F-test that the total number of parameters in the full model is \\(p^{*}+k^{*}\\); whereas in the GLH F-test it is just \\(p\\).\nFurther, for the partial F-test \\(k^{*}\\) is the number of parameters removed from the full model; whereas in the GLH F-test \\(r\\) is the rank of \\(\\mathbf{c}\\):\n\\[\\begin{bmatrix}\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{bmatrix}\\]\nwhere each column represents a beta coefficient, including the intercept, and each row represents a different hypothesis test being conducted jointly/simultaneously).\nFor completeness, the null hypothesis the GLH is testing is:\n\\[ H_0 \\colon \\; \\mathbf{c}\\boldsymbol{\\beta} = \\mathbf{d} \\implies \\begin{bmatrix}\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_{former} \\\\ \\beta_{current} \\\\ \\beta_{bmi} \\\\ \\beta_{age} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}  \\]"
  },
  {
    "objectID": "recitation/r2/index.html",
    "href": "recitation/r2/index.html",
    "title": "Mini-Simulation Study for Bias, Consistency, and Efficiency of the Sample Median versus the Sample Mean",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r2/index.html#bias-calculation-for-the-median",
    "href": "recitation/r2/index.html#bias-calculation-for-the-median",
    "title": "Mini-Simulation Study for Bias, Consistency, and Efficiency of the Sample Median versus the Sample Mean",
    "section": "Bias Calculation for the Median",
    "text": "Bias Calculation for the Median\nAssume that 100 patients are seen the next day. If Dr. Bob calculates the median height for the adult male patients seen on that day, what is the bias of his median estimate with respect to the population mean? (Hint: Simulate a normal distribution of size n=100)\n\n\nCode\nset.seed(6618) # set seed for reproducibility\n\nsim_norm &lt;- rnorm(n=100, mean=72, sd=sqrt(16)) # simulate N=100 observations\n\nmed &lt;- median(sim_norm) # estimate our median\n\nbias &lt;- med - 72 # calculate our bias (estimate - population mean)\n\nbias\n\n\n[1] -0.8545658\n\n\nThe median height from the sample is 71.15. Thus, the median is biased -0.85 inches with respect to the population mean of 72."
  },
  {
    "objectID": "recitation/r2/index.html#consistency-for-the-median",
    "href": "recitation/r2/index.html#consistency-for-the-median",
    "title": "Mini-Simulation Study for Bias, Consistency, and Efficiency of the Sample Median versus the Sample Mean",
    "section": "Consistency for the Median",
    "text": "Consistency for the Median\nAlthough improbable, now assume the number of patients seen in a day increases. Increase the sample size from 100 to 100,000, by 100 person increments. Calculate the bias for the varying sample sizes. Plot the results. What does this say about the consistency of the median estimate with respect to the population mean?\n\n\nCode\nset.seed(6618) # Set seed for reproducibility\n\n# Increasing sample size\nns &lt;- seq(100,100000,by=100)\n\nbias_median&lt;-sapply(ns, function(x){\n  sim&lt;-rnorm(n=x,mean=72,sd=sqrt(16)) #Simulate normal dist. for each n\n  medians&lt;-median(sim) # Calculate the median of the samples\n  bias&lt;-medians-72 # Calculate the bias of the samples\n  return(bias)\n}) # Note, \")\" matches to the left parenthesis for sapply\n\n# Plot the results\nplot(x=ns,y=bias_median,type='l')\n\n\n\n\n\nWhen the sample size increases to 100,000, the bias of the median appears to vary about 0. Thus, this simulation exercise suggests that the median becomes unbiased as the sample size increases infinitely, and the median estimator could be considered consistent with respect to the population mean."
  },
  {
    "objectID": "recitation/r2/index.html#variance-as-our-sample-size-increases",
    "href": "recitation/r2/index.html#variance-as-our-sample-size-increases",
    "title": "Mini-Simulation Study for Bias, Consistency, and Efficiency of the Sample Median versus the Sample Mean",
    "section": "Variance as Our Sample Size Increases",
    "text": "Variance as Our Sample Size Increases\nHow does the variance of the data with respect to the median estimator change as the sample size increases?\nOne formula for the variance we could use is:\n[ Var(X)=_{i} (x_i-median)^2/n, ]\nwhere \\(\\forall i\\) means “for all” values of \\(i\\). In other words, we need to convert this from a mathematical formula to something we can code in R.\n\n\nCode\nset.seed(6618) # set for reproducibility\n\n# Increasing sample size\nns &lt;- seq(100,100000,by=100)\n\nvar_median &lt;- rep(NA, length(ns))\n\nfor( i in 1:length(ns) ){\n\n    sim &lt;- rnorm(n=ns[i], mean=72, sd=sqrt(16))\n    \n    med &lt;- median(sim)\n    \n    var_median[i] &lt;- sum((sim-med)^2) / length(sim) # converting that formula to code\n}\n\n# Plot the results\nplot(x=ns,y=var_median,type='l')\n\n\n\n\n\nAs the sample size increases, the variance about the median varies less and centers about the population variance. This is something we could work to derive theoretically, but it isn’t really the purpose of our methods class (maybe in theory, maybe…). Instead, via a small simulation, we’ve identified that the variance becomes less erratic as our sample size increases and also converges to the variance we simulated our data from.\nNote, if you do a similar plot with the mean instead of the median, you should see a very similar result for our simulated normal data across the sample sizes:\n\n\nCode\nset.seed(6618) # set for reproducibility\n\n# Increasing sample size\nns&lt;-seq(100,100000,by=100)\n\nvar_mean &lt;- rep(NA, length(ns))\n\nfor( i in 1:length(ns) ){\n\n    sim &lt;- rnorm(n=ns[i], mean=72, sd=sqrt(16))\n    \n    med &lt;- mean(sim)\n    \n    var_mean[i] &lt;- sum((sim-med)^2) / length(sim) # converting that formula to code\n}\n\n# Plot the results\nplot(x=ns,y=var_mean,type='l', ylab='Woah, this is the variance of the mean!', xlab='Poorly formatted sample sizes')"
  },
  {
    "objectID": "recitation/r2/index.html#asymptotic-relative-efficiency",
    "href": "recitation/r2/index.html#asymptotic-relative-efficiency",
    "title": "Mini-Simulation Study for Bias, Consistency, and Efficiency of the Sample Median versus the Sample Mean",
    "section": "Asymptotic Relative Efficiency",
    "text": "Asymptotic Relative Efficiency\nDr. Billy bets Dr. Bob that the sample mean is more efficient (i.e. less variable about the population mean) than the sample median. To compare the relative efficiency of estimators, simulate 10,000 normal distributions with sample size n=1000, population mean=72 inches, and variance=16 inches\\(^2\\). Calculate the median and mean for each simulation. Then compare the variance of the set of sample medians to the variance of the set of sample means. Using the results of your simulation, which estimator is more efficient?\nTo answer this question we need to evaluate the asymptotic relative efficiency of our estimators. If two estimators (\\(V_n\\), \\(W_n\\)) are consistent and their variances converge in distribution, the ARE of \\(V_n\\) with respect to (with respect to) \\(W_n\\) is given by:\n\\[ARE(V_n,W_n)=Var(W_n)/Var(V_n)\\].\n\n\nCode\nset.seed(6618) # Set seed the same as before\n\n# Create a vector of 10,000 1,000's\nns&lt;-rep(1000, 10000)\nmat_res &lt;- matrix(nrow=10000, ncol=2, dimnames=list(1:10000, c('mean','median')) )\n\nfor( k in 1:length(ns) ){\n  \n  sim &lt;- rnorm(n=ns[k], mean=72, sd=sqrt(16))\n  \n  median_est &lt;- median(sim)\n  mean_est &lt;- mean(sim)\n  \n  mat_res[k,c('mean','median')] &lt;- c(mean_est, median_est) # NOTE: here I included the column names in the matrix, but with only 2 values we could also have left it blank and R would have assumed we wanted to fill in the two values\n  \n}\n\nvar(mat_res[,'mean'])/var(mat_res[,'median']) # Relative efficiency\n\n\n[1] 0.6398269\n\n\nThe variance of the sample medians is 0.0259, and the variance of the sample means is 0.0166. Thus, the relative efficiency of the sample median with respect to to the sample mean is 0.6398. Thus, the sample mean is more efficient (i.e. less variable about the population mean) than the sample median."
  },
  {
    "objectID": "recitation/r2/index.html#graphing-variation",
    "href": "recitation/r2/index.html#graphing-variation",
    "title": "Mini-Simulation Study for Bias, Consistency, and Efficiency of the Sample Median versus the Sample Mean",
    "section": "Graphing Variation",
    "text": "Graphing Variation\nLet’s assume that we wanted to somehow plot the variances of the mean and median together in some way for a comparison. There are many possible approaches, but we’ll present a few below with regards to plotting them on the same figure or creating a panel figure.\nAs a quick check of our results, the range across all sample sizes is:\n\n\nCode\nrange(var_mean) # range of variance values for all means\n\n\n[1] 14.60288 17.64583\n\n\nCode\nrange(var_median) # range of variance values for all medians\n\n\n[1] 14.60315 17.65101\n\n\nWe see that they are very similar, but they are slightly different.\n\nApproach 1: The Same Figure with Base R Plotting\nIn this approach, we are plotting both on the same figure. We can start by (1) plotting one of our vectors, (2) adding the other vector, and then (3) adding a legend. While we are at it, we can also clean up the labels to be a little more reader-friendly:\n\n\nCode\nns&lt;-seq(100,100000,by=100) # need to re-enter ns since it is replaced in HW 1, Q3c/d\n\nplot(x=ns, y=var_mean, type='l', ylab='Variance Estimate', xlab='Sample Size (in 10,000s)', xaxt='n')\n# The xaxt='n' argument tells R we don't want to plot the x-axis's labels/ticks\n\n# The axis() function allows us to customize and add details about a given axis, here the x-axis:\naxis(side=1, at=seq(from=0, to=100000, by=20000), labels=seq(from=0,to=10,by=2))\n\n# The lines() function adds line graphs given the provided coordinates [note, points() is a related argument to add scatterplot points]\nlines(x=ns, y=var_median, lty=2, col='orangered2')\n\n# We can add a legend\nlegend('topright', bty='n', lty=c(1,2), col=c('black','orangered2'), legend=c('Mean','Median'))\n\n\n\n\n\nCode\n# You can specify relational locations (e.g., 'topright') or x-y coordinates\n# bty removes a border around the legend (which is personal preference for me)\n\n\nWe see with this example, the results are very similar, so they appear nearly on top of one another.\n\n\nApproach 2: Panel Figure in Base R Plotting\nIn this approach, we want to plot the mean and median variance estimates side-by-side in two separate figures. There are many packages that can help layout this type of plot, but one easy approach is to use the built-in par( mfrow=c(1,2) ) function to specify a multi-panel figure with 1 row and 2 columns:\n\n\nCode\npar( mfrow=c(1,2) ) # tell R we want to create a panel figure\n\n# Plot the mean in our first figure\nplot(x=ns, y=var_mean, type='l', ylab='Variance Estimate', xlab='Sample Size (in 10,000s)', xaxt='n', main='Mean', xlim=c(0,100000), ylim=c(14.5,17.8))\naxis(side=1, at=seq(from=0, to=100000, by=20000), labels=seq(from=0,to=10,by=2))\n\n# Plot the median in our second figure\nplot(x=ns, y=var_median,type='l', ylab='Variance Estimate', xlab='Sample Size (in 10,000s)', xaxt='n', main='Median', xlim=c(0,100000), ylim=c(14.5,17.8))\naxis(side=1, at=seq(from=0, to=100000, by=20000), labels=seq(from=0,to=10,by=2))\n\n\n\n\n\nWhen I make panel figures, some things I like to try and keep in mind:\n\nIs it useful to add labels (with main=) so a reader knows what is in each plot?\nIs it useful to standardize the x-axis and y-axis ranges? (For example, if we are trying to make an apples-to-apples comparison, it would be helpful to have the same ranges to help a reader more quickly see potential differences.)\nSince I’m generally squeezing more information into the same space, do I need to modify the plotting characteristics (e.g., line types, colors, points, etc.)?\n\n\n\nApproach 3: The Same Figure with ggplot\nA popular package for creating figures is ggplot. It has its own structure, unique style(s), and syntax. It is helpful if we first combine the two sets of results into a data frame to provide to ggplot:\n\n\nCode\n# Step 1: Create a data frame of our resulting variance estimates\ndat &lt;- data.frame( n = c(ns, ns),\n                   var = c(var_mean, var_median),\n                   group = c(rep('Mean', times=length(ns)), rep('Median', times=length(ns) )) \n                   )\n\nhead(dat) # see first few rows\n\n\n    n      var group\n1 100 17.17715  Mean\n2 200 16.65669  Mean\n3 300 14.60288  Mean\n4 400 17.64583  Mean\n5 500 16.68933  Mean\n6 600 15.79174  Mean\n\n\nCode\ntail(dat) # see last few rows\n\n\n          n      var  group\n1995  99500 15.97743 Median\n1996  99600 15.90350 Median\n1997  99700 15.90833 Median\n1998  99800 16.03050 Median\n1999  99900 15.93699 Median\n2000 100000 16.00330 Median\n\n\nNow we can load the ggplot2 package and create a line graph:\n\n\nCode\nlibrary(ggplot2)\n\nggplot(dat, aes(x=n, y=var, group=group)) +\n  geom_line(aes(color=group))+\n  geom_point(aes(color=group)) # can remove if you don't want points added\n\n\n\n\n\nAgain, our results are so similar, this plot largely covers the mean group.\nOne cool aspect of ggplot2 is that we can save the plot as an object and then make other modifications:\n\n\nCode\np &lt;- ggplot(dat, aes(x=n, y=var, group=group)) +\n  geom_line(aes(color=group))\n\np + scale_color_grey()  # Makes our figure grayscale\n\n\n\n\n\nCode\np + scale_color_brewer(palette = 'Pastel2') # choose a different color palette\n\n\n\n\n\nCode\np + theme_classic() # Plots it in a base R theme\n\n\n\n\n\nCode\np + scale_color_brewer(palette = 'Accent') + theme_classic() # combine colors and theme\n\n\n\n\n\n\n\nApproach 4: Panel Figure with ggplot\nWe can also use various approach to create a panel figure once we’ve generated our two individual plots. Here, we’ll use the patchwork library:\n\n\nCode\nlibrary(patchwork)\n\n\nWarning: package 'patchwork' was built under R version 4.4.1\n\n\nCode\np_mean &lt;- p &lt;- ggplot(dat[ which(dat$group=='Mean'),], aes(x=n, y=var, group=group)) +\n  geom_line() +\n  ggtitle('Mean') + xlab('Sample Size') + ylab('Variance') + ylim(14.5,17.8)\n\np_median &lt;- p &lt;- ggplot(dat[ which(dat$group=='Median'),], aes(x=n, y=var, group=group)) +\n  geom_line() +\n  ggtitle('Median') + xlab('Sample Size') + ylab('Variance') + ylim(14.5,17.8)\n\np_mean + p_median\n\n\n\n\n\nHere we see we have the figures with the default ggplot-styling side-by-side."
  },
  {
    "objectID": "recitation/r21/index.html",
    "href": "recitation/r21/index.html",
    "title": "Precision Variables",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nPrecision Variables\nAnother role a predictor can play in our model is to be a precision variable.\nThe term precision refers to the size of an estimator’s variance, or equivalently, the narrowness of a confidence interval for the parameter being estimated.\nThe smaller the variance of the estimator, the higher the precision of the estimator:\n\\[ \\frac{Var(\\hat{\\beta}_{adj})}{Var(\\hat{\\beta}_{crude})}=\\frac{1-\\hat{\\rho}_{YZ | X}^2}{n-3}\\left(\\frac{n-2}{1-\\hat{\\rho}_{XZ}^2}\\right) \\]\nwhere \\(Z\\) is another independent variable, \\(\\hat{\\rho}_{YZ|X}\\) is the partial correlation between \\(Y\\) and \\(Z\\) that controls for \\(X\\), and \\(\\hat{\\rho}_{XZ}\\) is the correlation between \\(X\\) and \\(Z\\).\nA strong association between \\(Y\\) and \\(Z\\) has a beneficial effect upon the precision of \\(\\hat{\\beta}_{adj}\\) (i.e., it decreases \\(SE(\\hat{\\beta}_{adj})\\)).\nA strong association between \\(X\\) and \\(Z\\) has a detrimental effect on the precision of \\(\\hat{\\beta}_{adj}\\) (i.e., it increases \\(SE(\\hat{\\beta}_{adj})\\)).\nThus, the precision of \\(\\hat{\\beta}_{adj}\\) reflects the competing effects of the \\(Y\\)-\\(Z\\) and \\(X\\)-\\(Z\\) relationships. A precision variable improves the precision of the estimate of the PEV.\nIn our regression model, if we have ruled out the variable \\(Z\\) as being a potential confounder, we can evaluate the change in the variability of our PEV’s beta coefficient to see if it may be a precision variable. If there is a strong association between \\(Z\\) and \\(Y\\), then we would expect our variance of \\(\\hat{\\beta}_{adj}\\) to decrease (i.e., the presence of \\(Z\\) helped address the variability in a way that improved our estimation).\nHowever, if there is a strong(er) association between \\(Z\\) and \\(X\\), we may actually increase our variance of \\(\\hat{\\beta}_{adj}\\). This may lead us to remove it from the model (assuming we have no strong biological/scientific reasons to keep the variable).\nThe consequence of increasing the variance of our beta coefficients is that it may lead to a smaller test statistic and larger p-value."
  },
  {
    "objectID": "recitation/r23/index.html",
    "href": "recitation/r23/index.html",
    "title": "Splines in Linear Regression",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r23/index.html#why-extra-knots",
    "href": "recitation/r23/index.html#why-extra-knots",
    "title": "Splines in Linear Regression",
    "section": "Why Extra Knots?",
    "text": "Why Extra Knots?\nIn our lecture slides we noted:\nThe B-spline basis is a common spline basis and can be fit in R using the splines::bs() function.\nThe B-spline basis is based on the knot sequence: \\[\\begin{align*}\n\\xi_1 \\leq \\cdots &  \\leq \\xi_d \\leq \\color{red}{\\xi_{d+1}} &lt; \\color{blue}{\\xi_{d+2}} &lt; \\cdots \\color{blue}{\\xi_{d+K+1}} \\\\\n& &lt; \\color{red}{\\xi_{d+K+2}} \\leq \\xi_{d+K+3} \\leq \\cdots \\leq \\xi_{2d+K+2}\n\\end{align*}\\]\nThe “inner knots” are represented by \\(\\color{blue}{\\xi_{d+2}} = \\tau_1, \\cdots, \\color{blue}{\\xi_{d+K+1}}=\\tau_K\\).\nThe “boundary knots” are defined as \\(\\color{red}{\\xi_{d+1}}=a\\) and \\(\\color{red}{\\xi_{d+K+2}}=b\\).\nThe choice of additional knots \\(\\xi_1, \\cdots, \\xi_d\\) and \\(\\xi_{d+K+3}, \\cdots, \\xi_{2d+K+2}\\) is somewhat arbitrary, with a common strategy being to set them equal to the boundary knots.\nThe reason we set these additional knots before and after the boundary knots (even if we set them to be identical) is to assist in defining appropriate basis functions for each segment of our data (i.e., between any two knots). This could be thought of as a mathematical trick to ensure certain properties of the B-spline are maintained, but the specific mathematical details go beyond the material for our class."
  },
  {
    "objectID": "recitation/r23/index.html#selecting-optimal-parameters",
    "href": "recitation/r23/index.html#selecting-optimal-parameters",
    "title": "Splines in Linear Regression",
    "section": "Selecting Optimal Parameters",
    "text": "Selecting Optimal Parameters\nIn our lecture we also mentioned approaches to selecting optimal spline parameters:\n\nUse of visual evaluation (e.g., seeing that the data does not appear to be overfit)\nModel selection criterion (e.g., AIC, AICc, BIC, etc.) to compare models with different parameters (minimized AIC is preferred)\nCross validation (CV) by dividing the \\(N\\) data points into \\(K\\) groups/folds for train/test set evaluation\n\nWe’ve seen examples of visual evaluation already, so we’ll explore some approaches using model selection criteria. Let’s revisit our NHANES data example focusing on modeling testosterone in males:\n\n\nCode\ndat &lt;- read.csv('../../.data/nhanes1516_sexhormone.csv')\ndatm &lt;- dat[which(dat$MALE==TRUE),] # subset to males\n\nplot(x=datm$RIDAGEYR, y=datm$TESTOSTERONE, xlab='Age (years)', ylab='Testosterone (ng/dL)', col='gray85', cex=0.5)\n\n\n\n\n\nWe again see a clearly non-linear trend for age and testosterone. Let’s fit a range of models with different degrees of freedom (3, 4, and 5) using B-splines and natural cubic splines:\n\n\nCode\nlibrary(splines)\n\n# create empty lists to store results in\nbs_list &lt;- ns_list &lt;- list()\n\n# loop through degrees of freedom of 3 to 15 and save model\nfor( df in 3:15 ){\n  bs_list[[(df-2)]] &lt;- lm( TESTOSTERONE ~ bs(RIDAGEYR, df=df), data=datm)\n  ns_list[[(df-2)]] &lt;- lm( TESTOSTERONE ~ ns(RIDAGEYR, df=df), data=datm)\n}\n\n\nLet’s plot some of these trends:\n\n\nCode\nnewdatm &lt;- data.frame( RIDAGEYR = 6:80 )\n\npar(mfrow=c(2,2), mar=c(4,4,3,1))\n\n# df=3\nplot(x=datm$RIDAGEYR, y=datm$TESTOSTERONE, xlab='Age (years)', ylab='Testosterone (ng/dL)', col='gray85', cex=0.5, main='df=3')\nlines(x=newdatm$RIDAGEYR, y=predict(bs_list[[1]], newdata=newdatm))\nlines(x=newdatm$RIDAGEYR, y=predict(ns_list[[1]], newdata=newdatm), lty=2)\nlegend('topright', col=c('black','black'), lty=c(1,2), legend=c('bs','ns'), bty='n', cex=0.8)\n\n# df=6\nplot(x=datm$RIDAGEYR, y=datm$TESTOSTERONE, xlab='Age (years)', ylab='Testosterone (ng/dL)', col='gray85', cex=0.5, main='df=6')\nlines(x=newdatm$RIDAGEYR, y=predict(bs_list[[4]], newdata=newdatm))\nlines(x=newdatm$RIDAGEYR, y=predict(ns_list[[4]], newdata=newdatm), lty=2)\nlegend('topright', col=c('black','black'), lty=c(1,2), legend=c('bs','ns'), bty='n', cex=0.8)\n\n# df=8\nplot(x=datm$RIDAGEYR, y=datm$TESTOSTERONE, xlab='Age (years)', ylab='Testosterone (ng/dL)', col='gray85', cex=0.5, main='df=8')\nlines(x=newdatm$RIDAGEYR, y=predict(bs_list[[6]], newdata=newdatm))\nlines(x=newdatm$RIDAGEYR, y=predict(ns_list[[6]], newdata=newdatm), lty=2)\nlegend('topright', col=c('black','black'), lty=c(1,2), legend=c('bs','ns'), bty='n', cex=0.8)\n\n# df=9\nplot(x=datm$RIDAGEYR, y=datm$TESTOSTERONE, xlab='Age (years)', ylab='Testosterone (ng/dL)', col='gray85', cex=0.5, main='df=9')\nlines(x=newdatm$RIDAGEYR, y=predict(bs_list[[7]], newdata=newdatm))\nlines(x=newdatm$RIDAGEYR, y=predict(ns_list[[7]], newdata=newdatm), lty=2)\nlegend('topright', col=c('black','black'), lty=c(1,2), legend=c('bs','ns'), bty='n', cex=0.8)\n\n\n\n\n\nVisually evaluating the figures, we might think df=3 looks pretty decent (expect, perhaps, at the younger ages). The fit at lower ages is improved as we increase the degrees of freedom. We also might wish to compare the B-spline and the natural cubic splines within any degree of freedom.\nOne approach is to use model selection criteria. Let’s examine a table and plot of our values for AIC and BIC:\n\n\nCode\n# estimate AIC\naic_bs &lt;- sapply(1:13, function(x) AIC(bs_list[[x]]))\naic_ns &lt;- sapply(1:13, function(x) AIC(ns_list[[x]]))\n\n# estimate BIC\nbic_bs &lt;- sapply(1:13, function(x) BIC(bs_list[[x]]))\nbic_ns &lt;- sapply(1:13, function(x) BIC(ns_list[[x]]))\n\n# table of values\ntbl &lt;- cbind(aic_bs,aic_ns,bic_bs,bic_ns)\nrownames(tbl) &lt;- 3:15\ntbl # print\n\n\n     aic_bs   aic_ns   bic_bs   bic_ns\n3  47127.87 47050.22 47158.71 47081.05\n4  46835.47 46782.66 46872.48 46819.66\n5  46908.45 46808.94 46951.62 46852.11\n6  46719.40 46773.75 46768.74 46823.09\n7  46648.94 46675.38 46704.44 46730.89\n8  46678.38 46640.07 46740.05 46701.74\n9  46657.13 46630.35 46724.97 46698.19\n10 46644.05 46640.55 46718.06 46714.55\n11 46632.50 46640.48 46712.67 46720.65\n12 46637.82 46643.75 46724.16 46730.09\n13 46637.86 46631.73 46730.37 46724.24\n14 46643.69 46632.61 46742.36 46731.29\n15 46633.49 46634.43 46738.33 46739.27\n\n\nThere is a lot going on here. To help us identify the “best” model using AIC or BIC, let’s take the minimum AIC or BIC observed across both B-splines and natural cubic splines and compare:\n\n\nCode\ndiff &lt;- tbl - matrix(rep(c(min(tbl[,c('aic_bs','aic_ns')]),min(tbl[,c('bic_bs','bic_ns')])), each=nrow(tbl)*2), ncol=4, byrow=F)\nround(diff,1)\n\n\n   aic_bs aic_ns bic_bs bic_ns\n3   497.5  419.9  460.5  382.9\n4   205.1  152.3  174.3  121.5\n5   278.1  178.6  253.4  153.9\n6    89.0  143.4   70.5  124.9\n7    18.6   45.0    6.3   32.7\n8    48.0    9.7   41.9    3.6\n9    26.8    0.0   26.8    0.0\n10   13.7   10.2   19.9   16.4\n11    2.1   10.1   14.5   22.5\n12    7.5   13.4   26.0   31.9\n13    7.5    1.4   32.2   26.0\n14   13.3    2.3   44.2   33.1\n15    3.1    4.1   40.1   41.1\n\n\nWith these results and a rule of thumb of decreases in AIC or BIC greater than 4, we notice:\n\nFor AIC, the natural cubic spline model with df=9 minimizes the AIC. Models within 4 of this include:\n\ndf=11 B-spline (\\(\\Delta=2.1\\))\ndf=15 B-spline (\\(\\Delta=3.1\\))\ndf=13 natural cubic spline (\\(\\Delta=1.4\\))\ndf=14 natural cubic spline (\\(\\Delta=2.3\\))\nHowever, all these models have higher degrees of freedom and are less parsimonious.\n\nFor BIC, the natural cubic spline model with df=9 minimizes the BIC. Models within 4 of this include:\n\ndf=8 natural cubic spline (\\(\\Delta=3.6\\))\nSince df=8 has fewer degrees of freedom, could select it instead of df=9 because it is slightly more parsimonious\n\n\nHowever, it may also be useful to visualize if there is a point where we might start to experience “diminishing returns” for increasing our model complexity (and risking overfitting the data). Here we can plot the AIC and BIC for each method and degree of freedom:\n\n\nCode\n# plot of values\nplot(x=3:15, y=aic_bs, xlab='df', ylab='AIC/BIC Value', type='o', ylim=range(c(aic_bs,aic_ns,bic_bs,bic_ns)))\nlines(x=3:15, y=aic_ns, pch=2, lty=2, col='blue', type='o')\nlines(x=3:15, y=bic_bs, pch=16, lty=3, col='orangered2', type='o')\nlines(x=3:15, y=bic_ns, pch=17, lty=4, col='purple', type='o')\nlegend('topright', lty=1:4, pch=c(1,2,16,17), col=c('black','blue','orangered2','purple'), legend=c('AIC bs','AIC ns','BIC bs','BIC ns'))\n\n\n\n\n\nSome takeaways here:\n\nAround df=7 we stop seeing larger changes (there is still some moving around, e.g., B-splines do increase from df=7 to 8 before decreasing again). So, we might consider choosing df=7 even though it wasn’t “optimal” based on the change in AIC or BIC from above.\nWe can see that BIC (solid points) is larger than the AIC (open points) values, where both natural cubic splines and B-splines seem to converge to very similar results at larger degrees of freedom.\nIn practice there are many ways we could justify the choice of different splines, degrees of freedom, or choice across other models."
  },
  {
    "objectID": "recitation/r23/index.html#model-interpretation-with-splines",
    "href": "recitation/r23/index.html#model-interpretation-with-splines",
    "title": "Splines in Linear Regression",
    "section": "Model Interpretation with Splines",
    "text": "Model Interpretation with Splines\nWe’ve discussed the challenges of interpreting polynomial and spline models when our primary explanatory variable of interest is the one with the nonlinear trend. Here we will explore a model with both sex and age to see how interpretations may be affected.\n\nMLR with Sex and Age\nLet’s revisit our NHANES data example focusing on modeling testosterone, but this time in males and females and plot one multiple linear regression model with sex and age:\n\n\nCode\ndat &lt;- read.csv('../../.data/nhanes1516_sexhormone.csv')\n\n# fit MLR\nmod1 &lt;- lm( TESTOSTERONE ~ RIDAGEYR + MALE, data=dat )\nsummary(mod1)\n\n\n\nCall:\nlm(formula = TESTOSTERONE ~ RIDAGEYR + MALE, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-414.52  -51.93   -1.23   36.92 1638.21 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -30.34926    4.20501  -7.217 5.84e-13 ***\nRIDAGEYR      1.36133    0.08384  16.237  &lt; 2e-16 ***\nMALETRUE    337.68353    3.79058  89.085  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 160.9 on 7204 degrees of freedom\n  (814 observations deleted due to missingness)\nMultiple R-squared:  0.5318,    Adjusted R-squared:  0.5316 \nF-statistic:  4091 on 2 and 7204 DF,  p-value: &lt; 2.2e-16\n\n\nThe interpretations of our beta coefficients here are easy to interpret:\n\nAge: For a 1 year increase in age, testosterone increases by an average of 1.36 ng/dL after adjusting for sex.\nSex: Testosterone is an average of 337.68 ng/dL higher in males than females after adjusting for age.\n\nWe can visualize, however, that this isn’t likely the best fit:\n\n\nCode\n# create variable to change point type by sex\ndat$pch &lt;- 1\ndat$pch[which(dat$MALE==FALSE)] &lt;- 0\n\n# create plot\nplot(x=dat$RIDAGEYR, y=dat$TESTOSTERONE, xlab='Age (years)', ylab='Testosterone (ng/dL)', col='gray85', cex=0.5, pch=dat$pch)\nlegend('topleft', bty='n', pch=c(0,1,NA,NA), lty=c(NA,NA,1,1), legend=c('Females','Males','Females: Fitted Line','Males: Fitted Line'), col=c('gray85','gray85','green3','purple'), cex=0.8)\n\n# add regression fits\nnewdat_male &lt;- data.frame( RIDAGEYR=6:80, MALE=TRUE)\nnewdat_female &lt;- data.frame( RIDAGEYR=6:80, MALE=FALSE)\n\nlines(x=6:80, y=predict(mod1, newdata=newdat_male), col='purple', lwd=2)\nlines(x=6:80, y=predict(mod1, newdata=newdat_female), col='green3', lwd=2)\n\n\n\n\n\nIndeed, the fits are in both groups do not appear to be accurately reflecting the trend with age. Let’s try fitting a model with a natural cubic spline used for age next.\n\n\nSex and Age with Natural Cubic Spline\nHere we will add a natural cubic spline to age with a degree of freedom of 8 (based on our other question evaluating spline calibration):\n\n\nCode\nlibrary(splines)\n\n# fit MLR\nmod2 &lt;- lm( TESTOSTERONE ~ ns(RIDAGEYR,df=8) + MALE, data=dat )\nsummary(mod2)\n\n\n\nCall:\nlm(formula = TESTOSTERONE ~ ns(RIDAGEYR, df = 8) + MALE, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-395.19  -64.77  -26.38   51.29 1610.36 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -168.282      8.954  -18.79   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)1  356.634     12.141   29.38   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)2  218.216     15.274   14.29   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)3  232.027     13.689   16.95   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)4  202.993     14.118   14.38   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)5  225.764     13.394   16.86   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)6  203.791     11.424   17.84   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)7  192.410     21.816    8.82   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)8  194.185      8.674   22.39   &lt;2e-16 ***\nMALETRUE               339.050      3.428   98.91   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 145.3 on 7197 degrees of freedom\n  (814 observations deleted due to missingness)\nMultiple R-squared:  0.6183,    Adjusted R-squared:  0.6178 \nF-statistic:  1295 on 9 and 7197 DF,  p-value: &lt; 2.2e-16\n\n\nThe interpretations of our sex coefficient is still easy to interpret, but age not so much:\n\nAge: Given the splines, the rate of change in testosterone values will depend on the age we are comparing to.\nSex: Testosterone is an average of 339.05 ng/dL higher in males than females after adjusting for age.\n\nInterestingly, we do have a similar sex difference estimated between our two models. Visually, this looks like:\n\n\nCode\n# create plot\nplot(x=dat$RIDAGEYR, y=dat$TESTOSTERONE, xlab='Age (years)', ylab='Testosterone (ng/dL)', col='gray85', cex=0.5, pch=dat$pch)\nlegend('topleft', bty='n', pch=c(0,1,NA,NA), lty=c(NA,NA,1,1), legend=c('Females','Males','Females: Fitted Line','Males: Fitted Line'), col=c('gray85','gray85','green3','purple'), cex=0.8)\n\n\n# add regression fits\nlines(x=6:80, y=predict(mod2, newdata=newdat_male), col='purple', lwd=2)\nlines(x=6:80, y=predict(mod2, newdata=newdat_female), col='green3', lwd=2)\n\n\n\n\n\nWhile a little better, there does appear to be an issue with assuming the the sex difference is constant across ages. We can try to account for this by including an interaction term in our next model.\n\n\nSex and Age with Natural Cubic Spline and Their Interaction\nHere we will add an interaction between sex and age (with a spline) from our previous model:\n\n\nCode\nlibrary(splines)\n\n# fit MLR\nmod3 &lt;- lm( TESTOSTERONE ~ ns(RIDAGEYR,df=8) * MALE, data=dat )\nsummary(mod3)\n\n\n\nCall:\nlm(formula = TESTOSTERONE ~ ns(RIDAGEYR, df = 8) * MALE, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-449.07  -18.74   -4.24    9.12 1580.21 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      0.9041    10.8171   0.084   0.9334    \nns(RIDAGEYR, df = 8)1           37.6509    15.0955   2.494   0.0126 *  \nns(RIDAGEYR, df = 8)2           30.8584    18.5139   1.667   0.0956 .  \nns(RIDAGEYR, df = 8)3           27.0448    16.8762   1.603   0.1091    \nns(RIDAGEYR, df = 8)4           18.3903    17.1403   1.073   0.2833    \nns(RIDAGEYR, df = 8)5           21.6196    16.5372   1.307   0.1911    \nns(RIDAGEYR, df = 8)6           15.4002    14.1725   1.087   0.2772    \nns(RIDAGEYR, df = 8)7           29.5792    26.8170   1.103   0.2701    \nns(RIDAGEYR, df = 8)8           13.2040    10.6445   1.240   0.2148    \nMALETRUE                        -3.9349    15.4732  -0.254   0.7993    \nns(RIDAGEYR, df = 8)1:MALETRUE 635.7286    21.3614  29.761   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)2:MALETRUE 390.8591    26.9133  14.523   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)3:MALETRUE 410.8437    24.0858  17.058   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)4:MALETRUE 386.2810    24.8818  15.525   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)5:MALETRUE 408.8234    23.5709  17.344   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)6:MALETRUE 378.7641    20.1017  18.842   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)7:MALETRUE 341.0139    38.3930   8.882   &lt;2e-16 ***\nns(RIDAGEYR, df = 8)8:MALETRUE 359.5488    15.2690  23.548   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 127.8 on 7189 degrees of freedom\n  (814 observations deleted due to missingness)\nMultiple R-squared:  0.705, Adjusted R-squared:  0.7044 \nF-statistic:  1011 on 17 and 7189 DF,  p-value: &lt; 2.2e-16\n\n\nWhile interactions can be challenging to interpret, when it is an interaction with a spline (or polynomial) we once again cannot have a simple interpretation since within each sex there is a difference in testosterone with age.\nVisually, this looks like:\n\n\nCode\n# create plot\nplot(x=dat$RIDAGEYR, y=dat$TESTOSTERONE, xlab='Age (years)', ylab='Testosterone (ng/dL)', col='gray85', cex=0.5, pch=dat$pch)\nlegend('topleft', bty='n', pch=c(0,1,NA,NA), lty=c(NA,NA,1,1), legend=c('Females','Males','Females: Fitted Line','Males: Fitted Line'), col=c('gray85','gray85','green3','purple'), cex=0.8)\n\n\n# add regression fits\nlines(x=6:80, y=predict(mod3, newdata=newdat_male), col='purple', lwd=2)\nlines(x=6:80, y=predict(mod3, newdata=newdat_female), col='green3', lwd=2)\n\n\n\n\n\nHere we have a great looking model! If we wanted to do a numerical check that this model is an improvement (even if hard to interpret), we could calculate something like the AIC or BIC:\n\n\nCode\nc(AIC(mod1),AIC(mod2),AIC(mod3))\n\n\n[1] 93688.00 92228.98 90387.20\n\n\nCode\nc(BIC(mod1),BIC(mod2),BIC(mod3))\n\n\n[1] 93715.53 92304.70 90517.97\n\n\nBoth AIC and BIC agree the most complex model is an improvement over any of the simpler models."
  },
  {
    "objectID": "recitation/r25/index.html",
    "href": "recitation/r25/index.html",
    "title": "Why Do We Set Seeds?",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r25/index.html#pseudo-vs.-true-random-number-generation",
    "href": "recitation/r25/index.html#pseudo-vs.-true-random-number-generation",
    "title": "Why Do We Set Seeds?",
    "section": "Pseudo vs. True Random Number Generation",
    "text": "Pseudo vs. True Random Number Generation\nRandomness is fundamental to statistical inference. Our control of randomness is key to experimentation and the scientific method. We ultimately use pseudo-random number generation (PRNG) because of some of the properties described in the table below as compared to true random number generation (TRNG). PRNGs are generated via an algorithm(s) using mathematical formula to produce sequences of numbers that seem random (enough) to us. TRNGs are extracted from physical phenomena and introduced to a computer (e.g., lightning strikes, atmospheric noise). The table from random.org below describes some of these differences:\n\n\n\n\n\n\n\n\nCharacteristic\nPseudo Random Number Generators\nTrue Random Number Generators\n\n\n\n\nEfficiency\nExcellent\nPoor\n\n\nDeterminism\nDeterministic\nNondeterministic\n\n\nPeriodicity\nPeriodic\nAperiodic\n\n\n\nEfficiency is if we can produce many numbers in a short time.\nDeterminism is if we can reproduce our results (given a known starting point, i.e., set seed).\nPeriodicity is the idea that if we go for long enough, we will eventually repeat the sequence. This isn’t really a concern with PRNGs for what we’ll do in 6618."
  },
  {
    "objectID": "recitation/r25/index.html#seed-selection",
    "href": "recitation/r25/index.html#seed-selection",
    "title": "Why Do We Set Seeds?",
    "section": "Seed Selection",
    "text": "Seed Selection\nThe good news about selecting a seed, anything works as long as you know what the seed is! If you do not set the seed yourself, I believe R uses some combination fo current time and the process ID (i.e., not really reproducible). Let’s walk through a short example to illustrate why this is important by simulating some data from a Poisson distribution.\n\nGenerating Data Without Setting the Seed\nAs an example, let’s simulate 5 values from a Poisson distribution with \\(\\lambda=3\\) without setting the seed. Our results are\n\n\nCode\nfive_vals &lt;- rpois(n=5, lambda=3) # simulate 5 random values from a Poisson distribution with lambda=3\nfive_vals # print those 5 random values\n\n\n[1] 3 3 2 5 3\n\n\nCode\nmean(five_vals) # calculate the mean of those 5 random values\n\n\n[1] 3.2\n\n\nFrom this output we see that the mean of these values was 3.2. Let’s say we lost power or shared our code with someone else to run. If we just reran this code without setting the seed we would see 5 new estimates that do not necessarily match what we had before:\n\n\nCode\nfive_vals &lt;- rpois(n=5, lambda=3) # simulate 5 random values from a Poisson distribution with lambda=3\nfive_vals # print those 5 random values\n\n\n[1] 0 2 4 4 2\n\n\nCode\nmean(five_vals) # calculate the mean of those 5 random values\n\n\n[1] 2.4\n\n\n\n\nGenerating Data With set.seed\nIf, instead, we set the seed at some location, such as set.seed(6611) we see that we will get the same responses every time we simulate the five values (as long as we re-set the seed before running the code):\n\n\nCode\nset.seed(6611) # set the seed for reproducibility\n\nfive_vals_seed_set &lt;- rpois(n=5, lambda=3) # simulate 5 random values from a Poisson distribution with lambda=3\nfive_vals_seed_set # print those 5 random values\n\n\n[1] 3 1 2 3 3\n\n\nCode\nmean(five_vals_seed_set) # calculate the mean of those 5 random values\n\n\n[1] 2.4\n\n\nNow, let’s set the seed again and re-run the code:\n\n\nCode\nset.seed(6611) # set the seed for reproducibility\n\nfive_vals_seed_set &lt;- rpois(n=5, lambda=3) # simulate 5 random values from a Poisson distribution with lambda=3\nfive_vals_seed_set # print those 5 random values\n\n\n[1] 3 1 2 3 3\n\n\nCode\nmean(five_vals_seed_set) # calculate the mean of those 5 random values\n\n\n[1] 2.4\n\n\nIf we continue to generate new data without setting the seed to 6611 we see there are 5 new values:\n\n\nCode\nfive_vals_seed_set_five_more &lt;- rpois(n=5, lambda=3) # simulate 5 random values from a Poisson distribution with lambda=3\nfive_vals_seed_set_five_more # print those 5 random values\n\n\n[1] 2 1 3 4 1\n\n\nCode\nmean(five_vals_seed_set_five_more) # calculate the mean of those 5 random values\n\n\n[1] 2.2\n\n\nTherefore setting the seed helps you to recreate your past work and avoid subtle (or potentially large) changes in some simulated data estimate. My own personal go-to seeds include set.seed(515) (the area code for central Iowa where I grew up) or things like set.seed(090122) (i.e., the date of this recitation request)."
  },
  {
    "objectID": "recitation/r27/index.html",
    "href": "recitation/r27/index.html",
    "title": "Ways to Enter Data for t.test",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r27/index.html#approach-1-manually-enter-the-data-to-use",
    "href": "recitation/r27/index.html#approach-1-manually-enter-the-data-to-use",
    "title": "Ways to Enter Data for t.test",
    "section": "Approach 1: Manually enter the data to use",
    "text": "Approach 1: Manually enter the data to use\nFor this approach, we will manually create the vectors of data to use. Given that we have the sleep data frame already, this is an inefficient approach. However, it isn’t a terrible approach if you have a small data set that isn’t already in R.\n\n\nCode\ngroup1_extra &lt;- c(0.7,-1.6,-0.2,-1.2,-0.1,3.4,3.7,0.8,0.0,2.0)\ngroup2_extra &lt;- c(1.9,0.8,1.1,0.1,-0.1,4.4,5.5,1.6,4.6,3.4)\n\nt.test(x=group1_extra, y=group2_extra)\n\n\n\n    Welch Two Sample t-test\n\ndata:  group1_extra and group2_extra\nt = -1.8608, df = 17.776, p-value = 0.07939\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.3654832  0.2054832\nsample estimates:\nmean of x mean of y \n     0.75      2.33"
  },
  {
    "objectID": "recitation/r27/index.html#approach-2-subset-the-data-from-sleep-to-use",
    "href": "recitation/r27/index.html#approach-2-subset-the-data-from-sleep-to-use",
    "title": "Ways to Enter Data for t.test",
    "section": "Approach 2: Subset the data from sleep to use",
    "text": "Approach 2: Subset the data from sleep to use\nIn this approach we will extract the information from our sleep data frame and then enter it using the “Default S3 method” approach with x=...,y=.... This works well if you already have data entered into R as a data frame. There are a host of ways one can extract this information (and we’ll dig into some of these later as well):\n\n\nCode\n# Method 1: Pull out the column \"extra\" from the data frame sleep using the $ operator and subset by group\ngroup1_extra_extracted1 &lt;- sleep$extra[ which(sleep$group==1) ]\ngroup2_extra_extracted1 &lt;- sleep$extra[ which(sleep$group==2) ]\nt.test(x=group1_extra_extracted1, y=group2_extra_extracted1)\n\n\n\n    Welch Two Sample t-test\n\ndata:  group1_extra_extracted1 and group2_extra_extracted1\nt = -1.8608, df = 17.776, p-value = 0.07939\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.3654832  0.2054832\nsample estimates:\nmean of x mean of y \n     0.75      2.33 \n\n\nCode\n# Method 2: Subset the data from the columns using dataframe[row,column] notation\n# NOTE: the \"which(sleep$group==1)\" identifies the row numbers where group is equal to 1\n# NOTE: the \"'extra'\" piece is requesting the column of data called extra from the data frame\ngroup1_extra_extracted2 &lt;- sleep[which(sleep$group==1), 'extra' ]\ngroup2_extra_extracted2 &lt;- sleep[which(sleep$group==2), 'extra' ]\nt.test(x=group1_extra_extracted2, y=group2_extra_extracted2)\n\n\n\n    Welch Two Sample t-test\n\ndata:  group1_extra_extracted2 and group2_extra_extracted2\nt = -1.8608, df = 17.776, p-value = 0.07939\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.3654832  0.2054832\nsample estimates:\nmean of x mean of y \n     0.75      2.33 \n\n\nIf you have an object that is a matrix instead of a data frame object, the $ operator no longer works to extract a column. However, you can still reference with the matrix[row,column] approach. There is some wonky behavior in our case where we are coercing the data frame into a matrix (i.e., it made everything a character), so we have to add a step to make it numeric again. However, if we created the data in a matrix where everything was already numeric it would work fine:\n\n\nCode\nsleep_mat &lt;- as.matrix(sleep)\nsleep_mat # NOTICE IT HAS COERCED EVERYTHING TO BE A CHARACTER INSTEAD OF A NUMBER!!!\n\n\n      extra  group ID  \n [1,] \" 0.7\" \"1\"   \"1\" \n [2,] \"-1.6\" \"1\"   \"2\" \n [3,] \"-0.2\" \"1\"   \"3\" \n [4,] \"-1.2\" \"1\"   \"4\" \n [5,] \"-0.1\" \"1\"   \"5\" \n [6,] \" 3.4\" \"1\"   \"6\" \n [7,] \" 3.7\" \"1\"   \"7\" \n [8,] \" 0.8\" \"1\"   \"8\" \n [9,] \" 0.0\" \"1\"   \"9\" \n[10,] \" 2.0\" \"1\"   \"10\"\n[11,] \" 1.9\" \"2\"   \"1\" \n[12,] \" 0.8\" \"2\"   \"2\" \n[13,] \" 1.1\" \"2\"   \"3\" \n[14,] \" 0.1\" \"2\"   \"4\" \n[15,] \"-0.1\" \"2\"   \"5\" \n[16,] \" 4.4\" \"2\"   \"6\" \n[17,] \" 5.5\" \"2\"   \"7\" \n[18,] \" 1.6\" \"2\"   \"8\" \n[19,] \" 4.6\" \"2\"   \"9\" \n[20,] \" 3.4\" \"2\"   \"10\"\n\n\nCode\ngroup1_extra_extracted3 &lt;- sleep_mat[ which(sleep_mat[,'group']==1), 'extra']\ngroup1_extra_extracted3 &lt;- as.numeric(group1_extra_extracted3)\n\ngroup2_extra_extracted3 &lt;- sleep_mat[ which(sleep_mat[,'group']==2), 'extra']\ngroup2_extra_extracted3 &lt;- as.numeric(group2_extra_extracted3)\n\nt.test(x=group1_extra_extracted3, y=group2_extra_extracted3)\n\n\n\n    Welch Two Sample t-test\n\ndata:  group1_extra_extracted3 and group2_extra_extracted3\nt = -1.8608, df = 17.776, p-value = 0.07939\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.3654832  0.2054832\nsample estimates:\nmean of x mean of y \n     0.75      2.33"
  },
  {
    "objectID": "recitation/r27/index.html#approach-3-use-the-formula",
    "href": "recitation/r27/index.html#approach-3-use-the-formula",
    "title": "Ways to Enter Data for t.test",
    "section": "Approach 3: Use the formula",
    "text": "Approach 3: Use the formula\nOftentimes we have sample sizes that are too large to enter manually, and we are already reading in the data from either an external file or existing R data frame, we can just leverage the formula representation:\n\n\nCode\nt.test(extra ~ group, data=sleep)\n\n\n\n    Welch Two Sample t-test\n\ndata:  extra by group\nt = -1.8608, df = 17.776, p-value = 0.07939\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -3.3654832  0.2054832\nsample estimates:\nmean in group 1 mean in group 2 \n           0.75            2.33 \n\n\nIn this case, we put our outcome (extra) on the left side of the ~ operator and our group variable (group) on the right side. We’ll see this notation frequently used when we get to our linear regression programming approaches in the latter portion fo the semester. There are also other functions beyond the t-test that may have this functionality, so feel free to look at the help file or other code examples that are out there to play around with."
  },
  {
    "objectID": "recitation/r29/index.html",
    "href": "recitation/r29/index.html",
    "title": "Poisson Approximations to Exact Binomial Probabilities",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r29/index.html#plot-using-base-r-graphics",
    "href": "recitation/r29/index.html#plot-using-base-r-graphics",
    "title": "Poisson Approximations to Exact Binomial Probabilities",
    "section": "Plot Using Base R Graphics",
    "text": "Plot Using Base R Graphics\nOne way to display this complex set of varying factors is to consider creating a plot that leverages different line types/colors/etc. to differentiate one variable of interest (e.g., the population prevalence \\(p\\)) while plotting the other variable along the x-axis (e.g., the sample size \\(n\\)) with the outcome on the y-axis (i.e., np$diff).\nTo do this with Base R’s graphics will take a little work, but we’ll walk through the steps and functions one-by-one in our commented code and leverage a loop to add lines to the plot with different colors:\n\n\nCode\n# Step 1: Create vector for colors to use in plotting our lines\ncol_vec &lt;- rainbow( length(p) ) # use \"rainbow\" so that each \"p\" has unique color spanning the rainbow\ncol_vec # check out what it returns\n\n\n [1] \"#FF0000\" \"#FF9900\" \"#CCFF00\" \"#33FF00\" \"#00FF66\" \"#00FFFF\" \"#0066FF\"\n [8] \"#3300FF\" \"#CC00FF\" \"#FF0099\"\n\n\nCode\n# Step 2: Create a blank plot to loop through each p to add the lines to:\nplot(x=NA, y=NA, xlab='n', ylab='Difference', xlim=c(80,400), ylim=c(-0.004,0.0005))\n\n# Step 3: Loop through and add lines for each p\nfor( i in 1:length(p) ){\n  np_i &lt;- np[which(np$p == p[i]),] # subset the data to plot\n  lines(x=np_i$n, y=np_i$diff, col=col_vec[i])  \n}\n\n# Step 4: Add legend so we know what each line color/type is\nlegend('bottomright', lty=1, col=col_vec, legend=p, cex=0.7, bty='n') # use cex=0.7 to shrink the size to avoid overlapping the lines, bty='n' removes the default box outline from the legend"
  },
  {
    "objectID": "recitation/r29/index.html#plot-using-ggplot2",
    "href": "recitation/r29/index.html#plot-using-ggplot2",
    "title": "Poisson Approximations to Exact Binomial Probabilities",
    "section": "Plot Using ggplot2",
    "text": "Plot Using ggplot2\nWe can complete the same plot in essentially two lines of code with ggplot. If we coerce the np$p value to be a factor instead of numeric, ggplot will automatically provide a range of colors:\n\n\nCode\nlibrary(ggplot2)\n\n# Coerce \"p\" to be a factor for different plotting colors\nnp$p_factor &lt;- factor(np$p)\n\nggplot(data=np, aes(x=n,y=diff,group=p_factor,color=p_factor)) + # using group=p tells ggplot to essentially do the for loop we had to use above for the base R graphics\n  geom_line() + # specifies that we want a line graph\n  ylab('Difference')\n\n\n\n\n\nNote that if we didn’t coerce np$p to be a factor, we’d just have gradients of the same color (which kind of gets the point across but is a bit harder to tell apart):\n\n\nCode\n# Plot with the numeric version of our prevalence\nggplot(data=np, aes(x=n,y=diff,group=p,color=p)) + # using group=p tells ggplot to essentially do the for loop we had to use above for the base R graphics\n  geom_line() + # specifies that we want a line graph\n  ylab('Difference')"
  },
  {
    "objectID": "recitation/r30/index.html",
    "href": "recitation/r30/index.html",
    "title": "Distribution Functions in R (e.g., dnorm, qnorm, pnorm, rnorm)",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r30/index.html#dnorm-dbinom",
    "href": "recitation/r30/index.html#dnorm-dbinom",
    "title": "Distribution Functions in R (e.g., dnorm, qnorm, pnorm, rnorm)",
    "section": "dnorm, dbinom",
    "text": "dnorm, dbinom\nFor discrete distributions, like the binomial, the dxxx function provides the probability for a given combination of parameters without having to calculate the probability mass function by hand. For example, we see that using dbinom results in the same probability we calculated by hand above:\n\n\nCode\ndbinom(x=3, size=120, prob=0.01)\n\n\n[1] 0.08665163\n\n\nWe can use it to plot the overall PMF as well:\n\n\nCode\npar(mfrow=c(1,2)) # create panel with 1 row, 2 columns\n\n# Plot entire range\ncases &lt;- 0:120\nplot(cases, dbinom(x = cases, size=120, prob=0.01), type='h', xlab='x', ylab='Probability', main='Entire Sample Space')\n\n# Plot 0:8 to better see positive mass\ncases_sub &lt;- 0:8\nplot(cases_sub, dbinom(x = cases_sub, size=120, prob=0.01), type='h', xlab='x', ylab='Probability', main='Zooming In On 0:8')\n\n\n\n\n\nFor continuous distributions the dxxx functions are not as useful since we know that the probability calculations occur over a range of values and technically \\(P(X=x)=0\\). However, using dnorm will return the height of the probability density function, which is useful in creating a plot of the PDF:\n\n\nCode\nxlim &lt;- seq(-5,5,length.out=500)\nplot(x=xlim, y=dnorm(x=xlim), type='l', xlab='X Value', ylab='Density')"
  },
  {
    "objectID": "recitation/r30/index.html#pnorm-pbinom",
    "href": "recitation/r30/index.html#pnorm-pbinom",
    "title": "Distribution Functions in R (e.g., dnorm, qnorm, pnorm, rnorm)",
    "section": "pnorm, pbinom",
    "text": "pnorm, pbinom\nIf we instead wanted to calculate the cumulative probability up to some point instead of either the probability (discrete) or density (continuous), we could use the pxxx family of functions.\nFor example, if we wanted to know the probability of observing 3 or fewer cases of sarcoidosis we could either add 4 separate dbinom functions or use pbinom:\n\n\nCode\n# P(X&lt;=3) = P(X=0) + P(X=1) + P(X=2) + P(X=3) for discrete\ndbinom(x=0, size=120, prob=0.01) + dbinom(x=1, size=120, prob=0.01) + dbinom(x=2, size=120, prob=0.01) + dbinom(x=3, size=120, prob=0.01)\n\n\n[1] 0.9670151\n\n\nCode\n# We can also just use pbinom:\npbinom(q=3, size=120, prob=0.01)\n\n\n[1] 0.9670151\n\n\nFor the normal distribution, we can now calculate actual probabilities. Below we calculate the \\(P(X \\leq 1.96)\\) and the \\(P(-0.5 \\leq X \\leq 0.5)\\) using pnorm:\n\n\nCode\n# P(X &lt;= 1.96)\npnorm(q=1.96)\n\n\n[1] 0.9750021\n\n\nCode\n# P(-0.5 &lt;= X &lt;= 0.5)\npnorm(0.5) - pnorm(-0.5)\n\n\n[1] 0.3829249"
  },
  {
    "objectID": "recitation/r30/index.html#qnorm-qbinom",
    "href": "recitation/r30/index.html#qnorm-qbinom",
    "title": "Distribution Functions in R (e.g., dnorm, qnorm, pnorm, rnorm)",
    "section": "qnorm, qbinom",
    "text": "qnorm, qbinom\nIf we were interested in calculating the inverse of the CDF (i.e., what is the value of \\(x\\) so that a given percent of the distribution falls below it), we would use qxxx. For discrete distributions this is not always super useful since a given quantile may not fall exactly on the sample space:\n\n\nCode\nqbinom(p=seq(0,1,by=0.1), size=120, prob=0.01)\n\n\n [1]   0   0   0   1   1   1   1   2   2   3 120\n\n\nFor continuous distributions it is more useful because (1) they can take on any value in a given range, which may include infinity, and (2) we will need to calculate values in the future for use in confidence intervals where we want to identify the quantity that achieves a \\(1-\\alpha\\) confidence interval. For the standard normal distribution we may wish to see the value of \\(x\\) that has 97.5% of the PDF below it (i.e., the CDF would be 0.975):\n\n\nCode\nqnorm(0.975)\n\n\n[1] 1.959964\n\n\nLook at that, it matches pretty closely to our pnorm(q=0.1.96) from above!"
  },
  {
    "objectID": "recitation/r30/index.html#rnorm-rbinom",
    "href": "recitation/r30/index.html#rnorm-rbinom",
    "title": "Distribution Functions in R (e.g., dnorm, qnorm, pnorm, rnorm)",
    "section": "rnorm, rbinom",
    "text": "rnorm, rbinom\nWe’ve already been working to simulate data on HW, in lecture, and lab, but the rxxx functions simulate random data to use in calculations, simulation studies, etc.\nFor rbinom, it is helpful to be aware of the arguments (since we call it \\(n\\) in the PMF but size in function). n represents the number of simulated covariates we wish to generate from a binomial distribution with a given number of trials in each study represented by size. The easiest way to see the difference is to simulate some data and flip the n and size values:\n\n\nCode\nset.seed(1015)\nrbinom(n=10, size=1, prob=0.5)\n\n\n [1] 1 0 1 0 0 0 0 0 0 1\n\n\nCode\nrbinom(n=1, size=10, prob=0.5)\n\n\n[1] 5\n\n\nThe first rbinom(n=10, size=1, prob=0.5) produced ten simulated trials (since size=1 implies \\(x \\in (0,1)\\), or that our observed outcome in each trial is either a 0 or 1).\nThe second rbinom(n=1, size=10, prob=0.5) produced one simulated trial with up to 10 events (since size=10 implies \\(x \\in (0,1,2,...,10)\\)). In this case the simulated trial had 5/10 events observed.\nWe can see that if we specify a larger n, the number of observed events varies:\n\n\nCode\nset.seed(1016)\nout &lt;- rbinom(n=10000, size=10, prob=0.5)\nbarplot(table(out), ylim=c(0,2500), xlab=\"x\", ylab=\"Frequency\")"
  },
  {
    "objectID": "recitation/r32/index.html",
    "href": "recitation/r32/index.html",
    "title": "Central Limit Theorem Example: Chi-Squared Distributed Data",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\nNote, this builds off the week 3 practice problems."
  },
  {
    "objectID": "recitation/r32/index.html#creating-objects-within-the-loop-to-save-results-aka-you-wont-believe-this-crazy-programming-trick-that-ended-up-with-the-programmer-in-programmer-jail-red-pill-vs.-blue-pill-etc.",
    "href": "recitation/r32/index.html#creating-objects-within-the-loop-to-save-results-aka-you-wont-believe-this-crazy-programming-trick-that-ended-up-with-the-programmer-in-programmer-jail-red-pill-vs.-blue-pill-etc.",
    "title": "Central Limit Theorem Example: Chi-Squared Distributed Data",
    "section": "Creating Objects Within the Loop to Save Results (aka, You Won’t Believe This Crazy Programming Trick (that ended up with the programmer in Programmer Jail), Red Pill vs. Blue Pill, etc.)",
    "text": "Creating Objects Within the Loop to Save Results (aka, You Won’t Believe This Crazy Programming Trick (that ended up with the programmer in Programmer Jail), Red Pill vs. Blue Pill, etc.)\nA final way (not included in the solutions for the Week 3 practice problems) that I only include for completeness and in response to some recitation requests is to use an assign function TO CREATE OBJECTS TO STORE YOUR RESULTS INSIDE THE LOOP (cue: (1) with great power comes great responsibility, (2) there was weeping and gnashing of teeth, (3) if you play with fire you might get burned, etc., etc.).\nI generally do NOT like using assign functions within things like loops, apply functions, etc. since it can very quickly have unintended consequences. If you initialize vectors, matrices, lists, data frames, etc. beforehand to store results in you generally have some idea of what you are getting into. By creating new objects within something like a loop you can end up accidentally creating 100s, 1000s, etc. of objects that may eat up your memory and send you down Blue Screen Memory Lane. It can be useful if you are adding new columns to a data frame where you just want to loop through some calculation or transformation (which is fairly contained), but otherwise it is like playing with fire.\nOkay, with all these warnings and caveats, let’s see what happens:\n\n\nCode\nset.seed(6611) # set seed for reproducibility (we may be playing with fire, but we aren't monsters!)\nnsim &lt;- 500\nsizeVec &lt;- c(10,20,30,40,50)\n\nfor(j in 1:5){\n  \n  res_vec &lt;- rep(NA, nsim) # initialize object to store results in\n  \n  for(i in 1:nsim){\n    chisqData &lt;- rchisq(n=sizeVec[j], df=2)\n    res_vec[i] &lt;- mean(chisqData) # save the i-th out of nsim simulations in the i-th row and in the j-th column for its corresponding sample size\n  }\n  \n  # use the assign function to concatenate a vector name with paste0 and save the results\n  assign(paste0('chisq_mean_res_n',sizeVec[j]), # name of object\n         value = res_vec) # what to save in the object\n  \n}\n\n\nAnother limitation with this approach is that we still need to summarize the results, but we either have to loop through 5 different objects or combine the vectors into a single matrix:\n\n\nCode\n# use \"get\" to leverage our paste0 function to apply over the sample size objects/vectors\nmeanMatrix_highwaytohell &lt;- sapply(X=sizeVec, FUN = function(x) get(paste0('chisq_mean_res_n',x)))\n\nhead(meanMatrix_highwaytohell)\n\n\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 1.216040 2.327663 1.653050 1.787541 2.144370\n[2,] 2.341209 1.624362 1.814585 2.141050 2.024248\n[3,] 2.449218 1.499997 2.162688 2.297704 1.866892\n[4,] 2.852377 1.551701 1.786574 3.001661 1.903967\n[5,] 2.653542 2.332737 1.921918 2.001974 2.022062\n[6,] 2.000464 1.662547 2.120618 2.455170 1.600771\n\n\nFortunately, this time we played with fire and did not get burned."
  },
  {
    "objectID": "recitation/r34/index.html",
    "href": "recitation/r34/index.html",
    "title": "Power and Type I Error Rate Calculations via Simulation Studies",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\nFor this problem, we initially present power calculations via formulas/functions in R, before transitioning to how we could use simulation studies to estimate our statistical power. This is a great approach to starting to play around with simulation studies, where we have “known” solutions with existing functions or formulas, but want to see how close we can get via simulation. If we can structure these simulations correctly, we may be more confident for future problems where we don’t have an answer to necessarily compare to."
  },
  {
    "objectID": "recitation/r34/index.html#scenario-1-type-i-error-via-simulation",
    "href": "recitation/r34/index.html#scenario-1-type-i-error-via-simulation",
    "title": "Power and Type I Error Rate Calculations via Simulation Studies",
    "section": "Scenario 1: Type I Error via Simulation",
    "text": "Scenario 1: Type I Error via Simulation\nScenario 1: Use simulation to show that the one-sample t-test has the correct significance level, that is that under the null hypothesis the test rejects about 5% of the time. Use 10,000 iterations and the seed value 2345.\nUsing a loop in R, carry out Scenario 1. Summarize your results in a brief paragraph.\nWe’ll explore a few strategies below to structuring simulations.\n\nStrategy 1: The “Counter” Set-up\nOne strategy to estimate the power is to just keep track of how many times our p-value is less than \\(\\alpha\\) by increasing a “counter” by 1 for each case.\nHere we will loop through 10,000 simulated trials where \\(n=5\\), \\(\\sigma_{\\text{change}} = 75\\) mcg/dL, \\(\\alpha= 0.05\\), and our difference is 0 with a one-sample, two-sided t-test. Since we are using a counter, the index i does not appear anywhere within our loop:\n\n\nCode\n# Set the seed and define our known/desired values\nset.seed(2345)\nn &lt;- 5\nmean &lt;- 0\nsd &lt;- 75\nnumTrials &lt;-10000\nalpha &lt;- 0.05 \n  \n# For this for loop, we will \"count\" the number of times we have p&lt;0.05 for a t-test \ncount&lt;- 0\nfor(i in 1:numTrials){\n  y &lt;- rnorm(n,mean,sd) # simulate the data for the i-th simulation\n  t &lt;- t.test(y, mu = 0, alternative = \"two.sided\") # conduct a one-sample, two-sided t-test\n  if(t$p.value &lt; 0.05){ count &lt;- count + 1 }else{ count &lt;- count }\n}\n\n# calculate the type I error rate (i.e., the proportion of simulations where we reject H0 incorrectly since we are simulating the null)\ncount\n\n\n[1] 498\n\n\nCode\nrejectionRate &lt;- count/numTrials \nrejectionRate\n\n\n[1] 0.0498\n\n\nOur summary should detail the context of our simulation and denote the result:\nThe simulation results using 10,000 simulated data sets under the null hypothesis that the mean is 0, with a sample size of 5 and \\(\\sigma_{\\text{change}}\\) assumed to be 75 mcg/dL, indicate that we will reject the null hypothesis 4.98% of the time. This is very close to our desired type I error rate of 5%.\n\n\nStrategy 2: The “Counter” Set-up, but Let’s Calculate our Own p-value by “Hand”!\nIf we really wanted to, we could also calculate our t-test p-value by hand by comparing our calculated t-score (\\(t=\\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}}\\)) to its corresponding distribution of \\(t_{n-1}\\). In R we can accomplish this with the pt(q=tscore, df=n-1) function (i.e., the CDF of the t-distribution at the given t-score).\nWhen calculating the p-value from the t-score for a two-sided hypothesis test, we need to take the p-value and multiply by 2. However, if we have a positive t-score, then our calculated area under the curve (AUC) (i.e., our CDF from pt()) will be &gt;0.5, and lead to a p-value &gt;1 (i.e., impossible!). To account for this in our code example below, we can take the absolute value of our t-score calculation and multiply it by -1 to ensure we are estimating the AUC below and there have an estimate of the probability &lt;0.5. Then we can multiply by 2 to account for the two sided nature of our test.\nAs a brief check:\n\n\nCode\nset.seed(2345)\n\ny &lt;- rnorm(n=5, mean=0, sd=75)\ntstat &lt;- -1 * abs( (mean(y)-0) / (sd(y)/sqrt(n)) ) # t-statistic is (sample mean - null mean) / SE(sample mean)\n2 * pt( tstat, df=n-1) # our p-value\n\n\n[1] 0.673227\n\n\nCode\n# Compare to the t-test\nt.test(y, mu = 0, alternative = \"two.sided\")$p.value\n\n\n[1] 0.673227\n\n\n\n\nCode\n# Set the seed and define our known/desired values\nset.seed(2345)\nn &lt;- 5\nmean &lt;- 0\nsd &lt;- 75\nnumTrials &lt;-10000\nalpha &lt;- 0.05 \n  \n# For this for loop, we will \"count\" the number of times we have p&lt;0.05 for a t-test \ncount&lt;- 0\nfor(i in 1:numTrials){\n  y &lt;- rnorm(n,mean,sd) # simulate the data for the i-th simulation\n  tstat &lt;- -1 * abs( (mean(y)-0) / (sd(y)/sqrt(n)) )\n  tpval &lt;- pt( tstat, df=n-1) * 2\n  if(tpval &lt; 0.05){ count &lt;- count + 1 }else{ count &lt;- count }\n}\n\n# calculate the type I error rate (i.e., the proportion of simulations where we reject H0 incorrectly since we are simulating the null)\ncount\n\n\n[1] 498\n\n\nCode\nrejectionRate &lt;- count/numTrials \nrejectionRate\n\n\n[1] 0.0498\n\n\n\n\nStrategy 3: Track Each Simulation p-value\nWe could also use the index, i, if we wanted to keep track of each p-value:\n\n\nCode\n# Set the seed and define our known/desired values\nset.seed(2345)\nn &lt;- 5\nmean &lt;- 0\nsd &lt;- 75\nnumTrials &lt;-10000\nalpha &lt;- 0.05 \n  \n# For this for loop, we will save the p-value from each t.test result \npvec &lt;- rep(NA, numTrials)\nfor(i in 1:numTrials){\n  y &lt;- rnorm(n,mean,sd) # simulate the data for the i-th simulation\n  t &lt;- t.test(y, mu = 0, alternative = \"two.sided\") # conduct a one-sample, two-sided t-test\n  pvec[i] &lt;- t$p.value\n}\n\n# check pvec\nhead(pvec)\n\n\n[1] 0.6732270 0.3553252 0.1288888 0.5903654 0.7167337 0.5639400\n\n\nCode\n# calculate the type I error rate (i.e., the proportion of simulations where we reject H0 incorrectly since we are simulating the null)\nmean( pvec &lt; 0.05 ) \n\n\n[1] 0.0498\n\n\nWe see here we have the same estimated “power” (really the type I error rate since we’re simulating the assumed null mean value of 0).\nWe can also visualize in this approach the distribution of our p-values under the null:\n\n\nCode\nhist( pvec, xlab='p-value' )\n\n\n\n\n\nWe can note it is pretty uniform under the null."
  },
  {
    "objectID": "recitation/r34/index.html#scenario-2-power-via-simulation-i.e.-the-alternative-hypothesis-cases",
    "href": "recitation/r34/index.html#scenario-2-power-via-simulation-i.e.-the-alternative-hypothesis-cases",
    "title": "Power and Type I Error Rate Calculations via Simulation Studies",
    "section": "Scenario 2: Power via Simulation (i.e., The Alternative Hypothesis Case(s))",
    "text": "Scenario 2: Power via Simulation (i.e., The Alternative Hypothesis Case(s))\nTo calculate the results under the alternative we can just modify the above code for either strategy to replace mean &lt;- 0 with mean &lt;- 100.\nOne interesting thing to note, is that the distribution of the p-values will no longer be uniform:\n\n\nCode\n# Set the seed and define our known/desired values\nset.seed(2345)\nn &lt;- 5\nmean &lt;- 100 # CHANGED TO 100 FROM 0\nsd &lt;- 75\nnumTrials &lt;-10000\nalpha &lt;- 0.05 \n  \n# For this for loop, we will save the p-value from each t.test result \npvec &lt;- rep(NA, numTrials)\nfor(i in 1:numTrials){\n  y &lt;- rnorm(n,mean,sd) # simulate the data for the i-th simulation\n  t &lt;- t.test(y, mu = 0, alternative = \"two.sided\") # conduct a one-sample, two-sided t-test\n  pvec[i] &lt;- t$p.value\n}\n\nhist( pvec, xlab='p-value', main='H1: Mean of 100' )\n\n\n\n\n\nWe could also use our “counter” approach above:\n\n\nCode\n# Set the seed and define our known/desired values\nset.seed(2345)\nn &lt;- 5\nmean &lt;- 100 # CHANGED TO 100 FROM 0\nsd &lt;- 75\nnumTrials &lt;-10000\nalpha &lt;- 0.05 \n  \n# For this for loop, we will \"count\" the number of times we have p&lt;0.05 for a t-test \ncount&lt;- 0\nfor(i in 1:numTrials){\n  y &lt;- rnorm(n,mean,sd) # simulate the data for the i-th simulation\n  t &lt;- t.test(y, mu = 0, alternative = \"two.sided\") # conduct a one-sample, two-sided t-test\n  if(t$p.value &lt; 0.05){ count &lt;- count + 1 }else{ count &lt;- count }\n}\n\n# calculate the power (since we aren't simulating the null)\ncount\n\n\n[1] 6087\n\n\nCode\nrejectionRate &lt;- count/numTrials \nrejectionRate\n\n\n[1] 0.6087"
  },
  {
    "objectID": "recitation/r34/index.html#comparison-of-calculating-type-i-error-rates-and-power-from-simulation-studies",
    "href": "recitation/r34/index.html#comparison-of-calculating-type-i-error-rates-and-power-from-simulation-studies",
    "title": "Power and Type I Error Rate Calculations via Simulation Studies",
    "section": "Comparison of Calculating Type I Error Rates and Power from Simulation Studies",
    "text": "Comparison of Calculating Type I Error Rates and Power from Simulation Studies\nYou may be wondering, how could we use nearly identical code with the exception of changing mean &lt;- 0 to mean &lt;- 100 to estimate both the type I error rate and the statistical power? Let’s return to the four possible outcomes for our NHST (null hypothesis significance testing) case, but change the set-up a bit:\n\n\n\n\n\n\n\n\nReality (what we simulate)\nSample Result (the observed data analysis)\nConnection to rejection rate (RR): \\(p \\le \\alpha\\)\n\n\n\n\nNull Scenario (\\(H_0\\) true)\nFail to reject \\(H_0\\) (correct)\n1 - RR\n\n\nNull Scenario (\\(H_0\\) true)\nReject \\(H_0\\) (Type I Error)\nRR\n\n\nAlternative Scenario (\\(H_0\\) false)\nFail to reject \\(H_0\\) (Type II Error)\n1 - RR\n\n\nAlternative Scenario (\\(H_0\\) false)\nReject \\(H_0\\) (correct)\nRR\n\n\n\nWe can see from the table above, that the rejection rate from your simulation can represent our type I error if you are simulating data assuming the null hypothesis is true OR our power if we are simulating data where the null hypothesis is false.\nStrictly speaking, there is only one null value, but potentially infinite alternative hypotheses. For example, in a one-sample t-test we might assume \\(H_0 \\colon \\; \\mu_0 = 0\\). In this case, anything we simulate where our population mean isn’t equal to 0 (i.e., \\(\\mu \\neq 0\\)) is an alternative scenario!\nThe power will increase as our difference gets farther from the \\(H_0\\) (assuming we keep the same \\(n\\), effect size, and standard deviation):\n\n\nCode\n### SIMULATE DIFFERENCE OF ONLY 25, INSTEAD OF DESIRED 100 FROM PROBLEM\n\n# Set the seed and define our known/desired values\nset.seed(2345)\nn &lt;- 5\nmean &lt;- 25 # CHANGED TO 10 FROM 0\nsd &lt;- 75\nnumTrials &lt;-10000\nalpha &lt;- 0.05 \n  \n# For this for loop, we will \"count\" the number of times we have p&lt;0.05 for a t-test \ncount&lt;- 0\nfor(i in 1:numTrials){\n  y &lt;- rnorm(n,mean,sd) # simulate the data for the i-th simulation\n  t &lt;- t.test(y, mu = 0, alternative = \"two.sided\") # conduct a one-sample, two-sided t-test\n  if(t$p.value &lt; 0.05){ count &lt;- count + 1 }else{ count &lt;- count }\n}\n\nrejectionRate25 &lt;- count/numTrials \nrejectionRate25\n\n\n[1] 0.09\n\n\n\n\nCode\n### SIMULATE DIFFERENCE OF 250, INSTEAD OF DESIRED 100 FROM PROBLEM\n\n# Set the seed and define our known/desired values\nset.seed(2345)\nn &lt;- 5\nmean &lt;- 250 # CHANGED TO 10 FROM 0\nsd &lt;- 75\nnumTrials &lt;-10000\nalpha &lt;- 0.05 \n  \n# For this for loop, we will \"count\" the number of times we have p&lt;0.05 for a t-test \ncount&lt;- 0\nfor(i in 1:numTrials){\n  y &lt;- rnorm(n,mean,sd) # simulate the data for the i-th simulation\n  t &lt;- t.test(y, mu = 0, alternative = \"two.sided\") # conduct a one-sample, two-sided t-test\n  if(t$p.value &lt; 0.05){ count &lt;- count + 1 }else{ count &lt;- count }\n}\n\nrejectionRate250 &lt;- count/numTrials \nrejectionRate250\n\n\n[1] 0.9999\n\n\nIn these cases, we that an expected response of only 25 mcg/dL has 9% power, 100 mcg/dL has 60.87% power, and 250 mcg/dL has 99.99% power."
  },
  {
    "objectID": "recitation/r34/index.html#the-hw-hint",
    "href": "recitation/r34/index.html#the-hw-hint",
    "title": "Power and Type I Error Rate Calculations via Simulation Studies",
    "section": "The HW Hint",
    "text": "The HW Hint\nThe homework hint from which this recitation problem comes gives one approach which completely would avoid using loops or apply statements to generate the data. Its skeleton looks like:\n\n\nCode\n#### Using a function in R ####\n\ncompute_power = function(n, mean, sigma, numTrials, alpha){\n\n  # Generate a matrix with each column being 1 simulated data set\n  sample &lt;- matrix(rdist(n*numTrials, mean, sigma), ncol=numTrials)\n  # rdist should be replaced with desired distribution\n  \n  # Now, write out elements of the test statistic, e.g.\n  # xbar &lt;- apply(sample, 2, mean) #find mean of each column of matrix\n  # variance &lt;- apply(sample, 2, var) #find variance each column of matrix\n  # df.num = n-1 #e.g. degrees of freedom might be needed\n\n  # .\n  # combine elements of test statistic, e.g. numerator, denominator\n  # .\n  \n  test.stat = #test statistic formula based on elements above\n  \n  # Result of the function is the proportion of rejected hypothesis tests over all of the trials\n  return (mean(abs(test.stat) &gt;= qdist((1-(alpha/2)), parameter of dist))) \n  # qdist should be replaced with the quantile function for the sampling dist. of the test statistic \n}\n\n# Now, call the function with the arguments it needs - e.g. n, mean,\n# sd, number of trials, alpha\n\nset.seed(2345)\ncompute_power(n, mean, sd, trials, alpha)\n#power value will show here\n\n\nLet’s see one way we could utilize this function shell:\n\n\nCode\ncompute_rejectionrate_v1 &lt;- function(n, mean, sigma, numTrials, alpha){\n### Calculate the rejection rate for simulated data from a normal distribution using a one-sample t-test\n# n: sample size for each simulated data set\n# mean: mean to simulate from (0 = null hyp/type I error, anything else = alternative hyp/power)\n# sigma: standard deviation to simulate from\n# numTrials: number of simulated trials with n individuals to complete\n# alpha: the desired level of significance, type I error rate\n  \n  # First, simulate n times the number of trials worth of participants\n  sim_dat &lt;- rnorm(n = n*numTrials, mean = mean, sd = sigma)\n  \n  # Second, place the simulated data into a matrix with a column for each participant\n  # NOTE: to reproduce our for loop, we want the data to be entered by column (i.e., fill column 1 with the first \"n\" observations, then column 2 with the next \"n\", etc.)\n  sample &lt;- matrix( sim_dat, ncol=numTrials )\n  \n  # Third, calculate the test statistic \"by hand\"\n  # NOTE: since each column in sample is a simulated dataset, we need to somehow calculate the mean and variance for each column (here we use apply)\n  xbar &lt;- apply(sample, 2, mean)\n  variance &lt;- apply(sample, 2, var)\n  df.num = n-1\n  test.stat &lt;- (xbar-0)/sqrt(variance/n)\n  \n  # Finally, calculate the number of our test statistic exceeds the critical value\n  return(mean(abs(test.stat) &gt;= qt((1-(alpha/2)), df.num))) \n}\n\n# Compare that our type I error matches before\nset.seed(2345)\ncompute_rejectionrate_v1(n=5, mean=0, sigma=75, numTrials=10000, alpha=0.05)\n\n\n[1] 0.0498\n\n\nCode\n# Check that power matches prior estimate\nset.seed(2345)\ncompute_rejectionrate_v1(n=5, mean=100, sigma=75, numTrials=10000, alpha=0.05)\n\n\n[1] 0.6087\n\n\nThis, in and of itself, is somewhat opaque since we define our arguments and then have to see what happens. We can check our code by running it outside the function:\n\n\nCode\nset.seed(2345)\nn &lt;- 5\nmean &lt;- 0\nsigma &lt;- 75\nnumTrials &lt;-10000\nalpha &lt;- 0.05 \n\n# First, simulate n times the number of trials worth of participants\nsim_dat &lt;- rnorm(n = n*numTrials, mean = mean, sd = sigma)\nlength(sim_dat); head(sim_dat) # check length and first few values of data\n\n\n[1] 50000\n\n\n[1] -89.356848  41.197541  -4.680386  19.908113 -17.594813 -74.795370\n\n\nCode\n# Second, place the simulated data into a matrix with a column for each participant\n# NOTE: to reproduce our for loop, we want the data to be entered by column (i.e., fill column 1 with the first \"n\" observations, then column 2 with the next \"n\", etc.)\nsample &lt;- matrix( sim_dat, ncol=numTrials )\nsample[1:5,1:10] # see first 10 columns, i.e., trials\n\n\n           [,1]       [,2]      [,3]       [,4]       [,5]         [,6]\n[1,] -89.356848  -74.79537 154.15116  -72.09673  55.399446  -0.09609167\n[2,]  41.197541 -104.84739  68.26773  136.62011   3.887565  21.15608141\n[3,]  -4.680386  -16.97116 -28.79828 -114.18995 -37.435229  18.99210972\n[4,]  19.908113   21.73614  93.07556   12.54086  61.471853 -29.74726039\n[5,] -17.594813   34.07838  14.80340  -98.47009 -40.738694  20.50154995\n           [,7]     [,8]      [,9]     [,10]\n[1,]  -30.15872 60.55207  84.51153  13.61654\n[2,]   20.21886 20.47390  16.76241 -89.90383\n[3,] -103.08042 91.25193 -54.58619  12.73391\n[4,]  -74.07684 12.92561 -20.76861 -20.98467\n[5,]  -25.85880 93.79314 -26.96530 -77.98343\n\n\nCode\n# Third, calculate the test statistic \"by hand\"\n# NOTE: since each column in sample is a simulated dataset, we need to somehow calculate the mean and variance for each column (here we use apply)\nxbar &lt;- apply(sample, 2, mean)\nhead(xbar)\n\n\n[1] -10.105279 -28.159882  60.299915 -27.119160   8.516988   6.161278\n\n\nCode\nvariance &lt;- apply(sample, 2, var)\nhead(variance) \n\n\n[1]  2474.7792  3636.0611  4988.5517 10769.6817  2390.3356   480.9239\n\n\nCode\ndf.num = n-1\n\ntest.stat &lt;- (xbar-0)/sqrt(variance/n)\nhead(test.stat)\n\n\n[1] -0.4542188 -1.0442398  1.9090375 -0.5843322  0.3895306  0.6282285\n\n\nCode\n# Finally, calculate the number of our test statistic exceeds the critical value\n\nqt((1-(alpha/2)), df.num)\n\n\n[1] 2.776445\n\n\nCode\nmean( abs(test.stat) &gt;= qt((1-(alpha/2)), df.num) )\n\n\n[1] 0.0498"
  },
  {
    "objectID": "recitation/r36/index.html",
    "href": "recitation/r36/index.html",
    "title": "Creating ROC Curves from Tabular Data",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r36/index.html#strategy-1-create-a-data-frame-manually",
    "href": "recitation/r36/index.html#strategy-1-create-a-data-frame-manually",
    "title": "Creating ROC Curves from Tabular Data",
    "section": "Strategy 1: Create a Data Frame Manually",
    "text": "Strategy 1: Create a Data Frame Manually\nIf we haven’t been given any of the data in tabular form yet, it may be just as easy to create a data frame directly:\n\n\nCode\nceliac_df &lt;- data.frame(\n  celiac = c(rep(0,863), rep(1,14)), # create variable for disease status\n  ttg = c(rep(0,841),rep(4,22), rep(0,2),rep(4,3),rep(10,9)) # create variable for tTG value\n)\n\n\nThen we can see that using pROC::roc we can generate a receiver operating characteristic curve and calculated its AUC:\n\n\nCode\nceliac_roc &lt;- pROC::roc( celiac ~ ttg, data=celiac_df )\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\nNotice how the output tells us which celiac group is assumed to be the controls and the cases. This can be helpful to check that we have the right classifications for our interpretations. It also tells us which directionality of the numeric ttg is being used for classification (i.e., specifically that values below a threshold are assumed to predict controls and values above the threshold to predict cases).\n\n\nCode\nlibrary(pROC)\nplot(celiac_roc, print.auc=TRUE,print.auc.x=1, print.auc.y=1)\n\n\n\n\n\nCode\nauc(celiac_roc)\n\n\nArea under the curve: 0.924\n\n\nWith only 3 thresholds (i.e., 0, 4, and 10) it isn’t the most exciting ROC curve, but we do see that its high AUC suggests tTG as a predictive tool to identify Celiac disease is better than random chance."
  },
  {
    "objectID": "recitation/r36/index.html#strategy-2-convert-a-contingency-table-to-a-data-frame",
    "href": "recitation/r36/index.html#strategy-2-convert-a-contingency-table-to-a-data-frame",
    "title": "Creating ROC Curves from Tabular Data",
    "section": "Strategy 2: Convert a Contingency Table to a Data Frame",
    "text": "Strategy 2: Convert a Contingency Table to a Data Frame\nAnother option is to use some combination of functions to manipulate an existing contingency table of the \\(N\\)’s in each group. Here we see how we can use reshape2::melt and splitstackshape::expandRows:\n\n\nCode\n# Step 1: create a table of the information, here we'll just make it as a matrix object\nceliac_mat &lt;- matrix( c(841,22,0,2,3,9), ncol=2, byrow=FALSE, dimnames=list(c('&lt;4','4-10','&gt;10'), c('Not Celiac','Celiac')))\n\nceliac_mat\n\n\n     Not Celiac Celiac\n&lt;4          841      2\n4-10         22      3\n&gt;10           0      9\n\n\n\n\nCode\n# Step 2: convert to a data frame\nlibrary(reshape2)\nlibrary(splitstackshape)\n\nceliac_melt &lt;- melt(celiac_mat)\nceliac_melt\n\n\n  Var1       Var2 value\n1   &lt;4 Not Celiac   841\n2 4-10 Not Celiac    22\n3  &gt;10 Not Celiac     0\n4   &lt;4     Celiac     2\n5 4-10     Celiac     3\n6  &gt;10     Celiac     9\n\n\nCode\nceliac_df2 &lt;- expandRows(celiac_melt, 'value')\n\n\nThe following rows have been dropped from the input: \n\n3\n\n\nCode\ndim(celiac_df2)\n\n\n[1] 877   2\n\n\nCode\nceliac_df2[c(1,100,500,850,867,877),]\n\n\n      Var1       Var2\n1       &lt;4 Not Celiac\n1.99    &lt;4 Not Celiac\n1.499   &lt;4 Not Celiac\n2.8   4-10 Not Celiac\n5.1   4-10     Celiac\n6.8    &gt;10     Celiac\n\n\nFor the pROC::roc function, we need to convert this into a numeric format. We should also create a column for Celiac to avoid the function having to induce a logical outcome of TRUE or FALSE based on just the character string alone (i.e., being explicit helps us avoid unknown R defaults that might affect our results). One approach is to create new columns and leverage which() statements:\n\n\nCode\n# Create column for numeric version of tTG variable where we set lower limit as value\nceliac_df2$ttg &lt;- NA\nceliac_df2$ttg[which(celiac_df2$Var1 == '&lt;4')] &lt;- 0\nceliac_df2$ttg[which(celiac_df2$Var1 == '4-10')] &lt;- 4\nceliac_df2$ttg[which(celiac_df2$Var1 == '&gt;10')] &lt;- 10\n\n# Create column for logical indicator of Celiac\nceliac_df2$celiac &lt;- celiac_df2$Var2=='Celiac'\n\n\nWith a numeric variable, we can generate the ROC curve and calculate the AUC:\n\n\nCode\nceliac_roc2 &lt;- pROC::roc( celiac ~ ttg, data=celiac_df2 )\n\n\nSetting levels: control = FALSE, case = TRUE\n\n\nSetting direction: controls &lt; cases\n\n\nCode\nplot(celiac_roc2, print.auc=TRUE,print.auc.x=1, print.auc.y=1)\n\n\n\n\n\nCode\nauc(celiac_roc2)\n\n\nArea under the curve: 0.924"
  },
  {
    "objectID": "recitation/r38/index.html",
    "href": "recitation/r38/index.html",
    "title": "Tests of Association for 2x2 Tables",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r38/index.html#chi-squared-tests",
    "href": "recitation/r38/index.html#chi-squared-tests",
    "title": "Tests of Association for 2x2 Tables",
    "section": "Chi-Squared Tests",
    "text": "Chi-Squared Tests\nThe original statistic proposed by Karl Pearson is\n\\[ X^2 = \\sum_{i=1}^{2} \\sum_{j=1}^{2} \\frac{(O_{ij}-E_{ij})^2}{E_{ij}} \\sim \\chi^{2}_{1},  \\]\nwhere the \\(\\chi^{2}_{1}\\) is the square of a standard normal distribution, i.e \\(\\chi_{1}^{2} = Z^{2}\\).\nHowever, because we are applying a continuous distribution to discrete data, we often use the Yates-corrected version that corrects for continuity:\n\\[ X^2 = \\sum_{i=1}^{2} \\sum_{j=1}^{2} \\frac{(|O_{ij}-E_{ij}| - 0.5)^2}{E_{ij}} \\sim \\chi^{2}_{1}  \\]\nBoth versions have the same assumptions that must be met: all of the expected values must be greater than or equal to 5 and the two samples are independent. This is because for these test statistics, we are calculating significance (e.g., p-values) based on asymptotic properties. This means we have confidence in our assumption that the test statistic is truly distributed as a \\(\\chi^2_{1}\\) distribution. If this is violated, it is a good sign that we should examine use of other testing strategies (e.g., exact tests below, permutation tests, etc.)."
  },
  {
    "objectID": "recitation/r38/index.html#exact-tests",
    "href": "recitation/r38/index.html#exact-tests",
    "title": "Tests of Association for 2x2 Tables",
    "section": "Exact Tests",
    "text": "Exact Tests\nIf we notice that our assumption for the chi-squared test having at least 5 in each expected cell calculation is violated, we can use exact tests that are not based on approximations. Two different tests were discussed in our lecture slides: Fisher’s and Barnard’s exact tests.\nFisher’s exact test calculates an exact p-value based on the hypergeometric distribution. Barnard’s test will calculate an exact p-value from either a multinomial, binomial, or hypergeometric distribution depending on the study design being a cross-sectional study, case-control study, or one that stops after a set number of events has been observed, respectively.\nSince they work on calculating the exact probability, it can be computationally burdensome as the sample size increases to use exact tests. However, we can show that as the sample size increases, the resulting p-values tend to converge to what we would observe for the chi-squared test. This helps us identify that if we meet the assumptions of a chi-squared test, especially if we have larger sample sizes, we can just use the computationally less intense asymptotic tests."
  },
  {
    "objectID": "recitation/r38/index.html#paired-data-tests",
    "href": "recitation/r38/index.html#paired-data-tests",
    "title": "Tests of Association for 2x2 Tables",
    "section": "Paired Data Tests",
    "text": "Paired Data Tests\nIf our assumption of samples being independent is violated, we cannot use the chi-squared test of independence. This could occur if our study design involved matching two participants to belong to two separate groups (e.g., matching by age/sex/disease severity for participants in a study) or because the test if conducted on the same participant (e.g., a cross-over trial where participants receive both treatments).\nTo address this violation of the independence assumption, we need to use a test that accounts for the dependent nature of the data. McNemar’s test is one such test. If you have a small sample (e.g., the number of discordant pairs that have different outcomes is &lt;20) we can calculate an exact p-value using an exact binomial test, whereas if you have a “large” sample (e.g., &gt;20 discordant pairs) we can use asymptotic theory to essentially conduct a chi-squared test."
  },
  {
    "objectID": "recitation/r38/index.html#how-to-choose-what-to-use",
    "href": "recitation/r38/index.html#how-to-choose-what-to-use",
    "title": "Tests of Association for 2x2 Tables",
    "section": "How to Choose What to Use",
    "text": "How to Choose What to Use\nMany times when we are doing an analysis, we realize that there is not a single method that is “right” to use, but multiple choices. A good example of this is that if we have enough computation power to compare some exposure between two independent samples, we could run a Fisher’s exact test or a chi-squared test (with our without continuity correction) and have both approaches be perfectly valid.\nThis can lead to the very true concept that statistics is just as much art as science. If you are doing an analysis, some things to keep in mind that might help choose the best method(s):\n\nAre my assumptions met? If not, don’t do the test! (Or at least run some simulations to identify what are the implications of the violated assumptions.)\nWhat is the most parsimonious? Parsimony in statistics is the idea that we should choose the simplest model or approach for addressing a given question. If you have multiple variables to test for independence/association, you might choose to use Fisher’s exact tests for all of them to keep it simple when presenting your results (versus noting chi-squared versus Fisher’s exact tests used depending on modeling assumptions). When we transition to linear regression, we’ll see this concept also apply to reducing the complexity of the chosen regression model.\nWhat are the existing customs or practices in this discipline? In certain cases, if multiple approaches are equally valid, you may consider seeing if the field has a standard approach they like to use. As long as it doesn’t violate our assumptions and we don’t think it is overly complex for a given scenario, we might elect to use what others are familiar with to avoid potential confusion."
  },
  {
    "objectID": "recitation/r4/index.html",
    "href": "recitation/r4/index.html",
    "title": "Multiple Testing Correction and Their Use in Randomized Trials",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r4/index.html#multiple-testing-within-clinical-trials",
    "href": "recitation/r4/index.html#multiple-testing-within-clinical-trials",
    "title": "Multiple Testing Correction and Their Use in Randomized Trials",
    "section": "Multiple Testing Within Clinical Trials",
    "text": "Multiple Testing Within Clinical Trials\nWithin a randomized controlled trial, controlling for multiple testing is often considered very important. But there are some caveats:\n\nIt depends on the phase of the trial. Phase I (safety, dose-finding, feasibility) and Phase II (more safety, initial efficacy) may not be as concerned since we are looking for any signal of a significant association (for safety concerns, efficacy, etc.) to know if future phases are worth investing in.\nIt depends on whether the tests are for the primary, secondary, exploratory, or safety outcomes. Often we try to only define a single primary outcome, but sometimes we may have co-primary. Many times we have multiple secondary outcomes, and we do wish to control for multiple testing if we are in a Phase III (or Phase IIb potentially) trial (i.e., a trial used for FDA approval or to have “confirmatory” results). Exploratory and safety may be less concerning given the more open-ended nature (exploratory) or the desire to be conservative with respect to participant experiences (safety).\n\nIf we do want to consider multiple testing corrections, there exist a wide range of strategies:\n\nRun all your tests and don’t correct, noting in the results there is no adjustment for multiple testing. You could also provide justification like the Rubin paper above or this commentary by Parker and Weir from 2022 in Trials “Multiple secondary outcome analyses: precise interpretation is important”.\nUse one of the corrections or strategies from lecture (e.g., Bonferroni, FDR, etc.).\nEmploy a hierarchical testing strategy where you order your outcome(s) by priority and keep testing them at the \\(\\alpha\\) level until one is not significant. Check out the European Medicines Agency (EMA) guidance pages 3/10 and 6/10 in “Points to Consider on Multiplicity Issues in Clinical Trials”.\n\nAnother common setting where we correct for multiple testing is with interim monitoring (i.e., where we look at study data accrued at a given stage and determine if we should stop for futility, efficacy, safety, re-estimate the sample size, etc.). Depending on the method, if we see the unblinded trial data we need to account for that by “spending \\(\\alpha\\)” and adjusting our final threshold. For example, you may specify an overall \\(\\alpha=0.05\\), but with 3 interim looks using O’Brien-Fleming boundaries for efficacy our final threshold may be \\(p&lt;0.045\\) instead of \\(p&lt;0.05\\). The trade-off being we could stop the trial early (saving resources and money) if needed."
  },
  {
    "objectID": "recitation/r41/index.html",
    "href": "recitation/r41/index.html",
    "title": "Bootstraps: the Normal Percentile Interval and Coverage, the Bootstrap Percentile Interval and Accuracy",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r41/index.html#scenario-1-sample-mean-for-n10-from-nmu10-sigma29",
    "href": "recitation/r41/index.html#scenario-1-sample-mean-for-n10-from-nmu10-sigma29",
    "title": "Bootstraps: the Normal Percentile Interval and Coverage, the Bootstrap Percentile Interval and Accuracy",
    "section": "Scenario 1: Sample Mean for \\(n=10\\) from \\(N(\\mu=10, \\sigma^2=9)\\)",
    "text": "Scenario 1: Sample Mean for \\(n=10\\) from \\(N(\\mu=10, \\sigma^2=9)\\)\nFirst, let’s examine if we simulate a normal data set with a smaller sample size (\\(n=10\\)):\n\n\nCode\nset.seed(1008)\ndat1 &lt;- rnorm(n=10, mean=10, sd=3)\nhist(dat1, main='Histogram of n=10 from N(10,9)')\n\n\n\n\n\nThe data appears to be approximately normal (although with \\(n=10\\) we may need to use our imagination a bit). Let’s conduct a bootstrap sample with 10,000 bootstrap samples to estimate our 95% CI:\n\n\nCode\n# note, we set our seed above\nn &lt;- length(dat1)\nB &lt;- 10000\nboot_vec &lt;- as.numeric(B)\n\nfor(i in 1:B){\n  boot_vec[i] &lt;- mean(sample(dat1, n, replace = TRUE) )\n}\n\nhist(boot_vec, main='Bootstrap Distribution of Sample Mean, n=10 Normal')\n\n\n\n\n\nCode\nqqnorm(boot_vec); qqline(boot_vec)\n\n\n\n\n\nBased on the histogram and QQ plot, the distribution appears mostly normal (as we would expect when we simulate data from a normal distribution and calculate the mean, which we know is exactly distributed normally). The 95% normal percentile interval is\n\n\nCode\n#Lower limit of 95% Normal CI\nLL &lt;- mean(boot_vec)-1.96*sd(boot_vec) \nLL\n\n\n[1] 8.940995\n\n\nCode\n#Upper limit of 95% Normal CI\nUL &lt;- mean(boot_vec)+1.96*sd(boot_vec) \nUL\n\n\n[1] 13.10318\n\n\nCode\nsum(boot_vec &lt; LL)/B  # Coverage of CI at lower end\n\n\n[1] 0.0271\n\n\nCode\nsum(boot_vec &gt; UL)/B  # Coverage of CI at upper end \n\n\n[1] 0.0221\n\n\nWe see here the results are pretty close for coverage! We can also compare to the bootstrap percentile interval:\n\n\nCode\nquantile(boot_vec, c(0.025,0.975))\n\n\n     2.5%     97.5% \n 8.904613 13.054325 \n\n\nHere the two intervals are extremely similar, because the coverage is pretty close to our target of 2.5%.\nFor completeness, we can also calculate the ratio of the bias to the standard error to see if it exceeds ±0.10:\n\n\nCode\n# estimate of accuracy for our bootstrap percentile interval\n(mean(boot_vec)-mean(dat1)) / sd(boot_vec)\n\n\n[1] 0.001938323"
  },
  {
    "objectID": "recitation/r41/index.html#scenario-2-sample-mean-for-n200-from-nmu10-sigma29",
    "href": "recitation/r41/index.html#scenario-2-sample-mean-for-n200-from-nmu10-sigma29",
    "title": "Bootstraps: the Normal Percentile Interval and Coverage, the Bootstrap Percentile Interval and Accuracy",
    "section": "Scenario 2: Sample Mean for \\(n=200\\) from \\(N(\\mu=10, \\sigma^2=9)\\)",
    "text": "Scenario 2: Sample Mean for \\(n=200\\) from \\(N(\\mu=10, \\sigma^2=9)\\)\nWhat if we have a much larger sample that comes from a normal distribution?\n\n\nCode\nset.seed(1021)\ndat2 &lt;- rnorm(n=200, mean=10, sd=3)\nhist(dat2, main='Histogram of n=200 from N(10,9)')\n\n\n\n\n\nThe data appears to be pretty darned normal, but let’s conduct a bootstrap sample with 10,000 bootstrap samples:\n\n\nCode\n# note, we set our seed above\nn &lt;- length(dat2)\nB &lt;- 10000\nboot_vec2 &lt;- as.numeric(B)\n\nfor(i in 1:B){\n  boot_vec2[i] &lt;- mean(sample(dat2, n, replace = TRUE) )\n}\n\nhist(boot_vec2, main='Bootstrap Distribution of Sample Mean, n=200 Normal')\n\n\n\n\n\nCode\nqqnorm(boot_vec2); qqline(boot_vec2)\n\n\n\n\n\nBased on the histogram and QQ plot, the distribution appears extremely normal (as we would expect when we simulate data from a normal distribution and calculate the mean, which we know is exactly distributed normally). The 95% normal percentile interval is\n\n\nCode\n#Lower limit of 95% Normal CI\nLL &lt;- mean(boot_vec2)-1.96*sd(boot_vec2) \nLL\n\n\n[1] 9.515987\n\n\nCode\n#Upper limit of 95% Normal CI\nUL &lt;- mean(boot_vec2)+1.96*sd(boot_vec2) \nUL\n\n\n[1] 10.34445\n\n\nCode\nsum(boot_vec2 &lt; LL)/B  # Coverage of CI at lower end\n\n\n[1] 0.025\n\n\nCode\nsum(boot_vec2 &gt; UL)/B  # Coverage of CI at upper end \n\n\n[1] 0.0258\n\n\nWe see here the results are spot on for the lower limit (LL) and close for the upper limit (UL) coverage! We can also compare to the bootstrap percentile interval:\n\n\nCode\nquantile(boot_vec2, c(0.025,0.975))\n\n\n     2.5%     97.5% \n 9.516301 10.346530 \n\n\nAgain, the two confidence intervals match pretty closely.\nFor completeness, we can also calculate the ratio of the bias to the standard error to see if it exceeds ±0.10:\n\n\nCode\n# estimate of accuracy for our bootstrap percentile interval\n(mean(boot_vec2)-mean(dat2)) / sd(boot_vec2)\n\n\n[1] 0.001533928"
  },
  {
    "objectID": "recitation/r41/index.html#scenario-3-sample-mean-for-n10-from-explambdafrac13",
    "href": "recitation/r41/index.html#scenario-3-sample-mean-for-n10-from-explambdafrac13",
    "title": "Bootstraps: the Normal Percentile Interval and Coverage, the Bootstrap Percentile Interval and Accuracy",
    "section": "Scenario 3: Sample Mean for \\(n=10\\) from \\(Exp(\\lambda=\\frac{1}{3})\\)",
    "text": "Scenario 3: Sample Mean for \\(n=10\\) from \\(Exp(\\lambda=\\frac{1}{3})\\)\nFirst, let’s examine if we simulate an exponential data set with a smaller sample size (\\(n=10\\)):\n\n\nCode\nset.seed(1008)\ndat3 &lt;- rexp(n=10, rate=1/3)\nhist(dat3, main='Histogram of n=10 from Exp(rate=1/3)')\n\n\n\n\n\nThe data appears to be…not normal. Let’s conduct a bootstrap sample with 10,000 bootstrap samples to estimate the sample mean variability:\n\n\nCode\n# note, we set our seed above\nn &lt;- length(dat3)\nB &lt;- 10000\nboot_vec3 &lt;- as.numeric(B)\n\nfor(i in 1:B){\n  boot_vec3[i] &lt;- mean(sample(dat3, n, replace = TRUE) )\n}\n\nhist(boot_vec3, main='Bootstrap Distribution of Sample Mean, n=10 Exponential')\n\n\n\n\n\nCode\nqqnorm(boot_vec3); qqline(boot_vec3)\n\n\n\n\n\nBased on the histogram and QQ plot, the distribution has some right skew (evidence we may need a larger \\(n\\) for the CLT to really kick in). The 95% normal percentile interval is\n\n\nCode\n#Lower limit of 95% Normal CI\nLL &lt;- mean(boot_vec3)-1.96*sd(boot_vec3) \nLL\n\n\n[1] 0.5287681\n\n\nCode\n#Upper limit of 95% Normal CI\nUL &lt;- mean(boot_vec3)+1.96*sd(boot_vec3) \nUL\n\n\n[1] 1.535122\n\n\nCode\nsum(boot_vec3 &lt; LL)/B  # Coverage of CI at lower end\n\n\n[1] 0.0132\n\n\nCode\nsum(boot_vec3 &gt; UL)/B  # Coverage of CI at upper end \n\n\n[1] 0.0336\n\n\nWe see here the results are NOT close for coverage…we should be concerned about how appropriate the normal percentile CI is. Rather, we can calculate and compare to the bootstrap percentile interval:\n\n\nCode\nquantile(boot_vec3, c(0.025,0.975))\n\n\n     2.5%     97.5% \n0.5814501 1.5796031 \n\n\nHere the two intervals aren’t drastically different, but it isn’t trivial (0.529 vs. 0.581 for LL, 1.535 vs. 1.580 for UL).\nFor completeness, we can also calculate the ratio of the bias to the standard error to see if it exceeds ±0.10:\n\n\nCode\n# estimate of accuracy for our bootstrap percentile interval\n(mean(boot_vec3)-mean(dat3)) / sd(boot_vec3)\n\n\n[1] -0.01292271"
  },
  {
    "objectID": "recitation/r41/index.html#scenario-4-sample-mean-for-n200-from-explambdafrac13",
    "href": "recitation/r41/index.html#scenario-4-sample-mean-for-n200-from-explambdafrac13",
    "title": "Bootstraps: the Normal Percentile Interval and Coverage, the Bootstrap Percentile Interval and Accuracy",
    "section": "Scenario 4: Sample Mean for \\(n=200\\) from \\(Exp(\\lambda=\\frac{1}{3})\\)",
    "text": "Scenario 4: Sample Mean for \\(n=200\\) from \\(Exp(\\lambda=\\frac{1}{3})\\)\nWhat if we have a much larger sample that comes from an exponential distribution?\n\n\nCode\nset.seed(1021)\ndat4 &lt;- rexp(n=200, rate=1/3)\nhist(dat4, main='Histogram of n=200 from Exp(rate=1/3)')\n\n\n\n\n\nThe data is just straight up not normal. To estimate the variability of our sample mean, let’s conduct a bootstrap sample with 10,000 bootstrap samples:\n\n\nCode\n# note, we set our seed above\nn &lt;- length(dat4)\nB &lt;- 10000\nboot_vec4 &lt;- as.numeric(B)\n\nfor(i in 1:B){\n  boot_vec4[i] &lt;- mean(sample(dat4, n, replace = TRUE) )\n}\n\nhist(boot_vec4, main='Bootstrap Distribution of Sample Mean, n=200 Exponential')\n\n\n\n\n\nCode\nqqnorm(boot_vec4); qqline(boot_vec4)\n\n\n\n\n\nBased on the histogram and QQ plot, the distribution appears decently normal (here the CLT seems to be more applicable given our larger \\(n\\)). The 95% normal percentile interval is\n\n\nCode\n#Lower limit of 95% Normal CI\nLL &lt;- mean(boot_vec4)-1.96*sd(boot_vec4) \nLL\n\n\n[1] 2.566667\n\n\nCode\n#Upper limit of 95% Normal CI\nUL &lt;- mean(boot_vec4)+1.96*sd(boot_vec4) \nUL\n\n\n[1] 3.388219\n\n\nCode\nsum(boot_vec4 &lt; LL)/B  # Coverage of CI at lower end\n\n\n[1] 0.0207\n\n\nCode\nsum(boot_vec4 &gt; UL)/B  # Coverage of CI at upper end \n\n\n[1] 0.0283\n\n\nWe see here the normal percentile CI is still a bit off from what we would desire, even though the data appears to be generally normal. The bootstrap percentile confidence interval is\n\n\nCode\nquantile(boot_vec4, c(0.025,0.975))\n\n\n    2.5%    97.5% \n2.583263 3.399906 \n\n\nWhere we see similar point estimates, but with the percentile intervals we know we are closer to our desired target.\nFor completeness, we can also calculate the ratio of the bias to the standard error to see if it exceeds ±0.10:\n\n\nCode\n# estimate of accuracy for our bootstrap percentile interval\n(mean(boot_vec4)-mean(dat4)) / sd(boot_vec4)\n\n\n[1] -0.002842806"
  },
  {
    "objectID": "recitation/r41/index.html#scenario-5-sample-mean-for-n2000-from-explambdafrac13",
    "href": "recitation/r41/index.html#scenario-5-sample-mean-for-n2000-from-explambdafrac13",
    "title": "Bootstraps: the Normal Percentile Interval and Coverage, the Bootstrap Percentile Interval and Accuracy",
    "section": "Scenario 5: Sample Mean for \\(n=2000\\) from \\(Exp(\\lambda=\\frac{1}{3})\\)",
    "text": "Scenario 5: Sample Mean for \\(n=2000\\) from \\(Exp(\\lambda=\\frac{1}{3})\\)\nWhat if we have a much, much larger sample that comes from an exponential distribution?\n\n\nCode\nset.seed(1021)\ndat5 &lt;- rexp(n=2000, rate=1/3)\nhist(dat5, main='Histogram of n=2000 from Exp(rate=1/3)')\n\n\n\n\n\nThe data is just straight up not normal. To estimate the variability of our sample mean, let’s conduct a bootstrap sample with 10,000 bootstrap samples:\n\n\nCode\n# note, we set our seed above\nn &lt;- length(dat5)\nB &lt;- 10000\nboot_vec5 &lt;- as.numeric(B)\n\nfor(i in 1:B){\n  boot_vec5[i] &lt;- mean(sample(dat5, n, replace = TRUE) )\n}\n\nhist(boot_vec5, main='Bootstrap Distribution of Sample Mean, n=2000 Exponential')\n\n\n\n\n\nCode\nqqnorm(boot_vec5); qqline(boot_vec5)\n\n\n\n\n\nBased on the histogram and QQ plot, the distribution appears decently normal (here the CLT seems to be more applicable given our larger \\(n\\)). The 95% normal percentile interval is\n\n\nCode\n#Lower limit of 95% Normal CI\nLL &lt;- mean(boot_vec5)-1.96*sd(boot_vec5) \nLL\n\n\n[1] 2.779945\n\n\nCode\n#Upper limit of 95% Normal CI\nUL &lt;- mean(boot_vec5)+1.96*sd(boot_vec5) \nUL\n\n\n[1] 3.052643\n\n\nCode\nsum(boot_vec5 &lt; LL)/B  # Coverage of CI at lower end\n\n\n[1] 0.0218\n\n\nCode\nsum(boot_vec5 &gt; UL)/B  # Coverage of CI at upper end \n\n\n[1] 0.0265\n\n\nThe normal percentile CI is still a bit off from what we would desire, even though the data appears to be even more normally distributed. The bootstrap percentile confidence interval is\n\n\nCode\nquantile(boot_vec5, c(0.025,0.975))\n\n\n    2.5%    97.5% \n2.783262 3.054583 \n\n\nAs \\(n\\) increases, we see that (1) the CLT is more “accurate” (with respect to coverage) and (2) the normal percentile and bootstrap percentile intervals become increasingly similar.\nFor completeness, we can also calculate the ratio of the bias to the standard error to see if it exceeds ±0.10:\n\n\nCode\n# estimate of accuracy for our bootstrap percentile interval\n(mean(boot_vec5)-mean(dat5)) / sd(boot_vec5)\n\n\n[1] 0.007347859"
  },
  {
    "objectID": "recitation/r43/index.html",
    "href": "recitation/r43/index.html",
    "title": "Missing Data Considerations",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r43/index.html#removal-of-missing-data",
    "href": "recitation/r43/index.html#removal-of-missing-data",
    "title": "Missing Data Considerations",
    "section": "Removal of Missing Data",
    "text": "Removal of Missing Data\nLikely the easiest approach, we simply remove missing data from our analysis(es). This is known as a complete case analysis or listwise deletion. In the most extreme version, records with any missing data in the data set are removed. In practice, I think of this more as complete cases for a specific analysis (e.g., one record might only be missing one variable, so they would only be excluded for analyses that involve that variable, but are kept for everything else). If you pull up the documentation for base R’s correlation function, ?cor, we see it lists a use argument where you specify the approach for calculation pairwise correlations (ranging from returning NA values if missing one, to complete cases, to pairwise complete cases).\nThe limitation to this approach is that we are excluding potentially valuable information. Depending on the context, it can also lead to increasingly small sample sizes even if the original data set was much larger. Biases can also be introduced, such as those who finished a study may have had a better experience than those who are lost to follow-up or dropped out.\nAs we embark on regression analyses, if we do not specify any other approach, R will assume complete case analyses for the variables included in the model (e.g., our outcome, \\(Y\\), and all predictors \\(X\\))."
  },
  {
    "objectID": "recitation/r43/index.html#imputation",
    "href": "recitation/r43/index.html#imputation",
    "title": "Missing Data Considerations",
    "section": "Imputation",
    "text": "Imputation\nAn alternative to excluding missing data is to use some form of imputation. These range from simple to extremely complex:\n\nPlug in the sample mean, median, mode, etc.\nUse the last observation carried forward (i.e., LOCF)\nAttempt some form of interpolation if the missing data is between two longitudinal time points we do have\nImpute a single data set using statistical approaches\nImputation of multiple data sets (i.e., multiple imputation)\n\nThe most robust is generally multiple imputation, but it can be extremely complex to implement. The simpler choices may be better than nothing, but can also bias the results of the analysis with respect to the underlying “truth”.\nIn practice, the best choice will be context dependent. For BIOS 6618, we will rely primarily on removing missing data since we won’t discuss some of these more advanced approaches or the limitations of mean imputation or LOCF."
  },
  {
    "objectID": "recitation/r45/index.html",
    "href": "recitation/r45/index.html",
    "title": "The ANOVA Table and the \\(F\\)-test in SLR",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r45/index.html#f-test-for-simple-linear-regression",
    "href": "recitation/r45/index.html#f-test-for-simple-linear-regression",
    "title": "The ANOVA Table and the \\(F\\)-test in SLR",
    "section": "\\(F\\)-Test for Simple Linear Regression",
    "text": "\\(F\\)-Test for Simple Linear Regression\nFrom our ANOVA table we saw that the model mean square is the regression (model) sum of squares divided by the number of predictor variables, \\(p\\), in the model (\\(p=1\\) for SLR). Theoretically, the expectation of our MSModel is \\[ E(MS_{Model}) = \\sigma^{2}_{Y|X} + \\beta_{1}^{2} \\sum_{i=1}^{n} (X_i - \\bar{X})^2 \\]\nThe residual mean square was the residual sum of squares divided by its degrees of freedom (\\(n-2\\) for SLR). Its expectation is \\[ E(MS_{Error}) = E(s_{Y|X}^{2}) = \\sigma^{2}_{Y|X} \\]\nIt can be shown that the ratio of two variances follows an \\(F\\) distribution under the null hypothesis that the two variances are equal (\\(\\sigma_1^2 = \\sigma_2^2\\)): \\[ \\frac{s_{1}^{2} / \\sigma_{1}^{2}}{s_{2}^{2} / \\sigma_{2}^{2}} \\sim F_{n_{1}-1,n_{2}-1} \\]\nIn the context of regression, under the null hypothesis that the true slope of the regression line is zero (\\(H_0: \\beta_1=0\\)), both MSModel and MSError are independent estimates of \\(\\sigma^{2}_{Y|X}\\). Thus, the ratio of the regression mean square to the residual mean square will have an \\(F\\) distribution with \\(p\\) and \\(n-p-1\\) degrees of freedom: \\[ F = \\frac{MS_{Model}}{MS_{Error}} \\sim F_{p,n-p-1} \\]\nThe \\(F\\) test is used to test if the model including covariate(s) results in a significant reduction of the residual sum of squares compared to a model containing only an intercept.\nIf the null hypothesis is true, then the expected value of the \\(F\\) ratio should be 1. If the null hypothesis is false, then the expected value of the \\(F\\) ratio is greater than 1.\nThe t-test and the F-test are equivalent for testing \\(H_0: \\beta_1 = 0\\) in simple linear regression:\n\nIf \\(X \\sim t_n\\), then \\(X^2 \\sim F_{1,n}\\).\nRecall, \\(t = \\frac{\\hat\\beta_1}{\\hat{SE}(\\hat\\beta_1)}\\), where \\(t \\sim t_{n-p-1}\\) under \\(H_0\\)."
  },
  {
    "objectID": "recitation/r47/index.html",
    "href": "recitation/r47/index.html",
    "title": "Categorical Predictors in Linear Regression",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r47/index.html#how-is-the-reference-group-accounted-for",
    "href": "recitation/r47/index.html#how-is-the-reference-group-accounted-for",
    "title": "Categorical Predictors in Linear Regression",
    "section": "How is the reference group accounted for?",
    "text": "How is the reference group accounted for?\nFor this question we will assume we are working with the reference cell model formulation. Let’s use our carotenoids example from HW9 to illustrate how this works for the model with only smoking status.\nFirst, let’s remind ourselves of the mean plasma beta-carotene levels:\n\n\nCode\nlibrary(doBy)\ncarotenoids &lt;- read.table('../../.data/carotenoids.dat')\ncolnames(carotenoids) &lt;- c('age','sex','smoke','bmi','vitamins','calories','fat',\n                    'fiber','alcohol','chol','betadiet','retdiet','betaplas','retplas')\n\ncarotenoids$smoke_factor &lt;- factor(carotenoids$smoke, levels=c(1,2,3),\n                            labels=c('Never','Former','Current'))\n\nsummaryBy( betaplas ~ smoke_factor, data=carotenoids )\n\n\n  smoke_factor betaplas.mean\n1        Never      206.0510\n2       Former      193.4696\n3      Current      121.3256\n\n\nFor the reference cell model, we select one of our smoking groups to be the reference, which all other groups are compared to. Let’s use never smokers, like before, as our reference group. The indicator variables in this case would be\n\n\n\nGroup\n\\(I_N\\)\n\\(I_F\\)\n\\(I_C\\)\n\n\n\n\nNever\n1\n0\n0\n\n\nFormer\n0\n1\n0\n\n\nCurrent\n0\n0\n1\n\n\n\nThe expected regression model is then\n\\[ E[Y] = \\beta_0 + \\beta_1 I_{F} + \\beta_{2} I_{C} \\]\nwhere \\(I_F\\) and \\(I_C\\) are indicator variables for being a former smoker or current smoker, respectively. If \\(I_F = I_C = 0\\), then someone must be in the reference group and is a never smoker.\nIf we fit the model and look at the coefficients we can make a direct connection to our mean levels within each group:\n\n\nCode\nsummary(lm(betaplas ~ smoke_factor, data=carotenoids))$coefficients\n\n\n                     Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)         206.05096   14.48037 14.2296790 9.311836e-36\nsmoke_factorFormer  -12.58139   22.26974 -0.5649546 5.725106e-01\nsmoke_factorCurrent -84.72537   31.22916 -2.7130212 7.037481e-03\n\n\n\nWe see that \\(\\hat{\\beta}_0=206.05096\\) is our group mean! This makes sense given our interpretation of the intercept as the expected mean outcome when all predictors are equal to 0.\nOur estimated slope coefficients, \\(\\hat{\\beta}_{1}\\) and \\(\\hat{\\beta}_{2}\\), then represent the difference in the mean plasma beta-carotene from the reference category. For example, \\(\\hat{\\beta}_0 + \\hat{\\beta}_1 = 206.05096 + (-12.58139) = 193.4696\\), our estimated mean plasma beta-carotene levels for former smokers! Likewise, \\(\\hat{\\beta}_0 + \\hat{\\beta}_2 = 206.05096 + (-84.72537) = 121.3256\\), the estimated mean for current smokers."
  },
  {
    "objectID": "recitation/r47/index.html#what-category-should-be-my-reference",
    "href": "recitation/r47/index.html#what-category-should-be-my-reference",
    "title": "Categorical Predictors in Linear Regression",
    "section": "What category should be my reference?",
    "text": "What category should be my reference?\nThis question comes up often when working with collaborators, and we may ourselves wonder what makes the most sense. The good news, mathematically we have the same information included regardless of the choice of reference category! Essentially, the perspective of our results is what changes (i.e., beta coefficients, p-values, etc. will change to reflect whatever the reference category is), but changing a reference category won’t affect the actual significance (e.g., if never versus former smokers was not significant in the original model, it won’t be in a subsequent model either).\nLet’s fit all the possible combinations, and see the equivalence firsthand:\n\n\nCode\ncarotenoids$smoke_factor2 &lt;- factor(carotenoids$smoke, levels=c(2,3,1),\n                            labels=c('Former','Current','Never'))\ncarotenoids$smoke_factor3 &lt;- factor(carotenoids$smoke, levels=c(3,2,1),\n                            labels=c('Current','Former','Never'))\n\nc1 &lt;- round(summary(lm(betaplas ~ smoke_factor, data=carotenoids))$coefficients,4)\nc2 &lt;- round(summary(lm(betaplas ~ smoke_factor2, data=carotenoids))$coefficients,4)\nc3 &lt;- round(summary(lm(betaplas ~ smoke_factor3, data=carotenoids))$coefficients,4)\n\nc_tab &lt;- rbind('',c1, '', c2, '', c3)\nrownames(c_tab) &lt;- c('Reference Group: Never', rownames(c1), 'Reference Group: Former', rownames(c2), 'Reference Group: Current', rownames(c3))\n\nlibrary(kableExtra)\nkbl(c_tab, col.names=c('Estimate','SE','t','p-value'), align='cccc', escape=F) %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = F, position = \"left\")# print a formatted table\n\n\n\n\n\n\nEstimate\nSE\nt\np-value\n\n\n\n\nReference Group: Never\n\n\n\n\n\n\n(Intercept)\n206.051\n14.4804\n14.2297\n0\n\n\nsmoke_factorFormer\n-12.5814\n22.2697\n-0.565\n0.5725\n\n\nsmoke_factorCurrent\n-84.7254\n31.2292\n-2.713\n0.007\n\n\nReference Group: Former\n\n\n\n\n\n\n(Intercept)\n193.4696\n16.9192\n11.4349\n0\n\n\nsmoke_factor2Current\n-72.144\n32.4321\n-2.2245\n0.0268\n\n\nsmoke_factor2Never\n12.5814\n22.2697\n0.565\n0.5725\n\n\nReference Group: Current\n\n\n\n\n\n\n(Intercept)\n121.3256\n27.6691\n4.3849\n0\n\n\nsmoke_factor3Former\n72.144\n32.4321\n2.2245\n0.0268\n\n\nsmoke_factor3Never\n84.7254\n31.2292\n2.713\n0.007\n\n\n\n\n\n\n\nNotice how the point estimates are the the same number (but differ by + or -) across models depending on the reference category. Similarly, the p-values are the same as well for any similar pairwise comparison.\nFor nominal variables (i.e., categorical with no ordering) and ordinal variables (i.e., categorical with an ordering), your choice of reference will be driven by the question at hand. For ordinal variables, it can be helpful to choose the lowest or highest category since there is (hopefully) a linear trend across categories that will be evident from the regression results.\nIn future weeks we will also discuss contrasts as a method we can use to calculate different summaries, but one nice workaround can be to refit the model with different reference categories to better align with the perspective expected or desired."
  },
  {
    "objectID": "recitation/r47/index.html#can-i-reduce-the-number-of-groups",
    "href": "recitation/r47/index.html#can-i-reduce-the-number-of-groups",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Can I reduce the number of groups?",
    "text": "Can I reduce the number of groups?\nWhen we start with a data analysis, we might initially have \\(G\\) groups. With our reference cell model, assuming we include all the initial groups, we would have a regression model including only the group variable with \\(G-1\\) predictors: \\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 I_{g=1} + \\hat{\\beta}_2 I_{g=2} + ... + \\hat{\\beta}_{G-1} I_{g=G-1}\\).\nBut the question remains, do I need all these groups? Here are a few things to consider:\n\nFirst, is it reasonable clinically/biologically/contextually to collapse any of the groups together? For example, with smoking it might sense to collapse “Former” smokers with either group depending on if we are interested in (1) history of any smoking (Never vs. Former/Current) or (2) current behavior (Never/Former vs. Current). If we think the history of smoking, even without being a current smoker, is important we should not combine those groups unless there is a very compelling reason.\nDo we have “enough” data to use for inference? If a group has a very small sample size, we might need to combine it with a different group to successfully fit certain models or have confidence in our inference. If we keep the small group size and the model, one of the biggest impacts will be a less precise standard error estimate (and therefore a smaller test statistic and p-value since the CLT directly connects the sample size with \\(\\frac{\\sigma}{\\sqrt{n}}\\)).\nIs there concerns about reducing the degrees of freedom we are “spending” on all our predictors? For example, we know that our \\(t\\)-tests in our regression model for the coefficient table rely on a \\(t_{n-p-1}\\) distribution. Obviously, we cannot have more predictors than \\(n-1\\) since we’d have 0 degrees of freedom, but even if we leave a few degrees of freedom behind, it can still impact our power to detect the results (i.e., \\(t\\)-distributions with fewer degrees of freedom have fatter tails). If we can reduce the number of categories, we have more degrees of freedom to use for either other predictors or to have a more normal-like distribution for reference.\n\nAgain, one of the most sensible ways to combine groups is to use the scientific/contextual motivation. We can also consider combining groups that we identify as not being significantly different from one another when making a pairwise comparison. HOWEVER:\n\nBy combining groups it will alter the estimates and comparisons with the other (remaining) groups that are affected by sample size. For example, \\(n=3\\) won’t really impact the mean estimate for a group with \\(n=1000\\).\nIf we have small sample sizes, we may be underpowered to detect a group difference and would combine groups just because “\\(p&gt;0.05\\)”."
  },
  {
    "objectID": "recitation/r47/index.html#cells-means-model-formulation-interpretation-and-use",
    "href": "recitation/r47/index.html#cells-means-model-formulation-interpretation-and-use",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Cells means model formulation, interpretation, and use",
    "text": "Cells means model formulation, interpretation, and use\nThe cell means model makes the connection between the mean levels within a group more explicit. In this approach we would exclude the intercept and include an indicator variable for each group:\n\n\nCode\ncell_means &lt;- lm(betaplas ~ smoke_factor - 1, data=carotenoids)\nsummary(cell_means)$coefficients\n\n\n                    Estimate Std. Error   t value     Pr(&gt;|t|)\nsmoke_factorNever   206.0510   14.48037 14.229679 9.311836e-36\nsmoke_factorFormer  193.4696   16.91922 11.434896 1.593885e-25\nsmoke_factorCurrent 121.3256   27.66911  4.384875 1.588507e-05\n\n\nIn the model above, we see that the “reference” category from our reference cell model is now included as one of the predictors. We can also note that all the estimated coefficients represent the means for each smoking group!\nThe fitted regression model would be \\[ \\hat{Y} = 206.0510 I_N + 193.4696 I_F + 121.3256 I_C  \\]\nThe cell means model makes a more direct connection to the one-way ANOVA we’ve also seen (since we’re only considering categorical variables here). However, we can also get the same estimates, as we saw before, with a reference cell model by combining our different estimated \\(\\hat{\\beta}\\)’s.\n\nAn Example of Using Cell Means\nIn practice, I’ve only recommend a cell means model one time. The investigator wanted to summarize the mean level of response to a hearing test across a combination of gerbil age (young vs. old) and sound setting (three levels) and compare if each response was equal to 0, where in this case the response of 0 indicated a null effect of the combination. The cell means model made this extremely intuitive since it combined the two factors and provided that exact hypothesis test from the \\(t\\)-test.\nFor example, if we look at our plasma concentration for those under 50 years old vs. over 50 years old with smoking status, we would have:\n\n\nCode\ncarotenoids$AK_age_gte50 &lt;- carotenoids$age &gt;= 50\n\n# Calculate means by groups\nsummaryBy( betaplas ~ AK_age_gte50*smoke_factor, data=carotenoids)\n\n\n  AK_age_gte50 smoke_factor betaplas.mean\n1        FALSE        Never      194.7229\n2        FALSE       Former      209.6349\n3        FALSE      Current      123.8387\n4         TRUE        Never      218.7568\n5         TRUE       Former      173.8846\n6         TRUE      Current      114.8333\n\n\nCode\n# Fit cell means model with all groups\ncoef(lm( betaplas ~ AK_age_gte50:smoke_factor - 1, data=carotenoids))\n\n\n  AK_age_gte50FALSE:smoke_factorNever    AK_age_gte50TRUE:smoke_factorNever \n                             194.7229                              218.7568 \n AK_age_gte50FALSE:smoke_factorFormer   AK_age_gte50TRUE:smoke_factorFormer \n                             209.6349                              173.8846 \nAK_age_gte50FALSE:smoke_factorCurrent  AK_age_gte50TRUE:smoke_factorCurrent \n                             123.8387                              114.8333"
  },
  {
    "objectID": "recitation/r49/index.html",
    "href": "recitation/r49/index.html",
    "title": "Degrees of Freedom with Interactions, Polynomials, and Categorical Variables",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r49/index.html#multiple-category-predictors",
    "href": "recitation/r49/index.html#multiple-category-predictors",
    "title": "Degrees of Freedom with Interactions, Polynomials, and Categorical Variables",
    "section": "Multiple Category Predictors",
    "text": "Multiple Category Predictors\nLet’s now place some context on the problem, where we have a categorical variable with 4 categories (A, B, C, and D). We’ll consider this as a reference cell model with A as our reference group:\n\\[ Y = \\beta_0 + \\beta_1 X_B + \\beta_2 X_C + \\beta_3 X_D + \\epsilon; \\text{ where } \\epsilon \\sim N(0,\\sigma^{2}_{e})  \\]\nNotice the only thing we’ve changed is the subscript for our \\(X\\)’s, which isn’t necessary but helps us keep track of what each thing means. Depending on preference, since these are indicators, we also could have written them as \\(I_B\\) instead of \\(X_B\\).\nWe see that in this model we still have \\(p=3\\) since there are 3 predictors in the model, even though they technically are all part of the same variable!\nSince we are estimating the \\(\\hat{\\beta}\\)’s for each predictor, it takes 1 degree of freedom for each one. Overall, we might consider the variable to actually use 3 total degrees of freedom for estimation, but for the sake of our statistical inference (i.e., \\(t\\)-test, \\(F\\)-test, etc.), estimating the \\(\\hat{\\beta}\\) for each predictor contributes 1 DF."
  },
  {
    "objectID": "recitation/r49/index.html#interaction-terms",
    "href": "recitation/r49/index.html#interaction-terms",
    "title": "Degrees of Freedom with Interactions, Polynomials, and Categorical Variables",
    "section": "Interaction Terms",
    "text": "Interaction Terms\nLet’s change our context to consider an interaction between two variables:\n\\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1)(X_2) + \\epsilon; \\text{ where } \\epsilon \\sim N(0,\\sigma^{2}_{e})  \\]\nI think this format is the most obvious that we have an interaction between \\(X_1\\) and \\(X_2\\), implying that as either one changes, its magnitude will vary based on the other.\nHowever, this format can lead to confusion about the number of predictors (i.e., we only have 2 “unique” variables that appear throughout the model). So it might be easier when thinking about degrees of freedom to write is in our original format:\n\\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\epsilon; \\text{ where } \\epsilon \\sim N(0,\\sigma^{2}_{e})  \\]\nand then define \\(X_3 = X_1 \\times X_2\\). In this model we see a more direct connection with \\(p=3\\) as our number of predictors…just like in our generic example above!\nThis is because we really are creating a new predictor with different information for \\(X_3\\):\n\nIf we had age and BMI, a 50 year old with 25 kg/m2 would then have \\(X_3=50 \\times 25 = 1250\\)\nIf we had age and sex (where female is reference), a 50 year old female would have \\(X_3 = 50 \\times 0 = 0\\) and a 50 year old male would have \\(X_3 = 50 \\times 1 = 50\\)\nIf we had diabetes status (where no diabetes is reference) and sex (where female is reference), a diabetic male would have \\(X_3 = 1 \\times 1 = 1\\), a diabetic female would have \\(X_3 = 1 \\times 0 = 0\\), a non-diabetic male would have \\(X_3 = 0 \\times 1 = 0\\), and a non-diabetic female would have \\(X_3 = 0 \\times 0 = 0\\)\n\nThese are interaction terms are obviously quite different, but each introduces new information and flexibility to our regression model in what is essentially a new predictor. Therefore, we can think of any interaction terms as new predictors for the sake of degrees of freedom calculations (even though they do drastically alter how we interpret our estimated \\(\\hat{\\beta}\\)’s)."
  },
  {
    "objectID": "recitation/r49/index.html#polynomial-terms",
    "href": "recitation/r49/index.html#polynomial-terms",
    "title": "Degrees of Freedom with Interactions, Polynomials, and Categorical Variables",
    "section": "Polynomial Terms",
    "text": "Polynomial Terms\nLet’s change our context a final time to consider a polynomial regression model. This idea is similar to the interaction terms:\n\\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1^2 + \\beta_3 X_1^3 + \\epsilon; \\text{ where } \\epsilon \\sim N(0,\\sigma^{2}_{e})  \\]\nHere we see that we have only one “unique” variable in our model as written. However, there are still 3 predictors (a linear, quadratic, and cubic term for \\(X_1\\)) included in the model (and therefore we need to estimate three separate \\(\\hat{\\beta}\\)’s for our predictors). We can see this be rewriting our model in equivalent ways:\n\\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 (X_1)(X_1) + \\beta_3 (X_1)(X_1)(X_1) + \\epsilon; \\text{ where } \\epsilon \\sim N(0,\\sigma^{2}_{e})  \\]\nor as\n\\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\epsilon; \\text{ where } \\epsilon \\sim N(0,\\sigma^{2}_{e})  \\]\nwhere \\(X_2 = X_1^2\\) and \\(X_3 = X_1^3\\).\nWe see the same idea as before, that we are essentially transforming our variables to contribute new information and introduce different flexibility to our models."
  },
  {
    "objectID": "recitation/r50/index.html",
    "href": "recitation/r50/index.html",
    "title": "Estimating Pure Error (Polynomials)",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nEstimating Pure Error\nWhen we fit a regression model, we know that two factors contribute to the inflation of our sums of square due to error (SSE):\n\nThe true variability in our outcome, \\(Y\\) (pure error)\nError due to fitting an incorrect/suboptimal model (lack of fit error)\n\nOftentimes, we are unable to partition our error into its respective components (pure vs. lack of fit). However, if we have replicates (i.e., more than 1 measure) at each measure of our predictor, we are able to determine what the effect of lack of fit error is and to potentially identify a better model (e.g., should I add higher order polynomial(s) to my model?).\nOne aspect not explicitly noted in the slides, you are able to apply this method to decomposing the error for data that does not have replicates for every level of \\(X\\). However, the approach is more powerful when we have an increasing number of values of \\(X\\) with replicates.\nTo illustrate this point, let’s consider the equally spaced dose example with polynomial regression from the Polynomial Regression lecture:\n\n\n\nDose\nObs 1\nObs 2\nObs 3\n\n\n\n\n1\n0.9\n0.9\n0.8\n\n\n2\n1.1\n1.1\n1.2\n\n\n3\n1.6\n1.6\n1.4\n\n\n4\n2.3\n2.1\n2.2\n\n\n5\n3.5\n3.4\n3.2\n\n\n6\n5\n4.5\n4.8\n\n\n7\n6.6\n6.7\n6.7\n\n\n8\n8.7\n8.6\n8.8\n\n\n\nLet’s create a data frame with this data (and bring back our ANOVA table function):\n\n\nCode\n# Code to recreate figure\nwtgain &lt;- data.frame( dose=rep(1:8, each=3), \n    wgtgain=c(0.9,0.9,0.8,1.1,1.1,1.2,1.6,1.6,1.4,2.3,2.1,2.2,3.5,3.4,3.2,5,4.5,4.8,6.6,6.7,6.7,8.7,8.6,8.8) )\n\nplot(x=wtgain$dose, y=wtgain$wgtgain, xlab='Dose', ylab='Weight Gain', cex.lab=1.5, cex.axis=1.5, cex=1.5, ylim=c(0,10), pch=4)\n\n\n\n\n\nCode\nlinreg_anova_func &lt;- function(mod, ndigits=2, p_ndigits=3, format='kable'){\n### Function to create an ANOVA table linear regression results from lm or glm\n# mod: an object with the fitted model results\n# ndigits: number of digits to round to for most values, default is 2\n# p_digits: number of digits to round the p-value to, default is 3\n# format: desired format output (default is kable):\n## \"kable\" for kable table\n## \"df\" for data frame as table\n\n  # extract outcome from the object produced by the glm or lm function\n  if( class(mod)[1] == 'glm' ){\n    y &lt;- mod$y\n  }\n  if( class(mod)[1] == 'lm' ){\n    y &lt;- mod$model[,1] # first column contains outcome data\n  }  \n  \n  ybar &lt;- mean(y)\n  yhat &lt;- predict(mod)\n  p &lt;- length(mod$coefficients)-1\n  n &lt;- length(y)\n\n  ssm &lt;- sum( (yhat-ybar)^2 )\n  sse &lt;- sum( (y-yhat)^2 )\n  sst &lt;- sum( (y-ybar)^2 )\n  \n  msm &lt;- ssm/p\n  mse &lt;- sse/(n-p-1)\n  \n  f_val &lt;- msm/mse\n  p_val &lt;- pf(f_val, df1=p, df2=n-p-1, lower.tail=FALSE)\n  \n  # Create an ANOVA table to summarize all our results:\n  p_digits &lt;- (10^(-p_ndigits))\n  p_val_tab &lt;- if(p_val&lt;p_digits){paste0('&lt;',p_digits)}else{round(p_val,p_ndigits)}\n  \n  anova_table &lt;- data.frame( 'Source' = c('Model','Error','Total'),\n                          'Sums of Squares' = c(round(ssm,ndigits), round(sse,ndigits), round(sst,ndigits)),\n                          'Degrees of Freedom' = c(p, n-p-1, n-1),\n                          'Mean Square' = c(round(msm,ndigits), round(mse,ndigits),''),\n                          'F Value' = c(round(f_val,ndigits),'',''),\n                          'p-value' = c(p_val_tab,'',''))\n\n  if( format == 'kable' ){  \n    library(kableExtra)\n    kbl(anova_table, col.names=c('Source','Sums of Squares','Degrees of Freedom','Mean Square','F-value','p-value'), align='lccccc', escape=F) %&gt;%\n      kable_styling(bootstrap_options = \"striped\", full_width = F, position = \"left\")\n  }else{\n    anova_table\n  }\n}\n\n\nFirst, let’s fit the simple linear regression model with just dose as a predictor and return its ANOVA table:\n\n\nCode\n# Fit linear model\nlm1 &lt;- lm( wgtgain ~ dose, data=wtgain )\nlinreg_anova_func(lm1)\n\n\n\n\n\nSource\nSums of Squares\nDegrees of Freedom\nMean Square\nF-value\np-value\n\n\n\n\nModel\n155.67\n1\n155.67\n238.27\n&lt;0.001\n\n\nError\n14.37\n22\n0.65\n\n\n\n\nTotal\n170.04\n23\n\n\n\n\n\n\n\n\n\n\nThe MSE estimate from our SLR model is for the overall estimate of \\(\\sigma^{2}_{Y|X}\\) (the true underlying variance of \\(Y|X\\)), and includes both the pure error and any error due to lack of fit.\nNow let’s fit a model that adds a quadratic term and also return the ANOVA table:\n\n\nCode\n# Fit quadratic model\nwtgain$dose2 &lt;- wtgain$dose^2 # create variable for dose^2\nlm2 &lt;- lm( wgtgain ~ dose + dose2, data=wtgain)\nlm2_alt &lt;- lm( wgtgain ~ dose + I(dose^2), data=wtgain) #equivalent coding\nlinreg_anova_func(lm2)\n\n\n\n\n\nSource\nSums of Squares\nDegrees of Freedom\nMean Square\nF-value\np-value\n\n\n\n\nModel\n169.70\n2\n84.85\n5247.78\n&lt;0.001\n\n\nError\n0.34\n21\n0.02\n\n\n\n\nTotal\n170.04\n23\n\n\n\n\n\n\n\n\n\n\nSimilar to our SLR, this model’s MSE is our overall estimate of \\(\\sigma^{2}_{Y|X}\\) and still includes both pure and lack of fit error.\nFinally, let’s fit a saturated model that we can use to isolate our estimate of the pure error contribution from the MSE of its output:\n\n\nCode\n# Fit saturated model\nlm_pure &lt;- lm(wgtgain ~ as.factor(dose), data=wtgain)\nlinreg_anova_func(lm_pure)\n\n\n\n\n\nSource\nSums of Squares\nDegrees of Freedom\nMean Square\nF-value\np-value\n\n\n\n\nModel\n169.78\n7\n24.25\n1492.57\n&lt;0.001\n\n\nError\n0.26\n16\n0.02\n\n\n\n\nTotal\n170.04\n23\n\n\n\n\n\n\n\n\n\n\nThe MSE from this model represents the pure error in our outcome \\(Y\\) after accounting for dose. We can see this because the saturated model can perfectly estimate the mean of each dose level for weight gain based on the given information:\n\n\nCode\ndoBy::summaryBy(wgtgain ~ as.factor(dose), data=wtgain)\n\n\n  dose wgtgain.mean\n1    1    0.8666667\n2    2    1.1333333\n3    3    1.5333333\n4    4    2.2000000\n5    5    3.3666667\n6    6    4.7666667\n7    7    6.6666667\n8    8    8.7000000\n\n\nCode\ncoef(lm_pure)\n\n\n     (Intercept) as.factor(dose)2 as.factor(dose)3 as.factor(dose)4 \n       0.8666667        0.2666667        0.6666667        1.3333333 \nas.factor(dose)5 as.factor(dose)6 as.factor(dose)7 as.factor(dose)8 \n       2.5000000        3.9000000        5.8000000        7.8333333 \n\n\nwhere adding the intercept to any other coefficient matches its mean weight gain.\nThis saturated model can be used to conduct a lack of fit \\(F\\)-test, a special version of the partial \\(F\\)-test. In cases where we are considering polynomial models, this is a great test because we can fit the saturated model that represents:\n\\[ Y = \\beta_0 + \\beta_{linear} X + \\beta_{quadratic} X^2 + \\beta_{cubic} X^3 + \\beta_{quartic} X^4 + \\beta_{quintic} X^5 + \\beta_{sextic} X^6 + \\beta_{septic} X^7 + \\epsilon; \\text{ where } \\epsilon \\sim N(0,\\sigma^{2}_{e}) \\]\nand then evaluate reduced models that contain only lower level polynomials to evaluate which is optimal. Nicely, we can use the anova function to implement this test as we do with any other partial \\(F\\)-test.\nLet’s start by determining if a model with only the linear predictor for dose is needed:\n\n\nCode\n# Test lack of fit for straight-line (linear) model\nanova(lm_pure, lm1, test='F')\n\n\nAnalysis of Variance Table\n\nModel 1: wgtgain ~ as.factor(dose)\nModel 2: wgtgain ~ dose\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     16  0.260                                  \n2     22 14.373 -6   -14.113 144.75 4.995e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe see this matches our slide output (19 of 25) with \\(F=144.75\\) and \\(p&lt;0.001\\). This indicates we would reject our null hypothesis that all higher order terms are equal to 0, therefore at least one higher order term would improve our model fit (i.e., \\(H_0\\colon \\beta_{quadratic} = \\beta_{cubic} = ... = \\beta_{septic} = 0\\) is rejected).\nWe could then evaluate if a model with a linear and quadratic predictor for dose improves the model fit:\n\n\nCode\n# Test lack of fit for second-order polynomial (i.e., quadratic) model\nanova(lm_pure, lm2, test='F')\n\n\nAnalysis of Variance Table\n\nModel 1: wgtgain ~ as.factor(dose)\nModel 2: wgtgain ~ dose + dose2\n  Res.Df     RSS Df Sum of Sq     F Pr(&gt;F)\n1     16 0.26000                          \n2     21 0.33954 -5 -0.079544 0.979 0.4602\n\n\nThis matches our slide output (20 of 25) as well, with \\(F=0.979\\) and \\(p=0.460\\). In this case, we would fail to reject the null hypothesis that removing the cubic through septic terms is beneficial above any beyond having the linear and quadratic predictors (i.e., \\(H_0\\colon \\beta_{cubic} = ... = \\beta_{septic} = 0\\) is not rejected).\nWhile our second-order model still has some lack of fit error, it is minimized to the extent that increasing the complexity by adding higher order terms is not significantly beneficial and this simpler model represents our “best fit”.\nFor closing, let’s visualize the three models to see their predicted fits:\n\n\nCode\n# fit saturated model so predict can be easily used:\nlm_sat &lt;- lm( wgtgain ~ dose + I(dose^2)+ I(dose^3)+ I(dose^4)+ I(dose^5)+ I(dose^6)+ I(dose^7), data=wtgain)\n\nx_val &lt;- seq(1,8,length.out=100)\nx1 &lt;- predict(lm1, newdata=data.frame(dose=x_val))\nx2 &lt;- predict(lm2_alt, newdata=data.frame(dose=x_val))  \nxp &lt;- predict(lm_sat, newdata=data.frame(dose=x_val))  \n\n\nplot(x=wtgain$dose, y=wtgain$wgtgain, xlab='Dose', ylab='Weight Gain', cex.lab=1.5, cex.axis=1.5, cex=1.5, ylim=c(0,10), pch=4)\nlines(x=x_val, y=x1, lwd=2)\nlines(x=x_val, y=x2, lwd=2, col='blue', lty=2)\nlines(x=x_val, y=xp, lwd=2, col='orangered2', lty=3)\nlegend('topleft', bty='n', col=c('black','blue','orange'), lwd=c(2,2,2), lty=c(1,2,3), legend=c('Linear','Quadratic','Saturated'))\n\n\n\n\n\nWe see that the saturated model doesn’t vary much from the quadratic model, hence we can go with something a little less complicated that still approximates our pure error estimate well."
  },
  {
    "objectID": "recitation/r52/index.html",
    "href": "recitation/r52/index.html",
    "title": "The Three Models for Mediation and Confounding",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r52/index.html#proportion-mediated-versus-operational-definition-of-confounding",
    "href": "recitation/r52/index.html#proportion-mediated-versus-operational-definition-of-confounding",
    "title": "The Three Models for Mediation and Confounding",
    "section": "Proportion Mediated versus Operational Definition of Confounding",
    "text": "Proportion Mediated versus Operational Definition of Confounding\nA further connection can be made to our formula for operational confounding:\n\n\\(\\frac{\\hat{\\beta}_{crude} - \\hat{\\beta}_{adj}}{\\hat{\\beta}_{crude}} \\times 100\\) (favored by biostatisticians)\n\\(\\frac{\\hat{\\beta}_{crude} - \\hat{\\beta}_{adj}}{\\hat{\\beta}_{adj}} \\times 100\\) (favored by epidemiologists)\n\nand the formula for the proportion mediated by the indirect effect:\n\n\\(\\frac{\\text{indirect effect}}{\\text{total effect}} = \\frac{\\hat{\\beta}_{crude} - \\hat{\\beta}_{adj}}{\\hat{\\beta}_{crude}}\\)\n\nWe see here that the biostatistician-favored operational confounding criterion and the proportion mediated are the same! Again, mathematically the idea is equivalent, but the context and conclusions are very different based the question we are seeking to answer (i.e., is the variable a confounder with respect to our primary explanatory variable \\(X\\) and outcome \\(Y\\)?; is the variable a mediator with respect to our primary explanatory variable \\(X\\) and outcome \\(Y\\)?)."
  },
  {
    "objectID": "recitation/r54/index.html",
    "href": "recitation/r54/index.html",
    "title": "Mediation Modeling Example with Bootstrap CI Estimate",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r54/index.html#three-fundamental-models-of-mediation",
    "href": "recitation/r54/index.html#three-fundamental-models-of-mediation",
    "title": "Mediation Modeling Example with Bootstrap CI Estimate",
    "section": "Three Fundamental Models of Mediation",
    "text": "Three Fundamental Models of Mediation\nFit the three fundamental models of mediation analysis and fill in the following DAG:\n\n\n\n\n\nSolution:\n\n\nCode\nlead &lt;- read.csv('../../.data/lead2.csv') # read in data\n\nex2_crude &lt;- lm(iq ~ expose, data=lead) # fit crude model\nex2_adjusted &lt;- lm(iq ~ expose + resdur, data=lead) # fit adjusted model\nex2_covariate &lt;- lm(resdur ~ expose, data=lead) # fit covariate model\n\ncoef(ex2_crude)\n\n\n(Intercept)      expose \n 102.705128   -7.770346 \n\n\nCode\ncoef(ex2_adjusted)\n\n\n(Intercept)      expose      resdur \n108.0445890  -7.6357785  -0.7993818 \n\n\nCode\ncoef(ex2_covariate)\n\n\n(Intercept)      expose \n  6.6794872   0.1683389"
  },
  {
    "objectID": "recitation/r54/index.html#proportion-mediated-by-duration-at-residence",
    "href": "recitation/r54/index.html#proportion-mediated-by-duration-at-residence",
    "title": "Mediation Modeling Example with Bootstrap CI Estimate",
    "section": "Proportion Mediated by Duration at Residence",
    "text": "Proportion Mediated by Duration at Residence\nWhat is the proportion/percent mediated by duration at residence?\nSolution:\nThe proportion mediated is \\(\\frac{\\text{indirect effect}}{\\text{total effect}} = \\frac{\\hat{\\beta}_{crude} - \\hat{\\beta}_{adj}}{\\hat{\\beta}_{crude}} = \\frac{-7.77 - (-7.64)}{-7.77} = 0.0167\\). This corresponds to the percent mediated of \\(0.0167 \\times 100 = 1.67\\%\\)."
  },
  {
    "objectID": "recitation/r54/index.html#confidence-interval-around-proportion-mediated-and-p-value",
    "href": "recitation/r54/index.html#confidence-interval-around-proportion-mediated-and-p-value",
    "title": "Mediation Modeling Example with Bootstrap CI Estimate",
    "section": "Confidence Interval Around Proportion Mediated and p-Value",
    "text": "Confidence Interval Around Proportion Mediated and p-Value\nWhat is the 95% CI and corresponding p-value for the proportion/percent mediated by duration at residence using the normal approximation to estimate the standard error (i.e., Sobel’s test)?\nSolution:\nWe first need to calculate the standard error for our indirect effect: \\[ SE(\\hat{\\beta}_{crude} - \\hat{\\beta}_{adj}) = \\sqrt{{\\hat{\\gamma}}_X^2\\left(SE\\left({\\hat{\\beta}}_M\\right)\\right)^2+{\\hat{\\beta}}_M^2\\left(SE\\left({\\hat{\\gamma}}_X\\right)\\right)^2}  \\]\nWe can find this information in our coefficient tables:\n\n\nCode\nsummary(ex2_adjusted)$coefficient\n\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 108.0445890  3.2034439 33.727636 2.225895e-63\nexpose       -7.6357785  2.8676179 -2.662760 8.804541e-03\nresdur       -0.7993818  0.4020877 -1.988078 4.906059e-02\n\n\nCode\nsummary(ex2_covariate)$coefficient\n\n\n             Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) 6.6794872  0.3931584 16.9893014 6.025641e-34\nexpose      0.1683389  0.6455049  0.2607864 7.946969e-01\n\n\n\\[ SE(\\hat{\\beta}_{crude} - \\hat{\\beta}_{adj}) = \\sqrt{(0.17)^2(0.40)^2 + (-0.80)^2 (0.65)^2} = \\sqrt{0.275} = 0.524 \\]\nOur 95% CI for the indirect effect is\n\\[ -0.13 \\pm 1.96 \\times 0.524 = (-1.16, 0.90)  \\]\nThis corresponds to a 95% CI for our proportion mediated of:\n\\[ \\left(\\frac{-1.16}{-7.77}, \\frac{0.90}{-7.77} \\right) = (-0.1158, 0.1493) \\] The 95% CI may suggest an inconsistent mediation model since our confidence interval includes negative values or indicates an insignificant result, which we can verify by calculating the p-value.\nOur p-value is calculated from referencing the standard normal distribution (per Sobel’s test):\n\\[ Z = \\frac{\\text{indirect effect}}{SE(\\hat{\\beta}_{crude} - \\hat{\\beta}_{adj})} = \\frac{-0.13}{0.524} = -0.248 \\]\nThis corresponds to a p-value of 2 * pnorm(-0.248)=0.804, which is not significant."
  },
  {
    "objectID": "recitation/r54/index.html#bootstrap-percentile-ci-for-proportion-mediated",
    "href": "recitation/r54/index.html#bootstrap-percentile-ci-for-proportion-mediated",
    "title": "Mediation Modeling Example with Bootstrap CI Estimate",
    "section": "Bootstrap Percentile CI for Proportion Mediated",
    "text": "Bootstrap Percentile CI for Proportion Mediated\nCalculate the 95% bootstrap percentile confidence interval for the proportion/percent mediated. How does it compare with your estimate from 2c?\nSolution:\nFor our bootstrap, we will resample from the overall sample and implement 1,000 bootstrap resamples:\n\n\nCode\n# set seed for reproducibility\nset.seed(1631)\n\n# implement bootstrap\nB &lt;- 1000\nboot &lt;- numeric(B)\n\nfor(i in 1:B){\n  boot_dat &lt;- lead[sample(1:nrow(lead), size=nrow(lead), replace=T),]\n  boot_crude &lt;- lm(iq ~ expose, data=boot_dat)\n  boot_adjusted &lt;- lm(iq ~ expose + resdur, data=boot_dat)\n  boot[i] &lt;- (boot_crude$coef['expose'] - boot_adjusted$coef['expose']) / boot_crude$coef['expose']\n}\n\n# 95% bootstrap percentile interval\nquantile(boot, c(0.025,0.975))\n\n\n      2.5%      97.5% \n-0.2419920  0.2177717 \n\n\nCode\n# accuracy to evaluate performance of bootstrap percentile interval\nbias_boot &lt;- 0.0167 - mean(boot)\nse_boot &lt;- sd(boot)\nabs(bias_boot)/se_boot\n\n\n[1] 0.02737862\n\n\nOur 95% bootstrap percentile confidence interval is (-0.24, 0.22) for the proportion mediated.This is wider than our estimated confidence interval based on the normal approximation for Sobel’s test, but still reflects our insignificant result since the interval includes 0 (which may also suggest an inconsistent mediation model). The accuracy of our bootstrap percentile interval is acceptable given the bias/SE is less than 0.10."
  },
  {
    "objectID": "recitation/r56/index.html",
    "href": "recitation/r56/index.html",
    "title": "Study Designs and RD, RR, and OR Terminology",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r56/index.html#case-control-studies",
    "href": "recitation/r56/index.html#case-control-studies",
    "title": "Study Designs and RD, RR, and OR Terminology",
    "section": "Case-Control Studies",
    "text": "Case-Control Studies\nWe cannot measure incidence in a case-control study since the design intentionally starts with cases and controls. Because of this, we have no way of knowing how many exposed individuals it would take to see the number of cases (since we selected the cases specifically because they had the condition). Similarly, we don’t know how many unexposed people it would take to observe the number of cases we selected. Sampling controls can then give us a reference group to compare to, but we will never know the total number exposed or not:\n\n\n\n\nCase\nControl\nTotal\n\n\n\n\nExposed\na\nb\nUnknown\n\n\nNon-Exposed\nc\nd\nUnknown"
  },
  {
    "objectID": "recitation/r56/index.html#cross-sectional-studies",
    "href": "recitation/r56/index.html#cross-sectional-studies",
    "title": "Study Designs and RD, RR, and OR Terminology",
    "section": "Cross-Sectional Studies",
    "text": "Cross-Sectional Studies\nYou may notice the asterisks in our RD, RR, and OR table above for the cross-sectional studies. While our slides noted we can calculate all of these summaries, we were being a bit cavalier since a cross-sectional study cannot measure incidence and can only measure the prevalence of an outcome. What we are really measuring is the, prevalence difference (instead of risk difference), prevalence ratio (instead of risk ratio), and prevalence odds ratio (instead of just odds ratio).\nThe reason we tend to still call these summaries the RD, RR, and OR is that they have the exact same formulas and approach to calculate them (e.g., those included in the slides, logistic regression for OR, log-binomial regression for RR, etc.). The important thing is that we correctly interpret the context (and likely I, and many others, should be a little more careful with our terminology).\nThere are some cases where the prevalence can be used to approximate the risk:\n\nAverage duration of disease is the same in groups (if curative)\nThe disease is rare (e.g., risk less than 5%)\nThe disease does not influence the presence of the exposure\n\nIn these contexts, we can imagine if we mapped out a hypothetical longitudinal study we could take cross-sectional looks at the data and have very similar estimates to a cohort study that does observe exposures and outcomes over time."
  },
  {
    "objectID": "recitation/r58/index.html",
    "href": "recitation/r58/index.html",
    "title": "Multiple Testing Adjusting in R with p.adjust",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords.\n\nMultiple Testing and p.adjust\nThere are a host of multiple comparison correction strategies:\n\nBonferroni: the simplest strategy since we just adjust our significance threshold by dividing by the number of tests (\\(C\\)): \\(\\frac{\\alpha}{C}\\)\nHolm-Bonferroni (or just Holm): a slightly more complex algorithm to the Bonferroni correction that is more powerful\nBenjamini-Hochberg False Discovery Rate: another algorithm that attempts to limit the number of false positive results to a reasonable level\n\nWhile it is possible to implement these algorithms on your own, it may be easiest to use an existing function, like p.adjust in R. One major piece of this function to be aware of is that it presents the adjusted p-values as the output. For methods that already calculate adjusted p-values (e.g., FDR), this will not change our results. However, for methods that adjust the significance threshold (e.g., Bonferroni), the results with p.adjust will be converted so that you can still use your original \\(\\alpha\\) threshold.\nFor example, let’s consider the simple case with 3 p-values: 0.01, 0.04, 0.60. If we interpret these unadjusted p-values with \\(\\alpha=0.05\\), we would conclude that the first two would indicate a significant result (i.e., we would reject whatever \\(H_0\\) we were testing).\nIf we apply the Bonferroni correction by hand, we’d use \\(\\frac{0.05}{3} = 0.0167\\) as our new threshold. Therefore, we’d only consider the first unadjusted p-value to be significant. The limitations here are that (1) a person seeing the results needs to know the number of tests considered for the correction to calculate the threshold and (2) the new threshold is likely some seemingly random number (i.e., not our still arbitrarily chosen 0.05).\nThese limitations are addressed if we calculate the adjusted p-values to present instead, where a reader could simply use the original \\(\\alpha\\) threshold to determine significance. In p.adjust, we simply multiply by the number of tests conducted, and set a maximum p-value of 1.0:\n\n\nCode\np_vec &lt;- c(0.01, 0.04,0.60)\np.adjust(p_vec, method='bonferroni')\n\n\n[1] 0.03 0.12 1.00\n\n\nHere we draw the same conclusion as our “by hand” calculation of the Bonferroni correction. However, we see that we might instead report \\(p_{adj}=0.03 &lt; 0.05\\). [Note, in practice, I mostly see these still reported as \\(p=0.03\\), but with a note in the methods section of the paper on how multiple testing was handled.]\nWe can see that for the FDR approach, we arrive at the same estimates. Our by-hand calculation, where \\(q = \\frac{kp}{\\text{rank}}\\) and the FDR is the minimum value for \\(q\\) at the given rank or higher, would look something like:\n\n\n\nTest\np-value\nRank\nq\nFDR\n\n\n\n\n1\n0.01\n1\n0.03\n0.03\n\n\n2\n0.04\n2\n0.06\n0.06\n\n\n3\n0.60\n3\n0.60\n0.60\n\n\n\nWhere we see the FDR adjusted p-values matches the output from p.adjust:\n\n\nCode\np.adjust(p_vec, method='fdr')\n\n\n[1] 0.03 0.06 0.60"
  },
  {
    "objectID": "recitation/r7/index.html",
    "href": "recitation/r7/index.html",
    "title": "What are some alternatives to p-values?",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r7/index.html#confidence-intervals",
    "href": "recitation/r7/index.html#confidence-intervals",
    "title": "What are some alternatives to p-values?",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nOne strategy that has been proposed and may be required by certain journals is to report findings as confidence intervals around your point estimate without the p-value. Some potential strengths/weaknesses:\n\nStrength: Summarizes the range of values taking into account sample size, variability, etc. Thus one can determine if the interval is wider than they may be comfortable with or if it includes effect sizes that they would be worried about (e.g., null, harmful, not strong enough, etc.)\nStrength: If we don’t report p-values, but only CIs, we do not have to worry about situations where the software has different assumptions for the p-value and confidence interval calculations. This can lead to cases where the CI and p-value do not agree and make for challenging interpretations (will potentially see this with quantile regression, more common with complex models that have various approximations in different packages).\nWeakness: Oftentimes we do have a direct connection between p-value and CI, so this doesn’t really address the use of p-values for “statistical significance” since we may be checking if our \\(H_0\\) value falls within our interval.\nWeakness: The actual interpretation of a CI is challenging (i.e., we are 95% confident that…).\nNote: If we use the brief, but complete, framework to writing our results we would have both the p-value and confidence interval to give a more full evaluation of the setting."
  },
  {
    "objectID": "recitation/r7/index.html#bayesian-alternatives",
    "href": "recitation/r7/index.html#bayesian-alternatives",
    "title": "What are some alternatives to p-values?",
    "section": "Bayesian Alternatives",
    "text": "Bayesian Alternatives\nAnother approach is to explore the Bayesian statistical framework. This includes summaries like Bayes factors, posterior probabilities, and credible intervals. We will discuss these in greater detail towards the end of the semester.\nWithin our paper repository, we have a nice paper by Steven Goodman on “Toward Evidence-Based Medical Statistics. 2: The Bayes Factor” where he walks through the concept and its potential use. Bayes factors compare two competing models based on the observed data (e.g., \\(H_0\\) versus a specific \\(H_1\\)). Some authors propose various thresholds/groupings to define the strength of the evidence (e.g., 1=no evidence in favor of \\(H_1\\), values &lt;1 show evidence in favor of \\(H_0\\), values &gt;1 show evidence in favor of \\(H_1\\)), but the specific threshold for “strong” evidence isn’t universally agreed upon. It is most similar to the frequentist likelihood ratio test.\nPosterior probabilities and credible intervals are most similar to p-values and confidence intervals, respectively. Some potential strengths/weaknesses:\n\nStrength: The PP and CrI have natural interpretations that represent what many think p-values or CIs are actually doing. The posterior probability is the probability of observing your specific hypothesis (e.g., \\(P(\\mu &gt; 0) = 0.92\\), the probability our mean is greater than 0 is 92%) and the X% CrI is we have X% probability that the true estimate lies within the interval.\nStrength: Strictly speaking, in a Bayesian philosophy you do not have to correct for multiple comparisons. However, in practice we still need to consider corrections if we wish to control the family-wise type I error rate because certain regulatory authorities (e.g., the FDA) expects all designs/approaches to maintain the “frequentist operating characteristics” (e.g., 80% power and 5% type I error rate).\nStrength: We don’t have to define a threshold for “significance”. Instead, one can leverage the PP and determine if the summary suggests sufficient evidence for their own comfort. However, we do need thresholds to calculate the frequentist operating characteristics.\nWeakness: Prior specification is challenging and seen as subjective. While true to some extent, best practice is to consider multiple priors and evaluate how/if the results change under different a priori assumptions."
  },
  {
    "objectID": "recitation/r9/index.html",
    "href": "recitation/r9/index.html",
    "title": "ROC Curves Under Different Strengths of Predictors",
    "section": "",
    "text": "This page is part of the University of Colorado-Anschutz Medical Campus’ BIOS 6618 Recitation collection. To view other questions, you can view the BIOS 6618 Recitation collection page or use the search bar to look for keywords."
  },
  {
    "objectID": "recitation/r9/index.html#a-perfect-predictor",
    "href": "recitation/r9/index.html#a-perfect-predictor",
    "title": "ROC Curves Under Different Strengths of Predictors",
    "section": "A Perfect Predictor",
    "text": "A Perfect Predictor\nA perfect predictor will classify all cases and controls without any inaccuracy and has an area under the curve (AUC) of 1. Let’s simulate data where this holds true and see what the ROC curve looks like:\n\n\nCode\nlibrary(pROC) # load package for ROC curve\nset.seed(511111) # set seed for reproducibility\n\nn_case &lt;- 100 # sample size for cases\nn_control &lt;- 100 # sample size for controls\n\n# simulate hypothetical predictor for cases and controls\n## this scenario has perfect discrimination (i.e., no overlap)\ncase &lt;- runif(n=n_case, min=0.501, max=1)\ncontrol &lt;- runif(n=n_control, min=0, max=0.5)\n\n# create data frame to use\nroc_dat1 &lt;- data.frame(pred=c(case,control), out=c( rep(1,n_case), rep(0,n_control)) )\n\n# plot ROC with AUC\nroc1 &lt;- roc(out ~ pred, data=roc_dat1, ci=T)\n\n\nWarning in ci.auc.roc(roc, ...): ci.auc() of a ROC curve with AUC == 1 is\nalways 1-1 and can be misleading.\n\n\nCode\nplot(roc1, print.auc=T, print.auc.x=0.7, print.auc.y=0.05)\n\n\n\n\n\nWe can see in the output that (1) it informs us what measure is a control/case, (2) the direction it is assuming for classification of cases and controls (i.e., controls &lt; cases here), and (3) that our estimate may be misleading because we appear to have perfect prediction.\nWe can also modify the code to request (select) thresholds are added with the value of (specificity, sensitivity) included:\n\n\nCode\n# add select thresholds\nplot(roc1, print.auc=T, print.auc.x=0.7, print.auc.y=0.05, print.thres=c(0.1,0.5,0.7,0.9), print.thres.cex=0.7) # note, print.thres has many options; cex controls size\n\n\n\n\n\nFrom this figure we can see that the “optimal” threshold is likely 0.5 (i.e., predict controls if &lt;0.5) since it has 100% specificity and sensitivity.\nWe can also note if we did choose a threshold of 0.7, we’d have 100% specificity (i.e., all controls correctly predicted), but our sensitivity drops to 57% (i.e., only 57% of cases correctly predicted)."
  },
  {
    "objectID": "recitation/r9/index.html#a-decent-predictor",
    "href": "recitation/r9/index.html#a-decent-predictor",
    "title": "ROC Curves Under Different Strengths of Predictors",
    "section": "A Decent Predictor",
    "text": "A Decent Predictor\nLet’s simulate data where we have a decent predictor but it isn’t perfect and evaluate the “best” threshold using Youden’s J-index (\\(max(se+sp)\\)) and the closest top-left (\\(min((1-se)^2+(1-sp)^2)\\)) methods:\n\n\nCode\nset.seed(511111) # set seed for reproducibility\n\nn_case &lt;- 100 # sample size for cases\nn_control &lt;- 100 # sample size for controls\n\n# simulate hypothetical predictor for cases and controls\n## this scenario is better than a coin flip but not perfect\ncase &lt;- rbeta(n=n_case, shape1=2, shape2=1)\ncontrol &lt;- rbeta(n=n_control, shape1=1, shape2=2)\n\n# create data frame to use\nroc_dat2 &lt;- data.frame(pred=c(case,control), out=c( rep(1,n_case), rep(0,n_control)) )\n\n# plot ROC with AUC\nroc2 &lt;- roc(out ~ pred, data=roc_dat2, ci=T)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\nCode\nplot(roc2, print.auc=T, print.auc.x=0.7, print.auc.y=0.05, print.thres=seq(0.1,0.9,0.1), print.thres.cex=0.7)\n\n\n\n\n\nCode\ncoords(roc2, x='best', best.method='youden')\n\n\n  threshold specificity sensitivity\n1 0.5017609        0.84        0.74\n\n\nCode\ncoords(roc2, x='best', best.method='closest.topleft')\n\n\n  threshold specificity sensitivity\n1 0.5017609        0.84        0.74\n\n\nThe “best” threshold is controls &lt; 0.5017609, which is in agreement for both approaches. We can see from our own figure that the 0.5 threshold has very similar sensitivity and specificity.\nThere are two interpretations of the AUC that are both correct:\n\nAn AUC of 0.855 &gt; 0.50, suggesting the score as a predictive tool is better than random chance (with the specific trade-off in sensitivity and specificity depending on the chosen threshold). Additionally, the 95% CI excludes 0.5, suggesting the AUC is significantly different from random chance.\nFor any randomly selected pair of one case and one control, the probability that the case is scored “more severely” is 0.855. Additionally, the 95% CI excludes 0.5, suggesting the probability is 0.804 at the lower end of the interval.\n\nWe can verify the second interpretation by randomly sampling a bunch of cases and controls and summarizing how often cases are greater than controls:\n\n\nCode\nset.seed(9182)\n\ncounter &lt;- 0\nnum_samps &lt;- 100000\n\nfor(i in 1:num_samps){\n  # randomly sample one case from data frame, one control\n  ## note, code written to be generalizable to other n_case, n_control\n  sample_case &lt;- sample(x=1:n_case, size=1, replace=F) # 1:100 rows in data frame with n_case=100\n  sample_control &lt;- sample(x=(n_case+1):(n_case+n_control), size=1, replace=F) # 101:200 rows in data frame with n_case=n_control=100\n\n  if( roc_dat2$pred[sample_control] &lt; roc_dat2$pred[sample_case] ){ counter &lt;- counter + 1 } # if control &lt; case, add 1 to counter\n}\n\ncounter/num_samps\n\n\n[1] 0.85707\n\n\nWe see that with 100,000 random pairs of cases/controls, 85.707% of them show a case with a higher score. This helps to confirm our 2nd definition of AUC from above."
  },
  {
    "objectID": "recitation/r9/index.html#a-less-decent-predictor",
    "href": "recitation/r9/index.html#a-less-decent-predictor",
    "title": "ROC Curves Under Different Strengths of Predictors",
    "section": "A Less Decent Predictor",
    "text": "A Less Decent Predictor\nLet’s simulate data with a less ideal predictor (i.e., closer to AUC of 0.5), where the cases are uniformly distributed from 0 to 1, but the controls follow a Beta(2,4) distribution (i.e., more concentrated towards lower values, but can span the range):\n\n\nCode\nset.seed(511111) # set seed for reproducibility\n\nn_case &lt;- 100 # sample size for cases\nn_control &lt;- 100 # sample size for controls\n\n# simulate hypothetical predictor for cases and controls\n## this scenario is better than a coin flip but not perfect\ncase &lt;- runif(n=n_case, min=0, max=1)\ncontrol &lt;- rbeta(n=n_control, shape1=2, shape2=4)\n\n# create data frame to use\nroc_dat3 &lt;- data.frame(pred=c(case,control), out=c( rep(1,n_case), rep(0,n_control)) )\n\n# plot ROC with AUC\nroc3 &lt;- roc(out ~ pred, data=roc_dat3, ci=T)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\nCode\nplot(roc3, print.auc=T, print.auc.x=0.7, print.auc.y=0.05, print.thres=seq(0.1,0.9,0.1), print.thres.cex=0.7)\n\n\n\n\n\nCode\ncoords(roc3, x='best', best.method='youden')\n\n\n  threshold specificity sensitivity\n1 0.4522853        0.77        0.51\n\n\nCode\ncoords(roc3, x='best', best.method='closest.topleft')\n\n\n  threshold specificity sensitivity\n1 0.4351246        0.72        0.54\n\n\nThe AUC here is 0.618, which is still significant although the lower limit of our CI goes toward 0.5. We can also note that the Youden’s J-index and closest top-left, while similar, no longer recommend the same threshold.\nThis raises the question of what we might want to do with this predictor. Here, we can see that it generally does better at higher values of specificity (i.e., true negatives) and doesn’t have stellar sensitivity (i.e., true positives). We could:\n\nChoose one of the “best” thresholds considering it balances sensitivity and specificity.\nChoose a threshold that favors one property at the expense of the other. For example, thresholds &lt;0.7 has 100% specificity, but only 21% sensitivity. This may be useful in practice (i.e., the utility of the test) depending on the prevalence and/or if we are using this test in conjunction with other test results."
  },
  {
    "objectID": "recitation/r9/index.html#little-to-no-effect-predictor",
    "href": "recitation/r9/index.html#little-to-no-effect-predictor",
    "title": "ROC Curves Under Different Strengths of Predictors",
    "section": "Little-to-No Effect Predictor",
    "text": "Little-to-No Effect Predictor\nLet’s simulate data where there isn’t really much of an effect:\n\n\nCode\nset.seed(511111) # set seed for reproducibility\n\nn_case &lt;- 100 # sample size for cases\nn_control &lt;- 100 # sample size for controls\n\n# simulate hypothetical predictor for cases and controls\n## this scenario is better than a coin flip but not perfect\ncase &lt;- rbeta(n=n_control, shape1=1.05, shape2=1)\ncontrol &lt;- rbeta(n=n_control, shape1=1, shape2=1.05)\n\n# create data frame to use\nroc_dat4 &lt;- data.frame(pred=c(case,control), out=c( rep(1,n_case), rep(0,n_control)) )\n\n# plot ROC with AUC\nroc4 &lt;- roc(out ~ pred, data=roc_dat4, ci=T)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\nCode\nplot(roc4, print.auc=T, print.auc.x=0.7, print.auc.y=0.05, print.thres=seq(0.1,0.9,0.1), print.thres.cex=0.7)\n\n\n\n\n\nCode\ncoords(roc4, x='best', best.method='youden')\n\n\n  threshold specificity sensitivity\n1 0.4490652        0.48        0.65\n\n\nCode\ncoords(roc4, x='best', best.method='closest.topleft')\n\n\n  threshold specificity sensitivity\n1 0.4490652        0.48        0.65\n\n\nIn our final example, we see that the AUC is near 0.5 and its 95% CI includes the null of 0.5. Therefore, we’d likely conclude that there is little use of this predictor overall. However, we might convince ourselves a given threshold would be useful in a broader context (e.g., different prevalence, multiple tests, etc.)."
  },
  {
    "objectID": "labs/prac13/index.html#a-comparing-simple-linear-regression-and-quantile-regression",
    "href": "labs/prac13/index.html#a-comparing-simple-linear-regression-and-quantile-regression",
    "title": "Week 13 Practice Problems",
    "section": "1a: Comparing Simple Linear Regression and Quantile Regression",
    "text": "1a: Comparing Simple Linear Regression and Quantile Regression\nFit and compare the simple linear regression model and quantile regression model for the outcome of infant birth weight in grams (i.e., dependent variable) and independent variable for mother’s age in years. State the interpretations of both models for the intercept and mother’s age, then compare the estimated coefficients and statistical significance for the two approaches."
  },
  {
    "objectID": "labs/prac13/index.html#b-comparing-multiple-linear-regression-and-quantile-regression",
    "href": "labs/prac13/index.html#b-comparing-multiple-linear-regression-and-quantile-regression",
    "title": "Week 13 Practice Problems",
    "section": "1b: Comparing Multiple Linear Regression and Quantile Regression",
    "text": "1b: Comparing Multiple Linear Regression and Quantile Regression\nNow add the predictors for prior cesarean delivery, gestational hypertension, and month of birth (as a categorical variable) to the models from 1a. Provide an interpretation of the beta coefficients for mother’s age from both models, then briefly compare the beta coefficients between the linear and quantile regression models."
  },
  {
    "objectID": "labs/prac13/index.html#c-quantile-regression-for-other-quantiles",
    "href": "labs/prac13/index.html#c-quantile-regression-for-other-quantiles",
    "title": "Week 13 Practice Problems",
    "section": "1c: Quantile Regression for Other Quantiles",
    "text": "1c: Quantile Regression for Other Quantiles\nFor the simple quantile regression in 1a, expand to include estimates for the 5th to the 95th quantile in increments of 5 and then create a scatterplot to demonstrate the model fits for each quantile. Briefly describe the figure."
  },
  {
    "objectID": "labs/prac13/index.html#d-quantile-regression-for-other-quantiles-with-splines",
    "href": "labs/prac13/index.html#d-quantile-regression-for-other-quantiles-with-splines",
    "title": "Week 13 Practice Problems",
    "section": "1d: Quantile Regression for Other Quantiles with Splines",
    "text": "1d: Quantile Regression for Other Quantiles with Splines\nTake your approach in 1c and now add some form of flexible spline to measure if there are any nonlinear effects. Create a new scatterplot with this information and provide a brief discussion if it seems like the splines may be a meaningful addition or if a simpler model for any/all quantiles may be recommended."
  },
  {
    "objectID": "labs/prac13/index.html#e-quantile-regression-with-an-interaction",
    "href": "labs/prac13/index.html#e-quantile-regression-with-an-interaction",
    "title": "Week 13 Practice Problems",
    "section": "1e: Quantile Regression with an Interaction",
    "text": "1e: Quantile Regression with an Interaction\nFrom the results in 1b, we noticed what may be an interesting association between infant birth weight and presence of gestational hypertension. For the quantile regression of the median, fit a model that includes mother’s age, gestational hypertension, and the interaction of the two. Create a scatterplot with the predicted birth weight’s by mother’s age and gestational hypertension status. From your figure, does there appear to be a significant difference? From the results of the regression model, does there appear to be a statistically significant difference?"
  },
  {
    "objectID": "labs/prac13/index.html#a-segmented-regression-with-1-breakpoint",
    "href": "labs/prac13/index.html#a-segmented-regression-with-1-breakpoint",
    "title": "Week 13 Practice Problems",
    "section": "2a: Segmented Regression with 1 Breakpoint",
    "text": "2a: Segmented Regression with 1 Breakpoint\nFit a segmented regression with 1 breakpoint and plot the fitted segmented regression line on a scatterplot of the observed data. What is the estimated breakpoint? Does it visually seem like there may be a meaningful change in slope where the breakpoint is plotted?"
  },
  {
    "objectID": "labs/prac13/index.html#b-significance-test-for-breakpoints",
    "href": "labs/prac13/index.html#b-significance-test-for-breakpoints",
    "title": "Week 13 Practice Problems",
    "section": "2b: Significance Test for Breakpoints",
    "text": "2b: Significance Test for Breakpoints\nUsing Davies’ test, determine if there is a significant change in slope. State your null and alternative hypothesis."
  },
  {
    "objectID": "labs/prac13/index.html#c-significance-testing-of-slopes",
    "href": "labs/prac13/index.html#c-significance-testing-of-slopes",
    "title": "Week 13 Practice Problems",
    "section": "2c: Significance Testing of Slopes",
    "text": "2c: Significance Testing of Slopes\nAre the slopes both significantly different from 0 before and after the breakpoint?"
  },
  {
    "objectID": "labs/prac13s/index.html#a-comparing-simple-linear-regression-and-quantile-regression",
    "href": "labs/prac13s/index.html#a-comparing-simple-linear-regression-and-quantile-regression",
    "title": "Week 13 Practice Problems: Solutions",
    "section": "1a: Comparing Simple Linear Regression and Quantile Regression",
    "text": "1a: Comparing Simple Linear Regression and Quantile Regression\nFit and compare the simple linear regression model and quantile regression model for the outcome of infant birth weight in grams (i.e., dependent variable) and independent variable for mother’s age in years. State the interpretations of both models for the intercept and mother’s age, then compare the estimated coefficients and statistical significance for the two approaches.\nSolution:\nThe simple linear regression model is\n\n\nCode\n# Fit with glm function\nglm1 &lt;- glm(dbwt ~ mager, data=dat)\nsummary(glm1)\n\n\n\nCall:\nglm(formula = dbwt ~ mager, data = dat)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3037.136     57.841  52.508  &lt; 2e-16 ***\nmager          8.641      1.955   4.421 1.03e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 311196.5)\n\n    Null deviance: 783450552  on 2499  degrees of freedom\nResidual deviance: 777368935  on 2498  degrees of freedom\nAIC: 38719\n\nNumber of Fisher Scoring iterations: 2\n\n\nThe simple quantile regression of the median model is\n\n\nCode\n# Fit with rq from the quantreg package\nlibrary(quantreg)\nqr1 &lt;- rq(dbwt ~ mager, data=dat, tau=c(0.5)) # note, default is to model the 50th percentile (i.e., median) but we include tau parameter for completeness\nsummary(qr1)\n\n\n\nCall: rq(formula = dbwt ~ mager, tau = c(0.5), data = dat)\n\ntau: [1] 0.5\n\nCoefficients:\n            Value      Std. Error t value    Pr(&gt;|t|)  \n(Intercept) 3097.50000   59.10962   52.40264    0.00000\nmager          7.50000    1.99064    3.76762    0.00017\n\n\nOur intercept interpretations are:\n\nFor linear regression, the average birth weight in grams is 3037.136 for a mother who is 0 years old.\nFor quantile regression, the median birth weight in grams is 3097.5 for a mother who is 0 years old.\nNote, the intercept doesn’t have a meaningful interpretation because it doesn’t make scientific sense for a mother to be 0 years old themselves and, even if it did, it would represent extrapolation from our data.\n\nOur slope interpretations for mother’s age are:\n\nFor linear regression, a one year increase in mother’s age leads to an average increase of 8.6 grams in the baby’s birth weight.\nFor quantile regression, a one year increase in mother’s age leads to an increase of 7.5 grams for the 50th percentile (i.e., median) birth weight.\n\nFor the most part, these coefficients are not drastically different and they are both statistically significant (p&lt;0.001 for both). This may suggest that the mean and median have similar trajectories across mother’s age when assuming a linear trend."
  },
  {
    "objectID": "labs/prac13s/index.html#b-comparing-multiple-linear-regression-and-quantile-regression",
    "href": "labs/prac13s/index.html#b-comparing-multiple-linear-regression-and-quantile-regression",
    "title": "Week 13 Practice Problems: Solutions",
    "section": "1b: Comparing Multiple Linear Regression and Quantile Regression",
    "text": "1b: Comparing Multiple Linear Regression and Quantile Regression\nNow add the predictors for prior cesarean delivery, gestational hypertension, and month of birth (as a categorical variable) to the models from 1a. Provide an interpretation of the beta coefficients for mother’s age from both models, then briefly compare the beta coefficients between the linear and quantile regression models.\nSolution:\n\n\nCode\n# Multiple linear regression\nglm2 &lt;- glm(dbwt ~ mager + rf_cesar + rf_ghype + as.factor(dob_mm), data=dat)\nsummary(glm2)\n\n\n\nCall:\nglm(formula = dbwt ~ mager + rf_cesar + rf_ghype + as.factor(dob_mm), \n    data = dat)\n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         3000.826     72.167  41.582  &lt; 2e-16 ***\nmager                  8.545      1.966   4.345 1.45e-05 ***\nrf_cesarY             26.123     30.229   0.864   0.3876    \nrf_ghypeY           -259.607     45.675  -5.684 1.47e-08 ***\nas.factor(dob_mm)2    -7.222     59.951  -0.120   0.9041    \nas.factor(dob_mm)3    70.440     56.992   1.236   0.2166    \nas.factor(dob_mm)4    68.558     58.141   1.179   0.2384    \nas.factor(dob_mm)5    72.556     56.815   1.277   0.2017    \nas.factor(dob_mm)6    65.468     56.648   1.156   0.2479    \nas.factor(dob_mm)7    95.284     57.351   1.661   0.0968 .  \nas.factor(dob_mm)8    34.017     56.953   0.597   0.5504    \nas.factor(dob_mm)9     5.302     56.840   0.093   0.9257    \nas.factor(dob_mm)10  108.976     60.754   1.794   0.0730 .  \nas.factor(dob_mm)11   17.148     57.610   0.298   0.7660    \nas.factor(dob_mm)12   73.305     56.602   1.295   0.1954    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 307537.2)\n\n    Null deviance: 783450552  on 2499  degrees of freedom\nResidual deviance: 764229872  on 2485  degrees of freedom\nAIC: 38703\n\nNumber of Fisher Scoring iterations: 2\n\n\nCode\n# Multiple quantile regression\nqr2 &lt;- rq(dbwt ~ mager + rf_cesar + rf_ghype + as.factor(dob_mm), data=dat, tau=c(0.5)) # note, default is to model the 50th percentile (i.e., median) but we include tau parameter for completeness\n\n\nWarning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique\n\n\nCode\nsummary(qr2)\n\n\n\nCall: rq(formula = dbwt ~ mager + rf_cesar + rf_ghype + as.factor(dob_mm), \n    tau = c(0.5), data = dat)\n\ntau: [1] 0.5\n\nCoefficients:\n                    Value      Std. Error t value    Pr(&gt;|t|)  \n(Intercept)         3124.16667   91.35139   34.19944    0.00000\nmager                  7.83333    1.99683    3.92289    0.00009\nrf_cesarY              7.16667   30.22528    0.23711    0.81259\nrf_ghypeY           -266.16667   53.07499   -5.01492    0.00000\nas.factor(dob_mm)2   -91.33333   86.48415   -1.05607    0.29104\nas.factor(dob_mm)3   -27.33333   75.57091   -0.36169    0.71761\nas.factor(dob_mm)4   -45.16667   78.09007   -0.57839    0.56305\nas.factor(dob_mm)5   -35.66667   79.23438   -0.45014    0.65265\nas.factor(dob_mm)6   -17.50000   77.58002   -0.22557    0.82155\nas.factor(dob_mm)7   -42.16667   84.30264   -0.50018    0.61699\nas.factor(dob_mm)8   -33.66667   76.57305   -0.43967    0.66022\nas.factor(dob_mm)9   -58.50000   75.42326   -0.77562    0.43805\nas.factor(dob_mm)10   43.00000   89.69914    0.47938    0.63171\nas.factor(dob_mm)11  -35.16667   77.67001   -0.45277    0.65075\nas.factor(dob_mm)12   58.50000   78.68332    0.74349    0.45726\n\n\nNote, you may receive a message “Warning: Solution may be nonunique”. You can learn more through the FAQ() of the quantreg package, but it is likely not too concerning in our case.\nWith regards to our interpretation, our slope parameters for mother’s age are:\n\nFor linear regression, a one year increase in mother’s age leads to an average increase of 8.5 grams in the baby’s birth weight after adjusting for previous cesarean, gestational hypertension, and month of the birth.\nFor quantile regression, a one year increase in mother’s age leads to an increase of 7.8 grams for the 50th percentile (i.e., median) birth weight after adjusting for previous cesarean, gestational hypertension, and month of the birth.\n\nWith regards to our other coefficients, we see some more substantial differences between the estimated effect of a coefficient on the mean of the outcome versus the median of the outcome. For example, a previous cesarean resulted in an average increase of 26 grams, after adjusting for all other terms. The increase in the median is lower at only 7 grams after adjusting for all other terms. This may indicate that we could have outliers affecting our mean estimate within certain categories or potentially that the mean and median have greater differences. When considering month-specific effects, we see the quantile regression has many negative coefficients with respect to the reference month of January, whereas the linear regression generally has higher mean birth weights relative to January."
  },
  {
    "objectID": "labs/prac13s/index.html#c-quantile-regression-for-other-quantiles",
    "href": "labs/prac13s/index.html#c-quantile-regression-for-other-quantiles",
    "title": "Week 13 Practice Problems: Solutions",
    "section": "1c: Quantile Regression for Other Quantiles",
    "text": "1c: Quantile Regression for Other Quantiles\nFor the simple quantile regression in 1a, expand to include estimates for the 5th to the 95th quantile in increments of 5 and then create a scatterplot to demonstrate the model fits for each quantile. Briefly describe the figure.\nSolution:\n\n\nCode\n# Fit quantile regression over all requested quantiles\nqr3 &lt;- rq(dbwt ~ mager, data=dat, tau=seq(0.05,0.95,by=0.05))\n\n# Create plot\npar( mar=c(5.1,4.1,4.1,4.1))\nplot(x=dat$mager, y=dat$dbwt, xlab=\"Mother's Age (Years)\", ylab='Infant Birth Weight (grams)', col='gray65')\n\nquant_col &lt;- rainbow(19) # create grid of 19 colors to plot lines for\nfor( i in 1:19 ){ abline(a=coef(qr3)[1,i], b=coef(qr3)[2,i], col=quant_col[i]) }\nlegend('right', xpd=T, inset=-0.1, col=rev(quant_col), legend=seq(95,5,-5), cex=0.7, bty='n', lty=rep(1,19))\n\n\n\n\n\nWe see that all modeled quantiles show increasing birth weights across the span of mother’s age at birth, but the slopes are not consistent. For example, the 10th percentile nearly combines with the 15th percentile at older ages. Likewise, some younger ages seem to converge before getting older and spreading out somewhat. It is also worth noting that with 2500 points, it is challenging to get a good sense of the density at a given age/weight combination since the points are mostly overlapping."
  },
  {
    "objectID": "labs/prac13s/index.html#d-quantile-regression-for-other-quantiles-with-splines",
    "href": "labs/prac13s/index.html#d-quantile-regression-for-other-quantiles-with-splines",
    "title": "Week 13 Practice Problems: Solutions",
    "section": "1d: Quantile Regression for Other Quantiles with Splines",
    "text": "1d: Quantile Regression for Other Quantiles with Splines\nTake your approach in 1c and now add some form of flexible spline to measure if there are any nonlinear effects. Create a new scatterplot with this information and provide a brief discussion if it seems like the splines may be a meaningful addition or if a simpler model for any/all quantiles may be recommended.\nSolution:\nHere we’ll use a B-spline for mother’s age. We’ll then predict the outcomes over a grid of ages to create nonlinear lines to plot:\n\n\nCode\nlibrary(splines)\n\n# Fit quantile regression over all requested quantiles\nqr4 &lt;- rq(dbwt ~ bs(mager), data=dat, tau=seq(0.05,0.95,by=0.05))\n\n# Create predictions\nnewage &lt;- data.frame('mager' = seq(14,49,length.out=100))\npred_y &lt;- predict(qr4, newdata=newage)\n\n# Create plot\npar( mar=c(5.1,4.1,4.1,4.1))\nplot(x=dat$mager, y=dat$dbwt, xlab=\"Mother's Age (Years)\", ylab='Infant Birth Weight (grams)', col='gray65')\n\nquant_col &lt;- rainbow(19) # create grid of 19 colors to plot lines for\nfor( i in 1:19 ){ lines(x=newage$mager, y=pred_y[,i], col=quant_col[i]) }\nlegend('right', xpd=T, inset=-0.1, col=rev(quant_col), legend=seq(95,5,-5), cex=0.7, bty='n', lty=rep(1,19))\n\n\n\n\n\nThere appears to be some potentially concerning bheavior near the tails of the distribution for mother’s age where different quantiles are overlapping (sometimes by multiple magnitudes of 5% increments). The general trend for many of the quantiles, especially between about 20-40 years of age where most of the data is, is largely linear. This suggests we may be fine with our more parsimonious quantile regression that only included a single predictor for mother’s age. It is possible that some of the more extreme quantiles and/or more extreme ages may benefit from a non-linear trend provided by the B-spline, but this may also be artificially present given the sparsity of the data at this points (e.g., the 5th percentile slopes strongly downward due to some lower birth weight’s at around 42 years of age and none later)."
  },
  {
    "objectID": "labs/prac13s/index.html#e-quantile-regression-with-an-interaction",
    "href": "labs/prac13s/index.html#e-quantile-regression-with-an-interaction",
    "title": "Week 13 Practice Problems: Solutions",
    "section": "1e: Quantile Regression with an Interaction",
    "text": "1e: Quantile Regression with an Interaction\nFrom the results in 1b, we noticed what may be an interesting association between infant birth weight and presence of gestational hypertension. For the quantile regression of the median, fit a model that includes mother’s age, gestational hypertension, and the interaction of the two. Create a scatterplot with the predicted birth weight’s by mother’s age and gestational hypertension status. From your figure, does there appear to be a significant difference? From the results of the regression model, does there appear to be a statistically significant difference?\nSolution:\n\n\nCode\nqr5 &lt;- rq(dbwt ~ mager * rf_ghype, data=dat, tau=c(0.5))\nsummary(qr5)\n\n\n\nCall: rq(formula = dbwt ~ mager * rf_ghype, tau = c(0.5), data = dat)\n\ntau: [1] 0.5\n\nCoefficients:\n                Value      Std. Error t value    Pr(&gt;|t|)  \n(Intercept)     3097.22222   60.09310   51.54040    0.00000\nmager              7.88889    1.98361    3.97705    0.00007\nrf_ghypeY       -367.05556  197.70765   -1.85656    0.06349\nmager:rf_ghypeY    3.40278    5.30995    0.64083    0.52169\n\n\nCode\n# Create predictions\nnewage &lt;- data.frame('mager' = c(seq(14,49,length.out=100),seq(14,49,length.out=100)),\n                     'rf_ghype' = c(rep('N',100), rep('Y',100)) ) # data frame needs variables for both age and hypertension\npred_y &lt;- predict(qr5, newdata=newage)\n\n# Create plot\npar( mar=c(5.1,4.1,4.1,4.1))\nplot(x=dat$mager, y=dat$dbwt, xlab=\"Mother's Age (Years)\", ylab='Infant Birth Weight (grams)', col='gray65')\nlegend('top', xpd=T, inset=-0.13, lty=c(1,2), lwd=c(2,2), col=c('black','blue'), legend=c('No Hypertension','Hypertension'), bty='n', horiz=T)\nlines(x=newage$mager[1:100], y=pred_y[1:100], col='black', lwd=2)\nlines(x=newage$mager[101:200], y=pred_y[101:200], col='blue', lwd=2, lty=2)\n\n\n\n\n\nFrom our figure, it appears that the trend of the median birth weight is very similar between mothers with and without gestational hypertension, with a slightly narrowing difference towards higher ages. However, \\(p=0.522\\) from our quantile regression, indicating we fail to reject the null hypothesis and there is not sufficient evidence to reject that there isn’t an interaction (i.e., that the beta coefficient could truly be 0). We could likely use a more parsimonious model without the interaction and it suggests that hypertension may have a largely similar effect across the age span."
  },
  {
    "objectID": "labs/prac13s/index.html#a-segmented-regression-with-1-breakpoint",
    "href": "labs/prac13s/index.html#a-segmented-regression-with-1-breakpoint",
    "title": "Week 13 Practice Problems: Solutions",
    "section": "2a: Segmented Regression with 1 Breakpoint",
    "text": "2a: Segmented Regression with 1 Breakpoint\nFit a segmented regression with 1 breakpoint and plot the fitted segmented regression line on a scatterplot of the observed data. What is the estimated breakpoint? Does it visually seem like there may be a meaningful change in slope where the breakpoint is plotted?\nSolution:\nWe will use the segmented package:\n\n\nCode\n# Fit our segmented regression model\nlibrary(segmented)\nmod2 &lt;- glm(dbwt ~ mager, data=dat)\nseg2 &lt;- segmented(mod2)\nsummary(seg2)\n\n\n\n    ***Regression Model with Segmented Relationship(s)***\n\nCall: \nsegmented.glm(obj = mod2)\n\nEstimated Break-Point(s):\n            Est. St.Err\npsi1.mager   28  2.056\n\nCoefficients of the linear terms:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2752.416    149.261  18.440  &lt; 2e-16 ***\nmager         20.473      6.362   3.218  0.00131 ** \nU1.mager     -20.397      7.460  -2.734       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 310361)\n\nNull     deviance: 783450552  on 2499  degrees of freedom\nResidual deviance: 774660965  on 2496  degrees of freedom\nAIC: 38714\n\nBoot restarting based on 10 samples. Last fit:\nConvergence attained in 0 iterations (rel. change 4.9469e-07)\n\n\nCode\n# Create the figure\nplot(x=dat$mager, y=dat$dbwt, xlab=\"Mother's Age (Years)\", ylab='Infant Birth Weight (grams)', col='gray65')\nplot(seg2, add=T, col='black', lwd=2)\n\n\n\n\n\nFrom our summary(seg2) output, the estimated breakpoint is at 28 years of age. Based on our scatterplot, it does appear that the slope seems to incresae slightly up until age 28, then it seems to stabilize as a fairly flat line (i.e., slope near 0)."
  },
  {
    "objectID": "labs/prac13s/index.html#b-significance-test-for-breakpoints",
    "href": "labs/prac13s/index.html#b-significance-test-for-breakpoints",
    "title": "Week 13 Practice Problems: Solutions",
    "section": "2b: Significance Test for Breakpoints",
    "text": "2b: Significance Test for Breakpoints\nUsing Davies’ test, determine if there is a significant change in slope. State your null and alternative hypothesis.\nSolution:\nOur null hypothesis is that the difference-in-slopes is equal to 0 (i.e., \\(H_0: \\beta = 0\\) if \\(\\beta\\) is our difference-in-slopes). The alternative hypothesis is that the difference-in-slopes is not equal to 0. The null hypothesis corresponds to no breakpoint in our data.\nWe can use the davies.test() function in the segmented package to test this hypothesis:\n\n\nCode\ndavies.test(mod2)\n\n\n\n    Davies' test for a change in the slope\n\ndata:  formula = dbwt ~ mager ,   method = glm \nmodel = gaussian , link = identity , statist = lrt \nsegmented variable = mager\n'best' at = 32.222, n.points = 10, p-value = 0.04485\nalternative hypothesis: two.sided\n\n\nFrom this output, we see that \\(p=0.045 &lt; 0.05\\). Therefore, we will reject the null hypothesis and conclude that there is a change in slopes around a single breakpoint.\nIf we are concerned that the “best” breakpoint of 32.222 doesn’t match our estimate of 28, we can also specify the \\(k\\) parameter and make it larger (although the p-value, since it is near 0.05, may cross over to being greater than 0.05):\n\n\nCode\ndavies.test(mod2, k=20) # still significant p-value\n\n\n\n    Davies' test for a change in the slope\n\ndata:  formula = dbwt ~ mager ,   method = glm \nmodel = gaussian , link = identity , statist = lrt \nsegmented variable = mager\n'best' at = 28.053, n.points = 20, p-value = 0.04883\nalternative hypothesis: two.sided\n\n\nCode\ndavies.test(mod2, k=100) # no longer significant p-value\n\n\n\n    Davies' test for a change in the slope\n\ndata:  formula = dbwt ~ mager ,   method = glm \nmodel = gaussian , link = identity , statist = lrt \nsegmented variable = mager\n'best' at = 27.838, n.points = 100, p-value = 0.05245\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "labs/prac13s/index.html#c-significance-testing-of-slopes",
    "href": "labs/prac13s/index.html#c-significance-testing-of-slopes",
    "title": "Week 13 Practice Problems: Solutions",
    "section": "2c: Significance Testing of Slopes",
    "text": "2c: Significance Testing of Slopes\nAre the slopes both significantly different from 0 before and after the breakpoint?\nSolution:\nFrom 2b and Davies’ test we know that the breakpoint is significant and the slopes change significantly before and after the breakpoint. However, we did not test if either (or both) slopes are significantly different from 0. We can evaluate this question using the slope() function:\n\n\nCode\nslope(seg2)\n\n\n$mager\n            Est. St.Err.  t value CI(95%).l CI(95%).u\nslope1 20.473000   6.362 3.218000    8.0035   32.9420\nslope2  0.075971   3.895 0.019505   -7.5581    7.7101\n\n\nFrom the output, we see that it automatically calculates the slope after the breakpoint while also presenting the slope1 parameter that matches the mager summary from summary(seg2). While we could manually calculate a p-value based on the provided \\(t\\)-value, we can also use the 95% CI to evaluate significance. In this case, only the slope1 confidence interval doesn’t contain 0, suggesting that the slope after the breakpoint is not significantly different from 0 (i.e., it matches our visual interpretation).\nWhile not asked for in this question, we could note that for every 1 year increase in age, the average infant birth weight increases by 20.5 grams (95% CI 8.0, 32.9 grams) before 28 years of maternal age and increases by 0.1 grams (95% CI: -7.6, 7.7 grams) above 28 years of maternal age."
  },
  {
    "objectID": "labs/prac14s/index.html#a-fitting-the-model-frequentist",
    "href": "labs/prac14s/index.html#a-fitting-the-model-frequentist",
    "title": "Week 14 Practice Problems: Solutions",
    "section": "1a: Fitting the Model (Frequentist)",
    "text": "1a: Fitting the Model (Frequentist)\nFit the frequentist multiple linear regression model for the outcome of throat pain (i.e., dependent variable) and independent variables for ASA score, gender, age, and treatment status. Print the summary table output for reference in the following questions.\nSolution:\nOur (frequentist) linear regression model is\n\n\nCode\n# Fit with glm function\nglm_full &lt;- glm(pacu30min_throatPain ~ treat + preOp_gender + preOp_age, data=dat)\nsummary(glm_full)\n\n\n\nCall:\nglm(formula = pacu30min_throatPain ~ treat + preOp_gender + preOp_age, \n    data = dat)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.229559   0.317234   3.876 0.000139 ***\ntreat        -0.744559   0.156179  -4.767 3.32e-06 ***\npreOp_gender -0.259238   0.159345  -1.627 0.105134    \npreOp_age    -0.001819   0.005051  -0.360 0.719128    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.415709)\n\n    Null deviance: 361.14  on 232  degrees of freedom\nResidual deviance: 324.20  on 229  degrees of freedom\nAIC: 748.19\n\nNumber of Fisher Scoring iterations: 2\n\n\nThe 95% confidence intervals (to compare with credible intervals later) are:\n\n\nCode\n# Print confidence intervals to use for later responses\nconfint(glm_full)\n\n\n                   2.5 %       97.5 %\n(Intercept)   0.60779299  1.851325830\ntreat        -1.05066508 -0.438452912\npreOp_gender -0.57154892  0.053072117\npreOp_age    -0.01171743  0.008080349"
  },
  {
    "objectID": "labs/prac14s/index.html#b-fitting-the-model-bayesian",
    "href": "labs/prac14s/index.html#b-fitting-the-model-bayesian",
    "title": "Week 14 Practice Problems: Solutions",
    "section": "1b: Fitting the Model (Bayesian)",
    "text": "1b: Fitting the Model (Bayesian)\nFit the Bayesian multiple linear regression model for the outcome of throat pain (i.e., dependent variable) and independent variables for ASA score, gender, age, and treatment status. Assume priors of \\(\\beta_i \\sim N(\\mu=0,\\sigma=100)\\) for all beta coefficients and use the default prior for \\(\\sigma\\) in your chosen package/software. Compare your results to the frequentist linear regression in 1a. Provide an interpretation of the slope for the treatment effect and its 95% credible interval.\nSolution:\nWe will use the brms package for our Bayesian implementation:\n\n\nCode\nlibrary(brms)\n\nset.seed(515) # set seed for reproducibility\nbayes1 &lt;- brm( pacu30min_throatPain ~ treat + preOp_gender + preOp_age, \n               data = dat,\n               family = gaussian(),\n               prior = c(set_prior(\"normal(0,10000)\", class='b'),\n                         set_prior(\"normal(0,10000)\", class='Intercept')) )\n\n\n\n\nCode\nlibrary(brms) # load package\nsummary(bayes1)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pacu30min_throatPain ~ treat + preOp_gender + preOp_age \n   Data: dat (Number of observations: 233) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept        1.24      0.32     0.61     1.84 1.00     5777     3483\ntreat           -0.75      0.16    -1.05    -0.44 1.00     5223     3217\npreOp_gender    -0.26      0.16    -0.58     0.05 1.00     5248     3078\npreOp_age       -0.00      0.01    -0.01     0.01 1.00     5645     3494\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.19      0.06     1.09     1.31 1.00     4529     3210\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nposterior_summary(bayes1) # get results to place into table, 95% CrI specifically\n\n\n                    Estimate   Est.Error          Q2.5         Q97.5\nb_Intercept     1.235084e+00 0.318260922    0.60580350    1.83967321\nb_treat        -7.455838e-01 0.156281746   -1.04588405   -0.44296585\nb_preOp_gender -2.568938e-01 0.158918083   -0.57933482    0.05474165\nb_preOp_age    -1.919861e-03 0.005008127   -0.01145995    0.00804481\nsigma           1.194846e+00 0.056497367    1.09124113    1.31046288\nIntercept       6.482126e-01 0.079092495    0.49225522    0.80525722\nlprior         -4.188818e+01 0.013426863  -41.91641877  -41.86429702\nlp__           -4.133291e+02 1.557275698 -417.19347046 -411.25006983\n\n\nCode\nprior_summary(bayes1) # check priors\n\n\n                prior     class         coef group resp dpar nlpar lb ub\n      normal(0,10000)         b                                         \n      normal(0,10000)         b    preOp_age                            \n      normal(0,10000)         b preOp_gender                            \n      normal(0,10000)         b        treat                            \n      normal(0,10000) Intercept                                         \n student_t(3, 0, 2.5)     sigma                                     0   \n       source\n         user\n (vectorized)\n (vectorized)\n (vectorized)\n         user\n      default\n\n\nFor those in the treatment group with licorice gargle, throat pain at 30 minutes in the PACU is 0.75 points lower (95% CrI: 0.61, 1.84) on average than those in the control group with sugar water after adjusting for age and gender.\nIn general, our beta coefficients and standard errors are quite similiar. The 95% confidence intervals and 95% credible intervals are also fairly similar. This suggests that using “non-informative” priors may result in similar findings to those of the standard frequentist analysis, but we would want to consider other priors that may be meaningful as well. We can also note that our Rhat estimates are all 1.0, suggesting good convergence for our MCMC."
  },
  {
    "objectID": "labs/prac14s/index.html#c-mcmc-diagnostic-plots",
    "href": "labs/prac14s/index.html#c-mcmc-diagnostic-plots",
    "title": "Week 14 Practice Problems: Solutions",
    "section": "1c: MCMC Diagnostic Plots",
    "text": "1c: MCMC Diagnostic Plots\nCreate the density and trace plots for your intercept and slope parameters from 1b. Briefly describe them and note any potential issues. Note, it is okay if additional parameters are included in your plots, just focus on the beta coefficients for this problem.\nSolution:\n\n\nCode\nplot(bayes1)\n\n\n\n\n\nWe see that the posterior distribution for each beta coefficient is approximately normal, although the tails may be fatter with some non-normal deviation. The trace plots show mixing across the sample space from all chains, suggesting adequate information for our posterior."
  },
  {
    "objectID": "labs/prac14s/index.html#d-more-informative-priors",
    "href": "labs/prac14s/index.html#d-more-informative-priors",
    "title": "Week 14 Practice Problems: Solutions",
    "section": "1d: More Informative Priors",
    "text": "1d: More Informative Priors\nLet’s assume that we a priori believed that the treatment should result in a 3 point reduction with a standard deviation of 0.25 (i.e., \\(\\beta_{treat} \\sim N(\\mu=-3,\\sigma=0.25)\\). Now fit the Bayesian model with this informative prior for treatment group but leaving all other priors unchanged from 1c. How do the beta coefficients change? Briefly discuss the potential impact of sample size on the informativeness of the prior.\nSolution:\n\n\nCode\nset.seed(515) # set seed for reproducibility\nbayes2 &lt;- brm( pacu30min_throatPain ~ treat + preOp_gender + preOp_age, \n               data = dat,\n               family = gaussian(),\n               prior = c(set_prior(\"normal(-3,0.0625)\", coef='treat'),\n                         set_prior(\"normal(0,10000)\", class='b'),\n                         set_prior(\"normal(0,10000)\", class='Intercept')) )\n\n\n\n\nCode\nsummary(bayes2)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pacu30min_throatPain ~ treat + preOp_gender + preOp_age \n   Data: dat (Number of observations: 233) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept        2.40      0.41     1.62     3.20 1.00     5189     3414\ntreat           -2.81      0.06    -2.93    -2.69 1.00     4308     3044\npreOp_gender    -0.17      0.21    -0.59     0.23 1.00     4853     3095\npreOp_age       -0.00      0.01    -0.02     0.01 1.00     5285     3276\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.59      0.08     1.45     1.74 1.00     4021     3336\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nposterior_summary(bayes2) # get results to place into table, 95% CrI specifically\n\n\n                    Estimate  Est.Error          Q2.5         Q97.5\nb_Intercept     2.404429e+00 0.40697609    1.62054091  3.202742e+00\nb_treat        -2.811520e+00 0.06188197   -2.93446188 -2.690691e+00\nb_preOp_gender -1.745726e-01 0.20811488   -0.58916498  2.295389e-01\nb_preOp_age    -4.780098e-03 0.00673394   -0.01792011  8.209132e-03\nsigma           1.585061e+00 0.07569206    1.44556915  1.742159e+00\nIntercept       6.492187e-01 0.10306522    0.44754935  8.461912e-01\nlprior         -3.504734e+01 3.04133539  -42.25318392 -3.054857e+01\nlp__           -4.718568e+02 1.55714624 -475.61981036 -4.697922e+02\n\n\nCode\nprior_summary(bayes2) # check priors\n\n\n                prior     class         coef group resp dpar nlpar lb ub\n      normal(0,10000)         b                                         \n      normal(0,10000)         b    preOp_age                            \n      normal(0,10000)         b preOp_gender                            \n    normal(-3,0.0625)         b        treat                            \n      normal(0,10000) Intercept                                         \n student_t(3, 0, 2.5)     sigma                                     0   \n       source\n         user\n (vectorized)\n (vectorized)\n         user\n         user\n      default\n\n\nThe most meaningful changes in our posterior inference are for the intercept and the treatment group beta coefficient. For example, our treatment estimate is now \\(\\hat{\\beta}_{treat}=-2.81\\) versus the original \\(\\hat{\\beta}_{treat}=-0.75\\) when we used a “non-informative” prior. This suggests our posterior is more reflective of our prior belief than the observed data.\nThis shift in the treatment group estimate then led to our intercept estimate of \\(\\hat{\\beta}_{0}=2.40\\) versus the estimate of \\(\\hat{\\beta}_{0}=1.24\\) with the “non-informative” prior. We see that there are some changes to gender and age, but not as large.\nOur sample size is 233 for this data set. In an analysis with a larger sample size, we may expect this “informative” prior to be less informative. However, it currently is providing more influence to our posterior estimate of treatment group than the observed likelihood."
  },
  {
    "objectID": "labs/prac14s/index.html#e-posterior-probability-calculations",
    "href": "labs/prac14s/index.html#e-posterior-probability-calculations",
    "title": "Week 14 Practice Problems: Solutions",
    "section": "1e: Posterior Probability Calculations",
    "text": "1e: Posterior Probability Calculations\nCalculate the posterior probabilities that \\(P(\\hat{\\beta}_{treat} \\leq 0)\\), \\(P(\\hat{\\beta}_{treat} &gt; 0)\\), and \\(P(\\hat{\\beta}_{treat} \\leq -1)\\) for both 1b and 1d. Summarize your results in a table and briefly compare model results.\nSolution:\n\n\nCode\n# extract posterior samples for calculations\npost1 &lt;- as_draws_df(bayes1)\npost2 &lt;- as_draws_df(bayes2)\n\n# P(betahat_treat &lt;= 0)\npp1_lte0 &lt;- mean( post1[,'b_treat'] &lt;= 0 )\npp2_lte0 &lt;- mean( post2[,'b_treat'] &lt;= 0 )\n\n# P(betahat_treat &gt; 0)\npp1_gt0 &lt;- mean( post1[,'b_treat'] &gt; 0 )\npp2_gt0 &lt;- mean( post2[,'b_treat'] &gt; 0 )\n\n# P(betahat_treat &lt;= -1)\npp1_lten1 &lt;- mean( post1[,'b_treat'] &lt;= -1 )\npp2_lten1 &lt;- mean( post2[,'b_treat'] &lt;= -1 )\n\n# make matrix for table of results, round to 4 places\nres_mat &lt;- matrix( round( c(pp1_lte0, pp1_gt0, pp1_lten1, \n                     pp2_lte0, pp2_gt0, pp2_lten1), 4), \n                   nrow=2, byrow=T,\n                   dimnames = list( c('Non-Informative Prior (1b)','Informative Prior (1d)') ,\n                                    c('&lt;=0','&gt;0','&lt;=-1')) )\n\n# print table\nres_mat\n\n\n                           &lt;=0 &gt;0   &lt;=-1\nNon-Informative Prior (1b)   1  0 0.0508\nInformative Prior (1d)       1  0 1.0000\n\n\nIn this example, we see both the non-informative and informative priors agree with respect to posterior probabilities estimated for 0 (i.e., 100% for being less than or equal to 0 and 0% for being greater than 0).\nHowever, we see very different results for \\(P(\\hat{\\beta}_{treat} \\leq -1\\)) where the non-informative prior’s posterior probability is only 5.08% (i.e., very unlikely), whereas the informative prior’s posterior probability is 100% (i.e., definitely less than -1). Given that we know the informative prior is very strong and disagrees with our likelihood, it seems more likely that the non-informative prior is appropriately summarizing our results based on the observed data.\nIn practice, we want to consider multiple priors and present all results while being transparent to their limitations. For instance, we could present our non-informative prior as a potentially more statistically and scientifically conservative priors, but then also present the informative prior and make it clear that this is very strong due to its larger than observed mean difference and small standard deviation. Then, a reader can use this information to decide amongst the results themselves."
  },
  {
    "objectID": "labs/prac14/index.html#a-fitting-the-model-frequentist",
    "href": "labs/prac14/index.html#a-fitting-the-model-frequentist",
    "title": "Week 14 Practice Problems",
    "section": "1a: Fitting the Model (Frequentist)",
    "text": "1a: Fitting the Model (Frequentist)\nFit the frequentist multiple linear regression model for the outcome of throat pain (i.e., dependent variable) and independent variables for ASA score, gender, age, and treatment status. Print the summary table output for reference in the following questions."
  },
  {
    "objectID": "labs/prac14/index.html#b-fitting-the-model-bayesian",
    "href": "labs/prac14/index.html#b-fitting-the-model-bayesian",
    "title": "Week 14 Practice Problems",
    "section": "1b: Fitting the Model (Bayesian)",
    "text": "1b: Fitting the Model (Bayesian)\nFit the Bayesian multiple linear regression model for the outcome of throat pain (i.e., dependent variable) and independent variables for ASA score, gender, age, and treatment status. Assume priors of \\(\\beta_i \\sim N(\\mu=0,\\sigma=100)\\) for all beta coefficients and use the default prior for \\(\\sigma\\) in your chosen package/software. Compare your results to the frequentist linear regression in 1a. Provide an interpretation of the slope for the treatment effect and its 95% credible interval."
  },
  {
    "objectID": "labs/prac14/index.html#c-mcmc-diagnostic-plots",
    "href": "labs/prac14/index.html#c-mcmc-diagnostic-plots",
    "title": "Week 14 Practice Problems",
    "section": "1c: MCMC Diagnostic Plots",
    "text": "1c: MCMC Diagnostic Plots\nCreate the density and trace plots for your intercept and slope parameters from 1b. Briefly describe them and note any potential issues. Note, it is okay if additional parameters are included in your plots, just focus on the beta coefficients for this problem."
  },
  {
    "objectID": "labs/prac14/index.html#d-more-informative-priors",
    "href": "labs/prac14/index.html#d-more-informative-priors",
    "title": "Week 14 Practice Problems",
    "section": "1d: More Informative Priors",
    "text": "1d: More Informative Priors\nLet’s assume that we a priori believed that the treatment should result in a 3 point reduction with a standard deviation of 0.25 (i.e., \\(\\beta_{treat} \\sim N(\\mu=-3,\\sigma=0.25)\\). Now fit the Bayesian model with this informative prior for treatment group but leaving all other priors unchanged from 1c. How do the beta coefficients change? Briefly discuss the potential impact of sample size on the informativeness of the prior."
  },
  {
    "objectID": "labs/prac14/index.html#e-posterior-probability-calculations",
    "href": "labs/prac14/index.html#e-posterior-probability-calculations",
    "title": "Week 14 Practice Problems",
    "section": "1e: Posterior Probability Calculations",
    "text": "1e: Posterior Probability Calculations\nCalculate the posterior probabilities that \\(P(\\hat{\\beta}_{treat} \\leq 0)\\), \\(P(\\hat{\\beta}_{treat} &gt; 0)\\), and \\(P(\\hat{\\beta}_{treat} \\leq -1)\\) for both 1b and 1d. Summarize your results in a table and briefly compare model results."
  }
]